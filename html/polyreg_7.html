<div class="container">

<table style="width: 100%;"><tr>
<td>polyFit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Polynomial Fit</h2>

<h3>Description</h3>

<p>Fit polynomial regression using a linear or logistic model; predict
new data.</p>


<h3>Usage</h3>

<pre><code class="language-R">polyFit(xy, deg, maxInteractDeg=deg, use = "lm", glmMethod="one", 
     return_xy=FALSE, returnPoly=FALSE, noisy=TRUE)
## S3 method for class 'polyFit'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xy</code></td>
<td>
<p>Data frame with response variable in the last
column.  Latter is numeric, except in the classification case: 
Categorical variables (&gt; 2 levels) must be passed as factors 
or character variables if <code>use</code> is 'glm'; an integer vector
must be used for the for the 'mvrlm' case, or for the 2-class case
(0s and 1s).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>deg</code></td>
<td>
<p>The max degree for polynomial terms.  A term such as uv,
for instance, is considered degree 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxInteractDeg</code></td>
<td>
<p>The max degree of interaction terms.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use</code></td>
<td>
<p>Set to 'lm' for linear regression, 'glm' for
logistic regression, or 'mvrlm' for multivariate-response <code>lm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>glmMethod</code></td>
<td>
<p>Defaults to 'one,' meaning the One Versus All Method.
Use 'all' for All Versus All.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>Data frame, one row for each "X" to be predicted.  Must
have the same column names as in <code>xy</code> (without "Y").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>An item of class 'polyFit' containing output. Can be 
used with predict().</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_xy</code></td>
<td>
<p>Return data? Default: FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>returnPoly</code></td>
<td>
<p>return polyMatrix object? Defaults to FALSE since may be quite large.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>noisy</code></td>
<td>
<p>Logical: display messages?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments for getPoly().</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>polyFit</code> function calls <code>getPoly</code> to generate
polynomial terms from predictor variables, then fits the generated
data to a linear or logistic regression model.  (Powers of dummy
variables will not be generated, other than degree 1, but interaction
terms will calculated.)
</p>
<p>When logistic regression for classification is indicated, with more
than two classes, All-vs-All or One-vs-All methods, coded
<code>'all'</code> and <code>'one'</code>, can be applied to deal with multiclass
problem.  
</p>
<p>Under the 'mvrlm' option in a classification problem, <code>lm</code> is
called with multivariate response, using <code>cbind</code> and dummy
variables for class membership as the response.  Since predictors are
used to form polynomials, this should be a reasonable model, and is
much faster than 'glm'.
</p>


<h3>Value</h3>

<p>The return value of <code>polyFit()</code> is an <code>polyFit</code> object.  The
orginal arguments are retained, along with the fitted models and so on.
</p>
<p>The prediction function <code>predict.polyFit</code> returns the predicted
value(s) for <code>newdata</code>. It also contains probability for each class as
an attribute named <code>prob</code>. In the classification case, these will be
the predicted class labels, 1,2,3,...
</p>


<h3>Examples</h3>

<pre><code class="language-R">
N &lt;- 125
xyTrain &lt;- data.frame(x1 = rnorm(N), 
                      x2 = rnorm(N),
                      group = sample(letters[1:5], N, replace=TRUE),
                      score = sample(100, N, replace = TRUE) # final column is y
                      )

pfOut &lt;- polyFit(xyTrain, 2)

# 4 new test points
xTest &lt;- data.frame(x1 = rnorm(4), 
                    x2 = rnorm(4),
                    group = sample(letters[1:5], 4, replace=TRUE))
  
predict(pfOut, xTest) # returns vector of 4 predictions

data(pef)
# predict wageinc
z &lt;- polyFit(pef[,c(setdiff(1:6,5),5)],2)
predict(z,pef[2000,c(setdiff(1:6,5),5)])  # 56934.39
# predict occ
z &lt;- polyFit(pef[,c(setdiff(1:6,3),3)],2,use='glm')
predict(z,pef[2000,c(setdiff(1:6,3),3)])  # '100', probs 0.43, 0.26,...

</code></pre>


</div>