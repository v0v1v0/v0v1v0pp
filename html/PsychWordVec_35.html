<div class="container">

<table style="width: 100%;"><tr>
<td>text_to_vec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Extract contextualized word embeddings from transformers (pre-trained language models).</h2>

<h3>Description</h3>

<p>Extract hidden layers from a language model and aggregate them to
get token (roughly word) embeddings and text embeddings
(all reshaped to <code>embed</code> matrix).
It is a wrapper function of <code>text::textEmbed()</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">text_to_vec(
  text,
  model,
  layers = "all",
  layer.to.token = "concatenate",
  token.to.word = TRUE,
  token.to.text = TRUE,
  encoding = "UTF-8",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>text</code></td>
<td>
<p>Can be:
</p>

<ul>
<li>
<p>a character string or vector of text (usually sentences)
</p>
</li>
<li>
<p>a data frame with at least one character variable
(for text from all character variables in a given data frame)
</p>
</li>
<li>
<p>a file path on disk containing text
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>Model name at <a href="https://huggingface.co/models">HuggingFace</a>.
See <code>text_model_download</code>.
If the model has not been downloaded, it would automatically download the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layers</code></td>
<td>
<p>Layers to be extracted from the <code>model</code>,
which are then aggregated in the function
<code>text::textEmbedLayerAggregation()</code>.
Defaults to <code>"all"</code> which extracts all layers.
You may extract only the layers you need (e.g., <code>11:12</code>).
Note that layer 0 is the <em>decontextualized</em> input layer
(i.e., not comprising hidden states).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layer.to.token</code></td>
<td>
<p>Method to aggregate hidden layers to each token.
Defaults to <code>"concatenate"</code>,
which links together each word embedding layer to one long row.
Options include <code>"mean"</code>, <code>"min"</code>, <code>"max"</code>, and <code>"concatenate"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>token.to.word</code></td>
<td>
<p>Aggregate subword token embeddings (if whole word is out of vocabulary)
to whole word embeddings. Defaults to <code>TRUE</code>, which sums up subword token embeddings.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>token.to.text</code></td>
<td>
<p>Aggregate token embeddings to each text.
Defaults to <code>TRUE</code>, which averages all token embeddings.
If <code>FALSE</code>, the text embedding will be the token embedding of <code>[CLS]</code>
(the special token that is used to represent the beginning of a text sequence).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>encoding</code></td>
<td>
<p>Text encoding (only used if <code>text</code> is a file).
Defaults to <code>"UTF-8"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other parameters passed to
<code>text::textEmbed()</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A <code>list</code> of:
</p>

<dl>
<dt><code>token.embed</code></dt>
<dd>
<p>Token (roughly word) embeddings</p>
</dd>
<dt><code>text.embed</code></dt>
<dd>
<p>Text embeddings, aggregated from token embeddings</p>
</dd>
</dl>
<h3>See Also</h3>

<p><code>text_init</code>
</p>
<p><code>text_model_download</code>
</p>
<p><code>text_model_remove</code>
</p>
<p><code>text_unmask</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# text_init()  # initialize the environment

text = c("Download models from HuggingFace",
         "Chinese are East Asian",
         "Beijing is the capital of China")
embed = text_to_vec(text, model="bert-base-cased", layers=c(0, 12))
embed

embed1 = embed$token.embed[[1]]
embed2 = embed$token.embed[[2]]
embed3 = embed$token.embed[[3]]

View(embed1)
View(embed2)
View(embed3)
View(embed$text.embed)

plot_similarity(embed1, value.color="grey")
plot_similarity(embed2, value.color="grey")
plot_similarity(embed3, value.color="grey")
plot_similarity(rbind(embed1, embed2, embed3))

## End(Not run)

</code></pre>


</div>