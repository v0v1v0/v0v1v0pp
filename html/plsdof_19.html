<div class="container">

<table style="width: 100%;"><tr>
<td>pls.ic</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Model selection for Partial Least Squares based on information criteria</h2>

<h3>Description</h3>

<p>This function computes the optimal model parameters using one of three
different model selection criteria (aic, bic, gmdl) and based on two
different Degrees of Freedom estimates for PLS.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pls.ic(
  X,
  y,
  m = min(ncol(X), nrow(X) - 1),
  criterion = "bic",
  naive = FALSE,
  use.kernel = FALSE,
  compute.jacobian = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m=ncol(X)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>criterion</code></td>
<td>
<p>Choice of the model selection criterion. One of the three
options aic, bic, gmdl.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>naive</code></td>
<td>
<p>Use the naive estimate for the Degrees of Freedom? Default is
<code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use.kernel</code></td>
<td>
<p>Use kernel representation? Default is
<code>use.kernel=FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compute.jacobian</code></td>
<td>
<p>Should the first derivative of the regression
coefficients be computed as well? Default is <code>FALSE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If <code>TRUE</code>, the function prints a warning if the
algorithms produce negative Degrees of Freedom. Default is <code>TRUE</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>There are two options to estimate the Degrees of Freedom of PLS:
<code>naive=TRUE</code> defines the Degrees of Freedom as the number of components
+1, and <code>naive=FALSE</code> uses the generalized notion of Degrees of
Freedom. If <code>compute.jacobian=TRUE</code>, the function uses the Lanczos
decomposition to derive the Degrees of Freedom, otherwise, it uses the
Krylov representation. (See Kraemer and Sugiyama (2011) for details.) The
latter two methods only differ with respect to the estimation of the noise
level.
</p>


<h3>Value</h3>

<p>The function returns an object of class "plsdof". </p>
<table>
<tr style="vertical-align: top;">
<td><code>DoF</code></td>
<td>
<p>Degrees
of Freedom</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m.opt</code></td>
<td>
<p>optimal number of components</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigmahat</code></td>
<td>
<p>vector of estimated model errors</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>intercept</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coefficients</code></td>
<td>
<p>vector of regression
coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>covariance</code></td>
<td>
<p>if <code>compute.jacobian=TRUE</code> and
<code>use.kernel=FALSE</code>, the function returns the covariance matrix of the
optimal regression coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m.crash</code></td>
<td>
<p>the number of components
for which the algorithm returns negative Degrees of Freedom</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Akaikie, H. (1973) "Information Theory and an Extension of the
Maximum Likelihood Principle". Second International Symposium on Information
Theory, 267 - 281.
</p>
<p>Hansen, M., Yu, B. (2001). "Model Selection and Minimum Descripion Length
Principle". Journal of the American Statistical Association, 96, 746 - 774
</p>
<p>Kraemer, N., Sugiyama M. (2011). "The Degrees of Freedom of Partial Least
Squares Regression". Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) "Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection", Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>
<p>Schwartz, G. (1979) "Estimating the Dimension of a Model" Annals of
Statistics 26(5), 1651 - 1686.
</p>


<h3>See Also</h3>

<p><code>pls.model</code>, <code>pls.cv</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
n&lt;-50 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

# compute linear PLS
pls.object&lt;-pls.ic(X,y,m=ncol(X))

</code></pre>


</div>