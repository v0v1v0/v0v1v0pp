<div class="container">

<table style="width: 100%;"><tr>
<td>policytree-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees</h2>

<h3>Description</h3>

<p>A package for learning simple rule-based policies, where the rule takes the form of a shallow decision tree. Applications include settings which require interpretable predictions, such as for example a medical treatment prescription. This package uses doubly robust reward estimates from <code>grf</code> to find a shallow, but globally optimal decision tree.
</p>
<p>Some helpful links for getting started:
</p>

<ul>
<li>
<p> The R package documentation contains usage examples and method references (<a href="https://grf-labs.github.io/policytree/">https://grf-labs.github.io/policytree/</a>).
</p>
</li>
<li>
<p> For community questions and answers around usage, see the GitHub issues page (<a href="https://github.com/grf-labs/policytree/issues">https://github.com/grf-labs/policytree/issues</a>).
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Erik Sverdrup <a href="mailto:erikcs@stanford.edu">erikcs@stanford.edu</a>
</p>
<p>Authors:
</p>

<ul>
<li>
<p> Ayush Kanodia
</p>
</li>
<li>
<p> Zhengyuan Zhou
</p>
</li>
<li>
<p> Susan Athey
</p>
</li>
<li>
<p> Stefan Wager
</p>
</li>
</ul>
<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/grf-labs/policytree">https://github.com/grf-labs/policytree</a>
</p>
</li>
<li>
<p> Report bugs at <a href="https://github.com/grf-labs/policytree/issues">https://github.com/grf-labs/policytree/issues</a>
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">
# Multi-action policy learning example.
n &lt;- 250
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y &lt;- X[, 1] + X[, 2] * (W == "B") + X[, 3] * (W == "C") + runif(n)
multi.forest &lt;- grf::multi_arm_causal_forest(X, Y, W)

# Compute doubly robust reward estimates.
Gamma.matrix &lt;- double_robust_scores(multi.forest)

# Fit a depth 2 tree on a random training subset.
train &lt;- sample(1:n, 200)
opt.tree &lt;- policy_tree(X[train, ], Gamma.matrix[train, ], depth = 2)
opt.tree

# Predict treatment on held out data.
predict(opt.tree, X[-train, ])


</code></pre>


</div>