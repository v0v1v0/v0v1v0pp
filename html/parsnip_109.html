<div class="container">

<table style="width: 100%;"><tr>
<td>details_mlp_brulee</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multilayer perceptron via brulee</h2>

<h3>Description</h3>

<p><code>brulee::brulee_mlp()</code> fits a neural network.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 6 tuning parameters:
</p>

<ul>
<li> <p><code>hidden_units</code>: # Hidden Units (type: integer, default: 3L)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.0)
</p>
</li>
<li> <p><code>epochs</code>: # Epochs (type: integer, default: 100L)
</p>
</li>
<li> <p><code>dropout</code>: Dropout Rate (type: double, default: 0.0)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.01)
</p>
</li>
<li> <p><code>activation</code>: Activation Function (type: character, default: ‘relu’)
</p>
</li>
</ul>
<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as glmnet).
The zeroing out of parameters is a specific feature the optimization
method used in those packages.
</p>
<p>Both <code>penalty</code> and <code>dropout</code> should be not be used in the same model.
</p>
<p>Other engine arguments of interest:
</p>

<ul>
<li> <p><code>momentum()</code>: A number used to use historical gradient infomration
during optimization.
</p>
</li>
<li> <p><code>batch_size()</code>: An integer for the number of training set points in
each batch.
</p>
</li>
<li> <p><code>class_weights()</code>: Numeric class weights. See
<code>brulee::brulee_mlp()</code>.
</p>
</li>
<li> <p><code>stop_iter()</code>: A non-negative integer for how many iterations with no
improvement before stopping. (default: 5L).
</p>
</li>
</ul>
<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) %&gt;%  
  set_engine("brulee") %&gt;% 
  set_mode("regression") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (regression)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
##   learn_rate = double(1)
## 
## Computational engine: brulee 
## 
## Model fit template:
## brulee::brulee_mlp(x = missing_arg(), y = missing_arg(), hidden_units = integer(1), 
##     penalty = double(1), dropout = double(1), epochs = integer(1), 
##     activation = character(1), learn_rate = double(1))
</pre></div>
<p>Note that parsnip automatically sets linear activation in the last
layer.
</p>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) %&gt;% 
  set_engine("brulee") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Single Layer Neural Network Model Specification (classification)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
##   learn_rate = double(1)
## 
## Computational engine: brulee 
## 
## Model fit template:
## brulee::brulee_mlp(x = missing_arg(), y = missing_arg(), hidden_units = integer(1), 
##     penalty = double(1), dropout = double(1), epochs = integer(1), 
##     activation = character(1), learn_rate = double(1))
</pre></div>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code>fit()</code>, parsnip will
convert factor columns to indicators.
</p>
<p>Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
</p>



<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul><li>
<p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li></ul>
</div>