<div class="container">

<table style="width: 100%;"><tr>
<td>linear.pls.fit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Linear Partial Least Squares Fit</h2>

<h3>Description</h3>

<p>This function computes the Partial Least Squares solution and the first
derivative of the regression coefficients. This implementation scales mostly
in the number of variables
</p>


<h3>Usage</h3>

<pre><code class="language-R">linear.pls.fit(
  X,
  y,
  m = ncol(X),
  compute.jacobian = FALSE,
  DoF.max = min(ncol(X) + 1, nrow(X) - 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m</code>=ncol(X).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compute.jacobian</code></td>
<td>
<p>Should the first derivative of the regression
coefficients be computed as well? Default is <code>FALSE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DoF.max</code></td>
<td>
<p>upper bound on the Degrees of Freedom. Default is
<code>min(ncol(X)+1,nrow(X)-1)</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>We first standardize <code>X</code> to zero mean and unit variance.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>coefficients</code></td>
<td>
<p>matrix of regression coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>vector of regression intercepts</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DoF</code></td>
<td>
<p>Degrees of
Freedom</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigmahat</code></td>
<td>
<p>vector of estimated model error</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Yhat</code></td>
<td>
<p>matrix
of fitted values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat</code></td>
<td>
<p>vector of squared length of fitted values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RSS</code></td>
<td>
<p>vector of residual sum of error</p>
</td>
</tr>
</table>
<p><code>covariance</code>if
<code>compute.jacobian</code> is <code>TRUE</code>, the function returns the array of
covariance matrices for the PLS regression coefficients. </p>
<table><tr style="vertical-align: top;">
<td><code>TT</code></td>
<td>
<p>matrix of
normalized PLS components</p>
</td>
</tr></table>
<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). "The Degrees of Freedom of
Partial Least Squares Regression". Journal of the American Statistical
Association 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>


<h3>See Also</h3>

<p><code>kernel.pls.fit</code>,
<code>pls.cv</code>,<code>pls.model</code>, <code>pls.ic</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
n&lt;-50 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

pls.object&lt;-linear.pls.fit(X,y,m=5,compute.jacobian=TRUE)

</code></pre>


</div>