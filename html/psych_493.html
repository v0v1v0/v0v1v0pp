<div class="container">

<table style="width: 100%;"><tr>
<td>splitHalf</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Alternative estimates of test reliabiity </h2>

<h3>Description</h3>

<p>Eight alternative estimates of test reliability include the six discussed by Guttman (1945), four discussed by ten Berge and Zergers (1978) (<code class="reqn">\mu_0 \dots \mu_3)</code> as well as <code class="reqn">\beta</code> (the worst split half, Revelle, 1979),  the glb (greatest lowest bound) discussed by Bentler and Woodward (1980), and <code class="reqn">\omega_h</code> and <code class="reqn">\omega_t</code> (McDonald, 1999; Zinbarg et al., 2005). Greatest and lowest split-half values are found by brute force or sampling. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">splitHalf(r,raw=FALSE,brute=FALSE,n.sample=15000,covar=FALSE,check.keys=TRUE,
           key=NULL,ci=.05,use="pairwise")
guttman(r,key=NULL) 
tenberge(r)
glb(r,key=NULL)
glb.fa(r,key=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>r</code></td>
<td>
<p>A correlation or covariance matrix or raw data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>raw</code></td>
<td>
<p>return a vector of split half reliabilities</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>brute</code></td>
<td>
<p>Use brute force to try all combinations of n take n/2. Be careful.  For e.g., n=24, this is 1,352,078 possible splits!</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.sample</code></td>
<td>
<p>If brute is false, how many samples of split halves should be tried? (16 items takes 12,780)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>covar</code></td>
<td>
<p>Should the covariances or correlations be used for reliability calculations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>check.keys</code></td>
<td>
<p>If TRUE, any item with a negative loading on the first factor will be flipped in sign</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>key</code></td>
<td>
<p>a vector of -1, 0, 1 to select or reverse key items.  See <code>scoreItems</code> for an example of keying. If the key vector is less than the number of variables, then item numbers to be reverse can be specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use</code></td>
<td>
<p>Should we find the correlations using "pairwise" or "complete" (see ?cor)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ci</code></td>
<td>
<p>The alpha level to use for the confidence intervals of the split half estimates</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Surprisingly, more than a century after Spearman (1904) introduced the concept of reliability to psychologists, there are still multiple approaches for measuring it. Although very popular, Cronbach's <code class="reqn">\alpha</code>  (1951) underestimates the reliability of a test and over estimates the first factor saturation. Using <code>splitHalf</code> for tests with 16 or fewer items, all possible splits may be found fairly easily.  For tests with 17 or more items, n.sample splits are randomly found. Thus, for 16 or fewer items, the upper and lower bounds are precise.  For 17 or more items, they are close but will probably slightly underestimate the highest and overestimate the lowest reliabilities.  
</p>
<p>The guttman function includes the six estimates discussed by Guttman (1945), four of ten Berge and Zergers (1978), as well as Revelle's <code class="reqn">\beta</code> (1979) using <code>splitHalf</code>. The companion function, <code>omega</code> calculates omega hierarchical (<code class="reqn">\omega_h</code>)  and omega total (<code class="reqn">\omega_t</code>). 
</p>
<p>Guttman's first estimate <code class="reqn">\lambda_1</code> assumes that all the variance of an item is error:
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_1 = 1 - \frac{tr(\vec{V_x})}{V_x} = \frac{V_x - tr(\vec{V}_x)}{V_x}
</code>
</p>

<p>This is a clear underestimate.
</p>
<p>The second bound, <code class="reqn">\lambda_2</code>, replaces the diagonal with a function of the square root of the  sums of squares of the off diagonal elements.  Let <code class="reqn">C_2 = \vec{1}( \vec{V}-diag(\vec{V})^2 \vec{1}' </code>, then 
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_2 = \lambda_1 + \frac{\sqrt{\frac{n}{n-1}C_2}}{V_x} = \frac{V_x - tr(\vec{V}_x) + \sqrt{\frac{n}{n-1}C_2} }{V_x}</code>
</p>

<p>Effectively, this is replacing the diagonal with  n * the square root of the average squared off diagonal element.  
</p>
<p>Guttman's 3rd lower bound, <code class="reqn">\lambda_3</code>, also  modifies <code class="reqn">\lambda_1</code> and estimates the true variance of each item as the average covariance between items and is, of course, the same as Cronbach's <code class="reqn">\alpha</code>. 
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_3 = \lambda_1 + \frac{\frac{V_X - tr(\vec{V}_X)}{n (n-1)}}{V_X} = \frac{n \lambda_1}{n-1} = \frac{n}{n-1}\Bigl(1 - \frac{tr(\vec{V})_x}{V_x}\Bigr) = \frac{n}{n-1} \frac{V_x - tr(\vec{V}_x)}{V_x} = \alpha
</code>
</p>

<p>This is just replacing the diagonal elements with the average off diagonal elements.  <code class="reqn">\lambda_2 \geq \lambda_3</code> with  <code class="reqn">\lambda_2 &gt; \lambda_3</code> if the covariances are not identical.
</p>
<p><code class="reqn">\lambda_3</code> and <code class="reqn">\lambda_2</code> are both corrections to <code class="reqn">\lambda_1</code> and this correction may  be  generalized  as an infinite set of successive improvements. (Ten Berge and Zegers, 1978) 
</p>
<p style="text-align: center;"><code class="reqn">
\mu_r = \frac{1}{V_x} \bigl( p_o + (p_1 + (p_2 + \dots (p_{r-1} +( p_r)^{1/2})^{1/2} \dots )^{1/2})^{1/2}    \bigr), r = 0, 1, 2, \dots
</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">
p_h = \sum_{i\ne j}\sigma_{ij}^{2h}, h = 0, 1, 2, \dots r-1
</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">
p_h = \frac{n}{n-1}\sigma_{ij}^{2h}, h = r 
</code>
</p>

<p>tenberge and Zegers (1978).  Clearly <code class="reqn">\mu_0 = \lambda_3 = \alpha</code> and <code class="reqn"> \mu_1 = \lambda_2</code>.  <code class="reqn">\mu_r \geq \mu_{r-1} \geq \dots \mu_1 \geq \mu_0</code>, although the series does not improve much after the first two steps.
</p>
<p>Guttman's fourth lower bound, <code class="reqn">\lambda_4</code> was originally proposed as any spit half reliability  but has been interpreted as the greatest split half reliability. If <code class="reqn">\vec{X}</code> is split into  two parts, <code class="reqn">\vec{X}_a</code> and <code class="reqn">\vec{X}_b</code>, with correlation <code class="reqn">r_{ab}</code> then
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_4 = 2\Bigl(1 - \frac{V_{X_a} + V_{X_b}}{V_X} \Bigr) =  \frac{4 r_{ab}}{V_x} = \frac{4 r_{ab}}{V_{X_a} + V_{X_b}+ 2r_{ab}V_{X_a}  V_{X_b}}
</code>
</p>

<p>which is just the normal split half reliability, but in this case, of the most similar splits. For 16 or fewer items, this is found by trying all possible splits.  For 17 or more items, this is estimated by taking n.sample random splits. 
</p>
<p><code class="reqn">\lambda_5</code>, Guttman's fifth lower bound, replaces the diagonal values with twice the square root of the maximum (across items) of the sums of squared interitem covariances
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_5 =  \lambda_1 + \frac{2 \sqrt{\bar{C_2}}}{V_X}.
</code>
</p>

<p>Although superior to <code class="reqn">\lambda_1</code>, <code class="reqn">\lambda_5</code> underestimates the correction to the diagonal.  A better estimate would be analogous to the correction used in <code class="reqn">\lambda_3</code>:
</p>
<p style="text-align: center;"><code class="reqn">
\lambda_{5+} =  \lambda_1 + \frac{n}{n-1}\frac{2 \sqrt{\bar{C_2}}}{V_X}.
</code>
</p>

<p><code class="reqn">\lambda_6</code>,Guttman's final bound considers the amount of variance in each item that can be accounted for the linear regression of all of the other items (the squared multiple correlation or smc), or more precisely, the variance of the errors, <code class="reqn">e_j^2</code>,  and is
</p>
<p style="text-align: center;"><code class="reqn">\lambda_6 = 1 - \frac{\sum e_j^2}{V_x} = 1 - \frac{\sum(1-r_{smc}^2)}{V_x}
</code>
</p>
<p>.
</p>
<p>The smc is found from all the items.  A modification to Guttman <code class="reqn">\lambda_6</code>, <code class="reqn">\lambda_6*</code> reported by the <code>score.items</code> function is to find the smc from the entire pool of items given, not just the items on the selected scale.  
</p>
<p>Guttman's <code class="reqn">\lambda_4</code> is the greatest split half reliability.  Although originally found here by combining the output from three different approaches,this has now been replaced by using <code>splitHalf</code> to find the maximum value by brute force (for 16 or fewer items) or by taking a substantial number of random splits.
</p>
<p>The algorithms that had been tried before included:
</p>
<p>a) Do an ICLUST of the reversed correlation matrix.  ICLUST normally forms the most distinct clusters.  By reversing the correlations, it will tend to find the most related clusters.  Truly a weird approach but tends to work.
</p>
<p>b) Alternatively, a kmeans clustering of the correlations (with the diagonal replaced with 0 to make pseudo distances) can produce 2 similar clusters.
</p>
<p>c) Clusters identified by assigning items to two clusters based upon their order on the first principal factor.  (Highest to cluster 1, next 2 to cluster 2, etc.)
</p>
<p>These three procedures will produce keys vectors for assigning items to the two splits.  The maximum split half reliability is found by taking the maximum of these three approaches.  This is not elegant but is fast.
</p>
<p>The brute force and the sampling procedures seem to provide more stable and larger estimates. 
</p>
<p>Yet another procedure, implemented in <code>splitHalf</code> is actually to form all possible (for n items &lt;= 16) or sample 15,000 (or more) split halfs corrected for test length.  This function returns the best and worst splits as item keys that can be used for scoring purposes, if desired.  Can do up to 24 items in reasonable time, but gets much slower for more than about 24 items. To do all possible splits of 24 items considers 1,352,078 splits.  This will give an exact value, but this will not differ that much from random samples. For a 24 item problem with exactly 2 factors (by simulation), the worst split half is much lower than just random sampling would indicate.  
</p>
<p>When consider split halfs, it is important to remember these are of roughly equal size.  So a correlation matrix of 3 unrelated factors (sharing no general factor) will have a "worst" split which is not 0.  See the examples.
</p>
<p>Timings on a MacPro for a 24 item problem with a 2.4 GHz 8 core are .24 secs for the default 10,000 samples, .678 for 30,000 samples and 22.58 sec for all possible.  
The values of the maximum split for these sample sizes  were .799,/.804 and .800 for three replications of the default sample size of 10000,  .805  and .806 for two sets of 30,000 and .812 for an exhaustive search.
</p>
<p>There are three greatest lower bound functions.  One, glb finds the greatest split half reliability, <code class="reqn">\lambda_4</code>. This considers the test as set of items and examines how best to partition the items into splits. The other two, <code>glb.fa</code> and <code>glb.algebraic</code>, are alternative ways of weighting the diagonal of the matrix. 
</p>
<p><code>glb.fa</code> estimates the communalities of the variables from a factor model where the number of factors is the number with positive eigen values.  Then reliability is found by 
</p>
<p style="text-align: center;"><code class="reqn">
glb = 1 - \frac{\sum e_j^2}{V_x} = 1 - \frac{\sum(1- h^2)}{V_x}
</code>
</p>

<p>This estimate will differ slightly from that found by  <code>glb.algebraic</code>, written by Andreas Moeltner which uses calls to  csdp in the Rcsdp package. His algorithm, which more closely matches the description of the glb by Jackson and Woodhouse, seems to have a positive bias (i.e., will over estimate the reliability of some items; they are said to be = 1) for small sample sizes.  More exploration of these two algorithms is underway. 
</p>
<p>Compared to <code>glb.algebraic</code>, <code>glb.fa</code> seems to have less (positive) bias for smallish sample sizes (n &lt; 500) but larger for large (&gt; 1000) sample sizes. This interacts with the number of variables so that equal bias sample size differs as a function of the number of variables.  The differences are, however small. As samples sizes grow,  <code>glb.algebraic</code> seems to converge on the population value while glb.fa has a positive bias. 
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>The worst split half reliability. This is an estimate of the general factor saturation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxrb</code></td>
<td>
<p>The maximimum split half reliability.  This is Guttman's lambda 4</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Also known as Guttman's Lambda 3</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ci</code></td>
<td>
<p>The 2.5%, 50%, and 97.5%  values of the raw or sampled split half.  Note that it necessary to specify raw=TRUE to get these.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tenberge$mu1</code></td>
<td>
<p>tenBerge mu 1 is functionally alpha</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tenberge$mu2</code></td>
<td>
<p>one of the sequence of estimates mu1 ... mu3</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>glb</code></td>
<td>
<p>glb found from factor analysis</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p> William Revelle </p>


<h3>References</h3>

<p>Cronbach, L.J. (1951) Coefficient alpha and the internal strucuture of tests.  Psychometrika, 16, 297-334.
</p>
<p>Guttman, L. (1945). A basis for analyzing test-retest reliability. Psychometrika, 10 (4), 255-282. 
</p>
<p>Revelle, W. (1979). Hierarchical cluster-analysis and the internal structure of tests. Multivariate Behavioral Research, 14 (1), 57-74. 
</p>
<p>Revelle, W. and Condon, D.M. (2019) Reliability from alpha to omega: A tutorial.  Psychological Assessment, 31, 12, 1395-1411. DOI: 10.1037/pas0000754.  
<a href="https://osf.io/preprints/psyarxiv/2y3w9">https://osf.io/preprints/psyarxiv/2y3w9</a> Preprint available from PsyArxiv 
</p>
<p>Revelle, W. and Zinbarg, R. E. (2009) Coefficients alpha, beta, omega and the glb: comments on Sijtsma. Psychometrika, 2009. 
</p>
<p>Ten Berge, J. M. F., &amp; Zegers, F. E. (1978). A series of lower bounds to the reliability of a test. Psychometrika, 43 (4), 575-579. 
</p>
<p>Zinbarg, R. E., Revelle, W., Yovel, I., &amp; Li, W. (2005). Cronbach's <code class="reqn">\alpha</code> , Revelle's <code class="reqn">\beta</code> , and McDonald's <code class="reqn">\omega_h</code> ): Their relations with each other and two alternative conceptualizations of reliability. <a href="https://doi.org/10.1007/s11336-003-0974-7">doi:10.1007/s11336-003-0974-7</a> Psychometrika, 70 (1), 123-133.
</p>


<h3>See Also</h3>

  <p><code>reliability</code>,  <code>alpha</code>,  <code>omega</code>, 
<code>ICLUST</code>,  <code>unidim</code>,  <code>glb.algebraic</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(attitude)
splitHalf(attitude)
splitHalf(attitude,covar=TRUE) #do it on the covariances
temp &lt;- splitHalf(attitude,raw=TRUE)
temp$ci #to show the confidence intervals, you need to specify that raw=TRUE

glb(attitude)
glb.fa(attitude)
if(require(Rcsdp)) {glb.algebraic(cor(attitude)) }
guttman(attitude)

#to show the histogram of all possible splits for the ability test
#sp &lt;- splitHalf(psychTools::ability,raw=TRUE)  #this saves the results
#hist(sp$raw,breaks=101,ylab="SplitHalf reliability",main="SplitHalf 
#    reliabilities of a test with 16 ability items")
sp &lt;- splitHalf(bfi[1:10],key=c(1,9,10))


#An example of how split half does not estimate the amount of general factor variance
#When the 

F &lt;- matrix(c(rep(.8,3),rep(0,9),rep(.8,3),rep(0,9),rep(.8,3)),ncol=3)
R &lt;- F 
diag(R) &lt;- 1
sp &lt;- splitHalf(R)   #shows a worst split half of .25 
 #    which is clearly not the amount of general variance
sp
sp$minAB      #these equal size splits

#but  ICLUST gets it right
iclust(R,plot=FALSE)$reliability

</code></pre>


</div>