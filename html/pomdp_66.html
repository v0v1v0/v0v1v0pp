<div class="container">

<table style="width: 100%;"><tr>
<td>RussianTiger</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Russian Tiger Problem POMDP Specification</h2>

<h3>Description</h3>

<p>This is a variation of the Tiger Problem introduces in Cassandra et al (1994)
with an absorbing state after a door is opened.
</p>


<h3>Format</h3>

<p>An object of class POMDP.
</p>


<h3>Details</h3>

<p>The original Tiger problem is available as Tiger. The original problem is
an infinite-horizon problem, where when the agent opens a door then the
problem starts over. The infinite-horizon problem can be solved if
a discount factor <code class="reqn">\gamma &lt; 1</code> is used.
</p>
<p>The Russian Tiger problem uses no discounting, but instead
adds an absorbing state <code>done</code>  which is reached
after the agent opens a door. It adds the action <code>nothing</code> to indicate
that the agent does nothing. The <code>nothing</code> action is only available in the
state <code>done</code> indicated by a reward of <code>-Inf</code> from all after states. A new
observation <code>done</code> is only emitted by the state <code>done</code>. Also, the Russian
tiger inflicts more pain with a negative reward of -1000.
</p>


<h3>See Also</h3>

<p>Other POMDP_examples: 
<code>POMDP()</code>,
<code>POMDP_example_files</code>,
<code>Tiger</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("RussianTiger")
RussianTiger

# states, actions, and observations
RussianTiger$states  
RussianTiger$actions 
RussianTiger$observations

# reward (-Inf indicates unavailable actions)
RussianTiger$reward

sapply(RussianTiger$states, FUN = function(s) actions(RussianTiger, s))

plot_transition_graph(RussianTiger, vertex.size = 30, edge.arrow.size = .3, margin = .5)

# absorbing states
absorbing_states(RussianTiger)

# solve the problem.
sol &lt;- solve_POMDP(RussianTiger)
policy(sol)
plot_policy_graph(sol)
</code></pre>


</div>