<div class="container">

<table style="width: 100%;"><tr>
<td>EPSGO</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>  Fits  SVM with variable selection using penalties. </h2>

<h3>Description</h3>

<p>Fits  SVM with feature selection  using penalties SCAD and 1 norm.
</p>


<h3>Usage</h3>

<pre><code class="language-R">EPSGO(Q.func, bounds,	parms.coding="none", fminlower=0, flag.find.one.min =FALSE,
			show=c("none", "final", "all"), N= NULL, maxevals = 500,  
	    pdf.name=NULL,  pdf.width=12,  pdf.height=12,   my.mfrow=c(1,1), 
	    verbose=TRUE, seed=123,  ...  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Q.func</code></td>
<td>
<p> name of the function to be  minimized. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bounds</code></td>
<td>
<p> bounds for parameters, see examples</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parms.coding</code></td>
<td>
<p> parmeters coding: none  or log2, default: none.  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fminlower</code></td>
<td>
<p> minimal value for the function Q.func, default is 0.     </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>flag.find.one.min</code></td>
<td>
<p>  do you want to find one min value and stop? Default: FALSE </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show</code></td>
<td>
<p>  show plots of  DIRECT algorithm:    none, final iteration, all iterations. Default: none  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N</code></td>
<td>
<p> define the number of start points, see details. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxevals</code></td>
<td>
<p> the maximum number of DIRECT function evaluations, default: 500.   </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pdf.name</code></td>
<td>
<p>pdf name      </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pdf.width</code></td>
<td>
<p> default 12 </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pdf.height</code></td>
<td>
<p> default 12 </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>my.mfrow</code></td>
<td>
<p> default c(1,1) </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p> verbose? default TRUE. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p> seed </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p> additional argument(s) </p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>if the number of start points (N)  is not defined by the user, it will be defined dependent on the dimensionality of the parameter space.
N=10D+1, where  D is the number of parameters, but for high dimensional parameter space with more than 6 dimensions,  
the initial set is restricted to 65. However for one-dimensional parameter space the N is set to 21 due to stability reasons.
</p>
<p>The idea of EPSGO (Efficient Parameter Selection via Global Optimization): Beginning
from an intial Latin hypercube sampling containing N starting points we train
an Online GP, look for the point with the maximal expected 	improvement, sample there and update the Gaussian Process(GP). Thereby
it is not so important that GP really correctly 	models the error surface of the SVM in parameter space, but
that it can give a us information about potentially interesting 	points in parameter space where we should sample next.
We continue with sampling points until some convergence criterion is met.
</p>
<p>DIRECT is a sampling algorithm which requires no knowledge of the objective function gradient.
Instead, the algorithm samples points in the domain, and uses the information it has obtained to decide where to
search next. The DIRECT algorithm will globally converge to the maximal value of the objective function. The name
DIRECT comes from the shortening of the phrase 'DIviding RECTangles', which describes the way the algorithm moves
towards the optimum.  
</p>
<p>The code source was adopted from MATLAB originals, special thanks to Holger Froehlich.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>fmin </code></td>
<td>
<p>minimal value of Q.func on the interval defined by bounds. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xmin </code></td>
<td>
<p>coreesponding parameters for the minimum</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter </code></td>
<td>
<p>number of iterations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>neval </code></td>
<td>
<p>  number of visited points </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxevals </code></td>
<td>
<p>  the maximum number of DIRECT function evaluations </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed </code></td>
<td>
<p>  seed</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bounds</code></td>
<td>
<p> bounds for parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Q.func </code></td>
<td>
<p>  name of the function to be  minimized. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>points.fmin </code></td>
<td>
<p>  the set of points with the same fmin </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Xtrain </code></td>
<td>
<p>  visited points </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Ytrain </code></td>
<td>
<p>  the output of Q.func at visited points Xtrain </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gp.seed </code></td>
<td>
<p> seed for Gaussian Process </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model.list </code></td>
<td>
<p> detailed information of the search process </p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Natalia Becker<br><a href="mailto:natalie_becker@gmx.de">natalie_becker@gmx.de</a>
</p>


<h3>References</h3>

<p>Froehlich, H. and Zell, A. (2005) "Effcient parameter selection for support vector
machines in classification and regression via model-based global optimization"
<em>In Proc. Int. Joint Conf. Neural Networks,  1431-1438 </em>.
</p>


<h3>See Also</h3>

 <p><code>svmfs</code>   </p>


<h3>Examples</h3>

<pre><code class="language-R">
	
	seed &lt;- 123
			
	train&lt;-sim.data(n = 200, ng = 100, nsg = 10, corr=FALSE, seed=seed )
	print(str(train)) 
			
	Q.func&lt;- ".calc.scad"
	
	bounds=t(data.frame(log2lambda1=c(-10, 10)))
							colnames(bounds)&lt;-c("lower", "upper")	
			
	print("start interval search")
	# computation intensive; 
	# for demostration reasons only for the first 100 features 
	# and only for 10 iterations maxIter=10, default maxIter=700
	system.time(fit&lt;-EPSGO(Q.func, bounds=bounds, parms.coding="log2", fminlower=0, 
		 show='none', N=21,  maxevals=500, 
		 pdf.name=NULL,  seed=seed,  
		 verbose=FALSE,
		 # Q.func specific parameters:
		 x.svm=t(train$x)[,1:100], y.svm=train$y,
		 inner.val.method="cv",
		 cross.inner=5, maxIter=10 ))
									 
	print(paste("minimal 5-fold cv error:", fit$fmin, "by log2(lambda1)=", fit$xmin))
		
	print(" all lambdas with the same minimum? ")
	print(fit$ points.fmin) 
			
	print(paste(fit$neval, "visited points"))
			
			
	print(" overview: over all visitied points in tuning parameter space 
				with corresponding cv errors")
	print(data.frame(Xtrain=fit$Xtrain, cv.error=fit$Ytrain))

	# create  3 plots om one screen: 
	# 1st plot: distribution of initial points in tuning parameter space
	# 2nd plot: visited lambda points vs. cv errors
	# 3rd plot: the same as the 2nd plot, Ytrain.exclude points are excluded. 
	#    The value cv.error = 10^16 stays for the cv error for an empty model ! 
	.plot.EPSGO.parms (fit$Xtrain, fit$Ytrain,bound=bounds, 
				Ytrain.exclude=10^16, plot.name=NULL )
	 # end of \donttest
</code></pre>


</div>