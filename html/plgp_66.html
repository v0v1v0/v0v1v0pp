<div class="container">

<table style="width: 100%;"><tr>
<td>PL</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Particle Learning Skeleton Method
</h2>

<h3>Description</h3>

<p>Implements the Particle Learning sequential Monte Carlo
algorithm on the data sequence provided, using re-sample and
propagate steps
</p>


<h3>Usage</h3>

<pre><code class="language-R">PL(dstream, start, end, init, lpredprob, propagate, prior = NULL,
   addpall = NULL, params = NULL, save = NULL, P = 100,
   progress = 10, cont = FALSE, verb = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>dstream</code></td>
<td>

<p>function generating the data stream; for examples see <code>data.GP</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>start</code></td>
<td>

<p>a scalar <code>integer</code> specifying the starting “time”;
the data entry/sample where PL will start
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>end</code></td>
<td>

<p>a scalar <code>integer</code> specifying the ending “time”;
the data entry/sample where PL will stop
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>

<p>function used to initialize the particles at the start of PL;
for examples see <code>draw.GP</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lpredprob</code></td>
<td>

<p>function used to calculate the predictive probability of an
observation (usually the next one in “time”) given a
particle.  This is the primary function used in the PL re-sample
step; for examples see <code>lpredprob.GP</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>propagate</code></td>
<td>

<p>function used to propagate particles given an observation (usually
the next one in “time”); for examples see
<code>propagate.GP</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>

<p>function used to generate prior parameters that may be
passed into the <code>dstream</code>, <code>init</code>,
<code>lpredprob</code> and <code>propagate</code>
functions as needed; for examples see <code>prior.GP</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>addpall</code></td>
<td>

<p>an optional function that adds the new observation (usually
the next one in “time”) to the  <code>pall</code> variable
in the <code>PL.env</code> environment (i.e., <code>PL.env$pall</code>),
which stores the sufficient information shared by all particles;
for examples see <code>addpall.GP</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>

<p>an optional function called each <code>progress</code> rounds
that collects parameters from the particles for
summary and visualization; for examples see <code>params.GP</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save</code></td>
<td>

<p>an option function that is called every round to save some
information about the particles
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>P</code></td>
<td>

<p>number of particles to use
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>progress</code></td>
<td>

<p>number of PL rounds after which to collect <code>params</code> and
draws histograms; a non-positive value or <code>params = NULL</code>
skips the progress meter
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cont</code></td>
<td>

<p>if <code>TRUE</code> then PL will try to use the existing set of particles
to “continue” where it left off; <code>start</code> and <code>end</code>
should be specified appropriately when continuing
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>

<p>if nonzero, then screen prints will indicate the proportion of PL
updates finished so far; <code>verb = 1</code> will cause PL to pause on
<code>progress</code> drawings for inspection
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Uses the PL SMC algorithm via the functions provided.  This function
is just a skeleton framework.  The hard work is in specifying the
arguments/functions which execute the calculations needed in the
re-sample and propagate steps.
</p>
<p>PL and uses the variables stored in the <code>PL.env</code> environment:
<code>pall</code>, containing
sufficient information common to all particles, <code>peach</code>,
containing sufficient information particular to each of the <code>P</code>
particles, and <code>psave</code> containing any saved information.  
These variables may be accessed as <code>PL.env$psave</code>, for example.
</p>
<p>Note that PL is designed to be fast for sequential updating
(of GPs) when new data arrive.  This facilitates efficient sequential
design of experiments by active learning techniques, e.g.,
optimization by expected improvement and sequential exploration of
classification label boundaries by the predictive entropy.  PL is not
optimized for static inference when all of the data arrive at once,
in batch
</p>


<h3>Value</h3>

<p>PL modifies the <code>PL.env$peach</code> variable, containing sufficient
information particular to each (of the <code>P</code>) particles
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Carvalho, C., Johannes, M., Lopes, H., and Polson, N. (2008).
“Particle Learning and Smoothing.”
Discussion Paper 2008-32, Duke University Dept. of Statistical
Science.
</p>
<p>Gramacy, R. and Polson, N. (2011).
“Particle learning of Gaussian process models for
sequential design and optimization.”
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
“Optimization under unknown constraints”.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
“Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences”.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code>papply</code>, <code>draw.GP</code>,
<code>data.GP</code>, <code>lpredprob.GP</code>,
<code>propagate.GP</code>, <code>params.GP</code>,
<code>pred.GP</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## See the demos via demo(package="plgp"); it is important to
## run them with the ask=FALSE argument so that the
## automatically generated plots may refresh automatically
## (without requiring the user to press RETURN)
## Not run: 
## Illustrates regression GPs on a simple 1-d sinusoidal
## data generating mechanism
demo("plgp_sin1d", ask=FALSE)

## Illustrates classification GPs on a simple 2-d exponential
## data generating mechanism
demo("plcgp_exp", ask=FALSE)

## Illustrates classification GPs on Ripley's Cushings data
demo("plcgp_cush", ask=FALSE)

## Illustrates active learning via the expected improvement
## statistic on a simple 1-d data generating mechanism
demo("plgp_exp_ei", ask=FALSE)

## Illustrates active learning via entropy with classification
## GPs on a simple 2-d exponential data generating mechanism
demo("plcgp_exp_entropy", ask=FALSE)

## Illustrates active learning via the integrated expected
## conditional improvement statistic for optimization
## under known constraints on a simple 1-d data generating
## mechanism
demo("plgp_1d_ieci", ask=FALSE)

## Illustrates active learning via the integrated expected
## conditional improvement statistic for optimization under
## unknown constraints on a simple 1-d data generating
## mechanism
demo("plconstgp_1d_ieci", ask=FALSE)

## Illustrates active learning via the integrated expected
## conditional improvement statistic for optimization under
## unknokn constraints on a simple 2-d data generating
## mechanism
demo("plconstgp_2d_ieci", ask=FALSE)

## End(Not run)
</code></pre>


</div>