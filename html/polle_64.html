<div class="container">

<table style="width: 100%;"><tr>
<td>q_model</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>q_model class object</h2>

<h3>Description</h3>

<p>Use <code>q_glm()</code>, <code>q_glmnet()</code>, <code>q_rf()</code>, and <code>q_sl()</code> to construct
an outcome regression model/Q-model object.
The constructors are used as input for <code>policy_eval()</code> and <code>policy_learn()</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">q_glm(
  formula = ~A * .,
  family = gaussian(),
  model = FALSE,
  na.action = na.pass,
  ...
)

q_glmnet(
  formula = ~A * .,
  family = "gaussian",
  alpha = 1,
  s = "lambda.min",
  ...
)

q_rf(
  formula = ~.,
  num.trees = c(250, 500, 750),
  mtry = NULL,
  cv_args = list(nfolds = 3, rep = 1),
  ...
)

q_sl(
  formula = ~.,
  SL.library = c("SL.mean", "SL.glm"),
  env = as.environment("package:SuperLearner"),
  onlySL = TRUE,
  discreteSL = FALSE,
  ...
)

q_xgboost(
  formula = ~.,
  objective = "reg:squarederror",
  params = list(),
  nrounds,
  max_depth = 6,
  eta = 0.3,
  nthread = 1,
  cv_args = list(nfolds = 3, rep = 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>An object of class formula specifying the design matrix for
the outcome regression model/Q-model at the given stage. The action at the
given stage is always denoted 'A', see examples. Use
<code>get_history_names()</code> to see the additional
available variable names.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>A description of the error distribution and link function to
be used in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>(Only used by <code>q_glm</code>) If <code>FALSE</code> model frame will
not be saved.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>(Only used by <code>q_glm</code>) A function which indicates what
should happen when the data contain NAs, see na.pass.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments passed to <code>glm()</code>, glmnet::glmnet,
ranger::ranger or SuperLearner::SuperLearner.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>(Only used by <code>q_glmnet</code>) The elasticnet mixing parameter
between 0 and 1. alpha equal to 1 is the lasso penalty, and alpha equal
to 0 the ridge penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p>(Only used by <code>q_glmnet</code>) Value(s) of the penalty parameter
lambda at which predictions are required, see <code>glmnet::predict.glmnet()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.trees</code></td>
<td>
<p>(Only used by <code>q_rf</code>) Number of trees.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtry</code></td>
<td>
<p>(Only used by <code>q_rf</code>) Number of variables to possibly split
at in each node.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv_args</code></td>
<td>
<p>(Only used by <code>q_rf</code>) Cross-validation parameters.
Only used if multiple hyper-parameters are given. <code>K</code> is the number
of folds and
<code>rep</code> is the number of replications.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>SL.library</code></td>
<td>
<p>(Only used by <code>q_sl</code>) Either a character vector of
prediction algorithms or a list containing character vectors,
see SuperLearner::SuperLearner.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>env</code></td>
<td>
<p>(Only used by <code>q_sl</code>) Environment containing the learner
functions. Defaults to the calling environment.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>onlySL</code></td>
<td>
<p>(Only used by <code>q_sl</code>) Logical. If TRUE, only saves and computes predictions
for algorithms with non-zero coefficients in the super learner object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>discreteSL</code></td>
<td>
<p>(Only used by <code>q_sl</code>) If TRUE, select the model with
the lowest cross-validated risk.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>objective</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) specify the learning
task and the corresponding learning objective, see xgboost::xgboost.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) list of parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nrounds</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) max number of boosting iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_depth</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) maximum depth of a tree.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) learning rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nthread</code></td>
<td>
<p>(Only used by <code>q_xgboost</code>) number of threads.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>q_glm()</code> is a wrapper of <code>glm()</code> (generalized linear model).<br><code>q_glmnet()</code> is a wrapper of <code>glmnet::glmnet()</code> (generalized linear model via
penalized maximum likelihood).<br><code>q_rf()</code> is a wrapper of <code>ranger::ranger()</code> (random forest).
When multiple hyper-parameters are given, the
model with the lowest cross-validation error is selected.<br><code>q_sl()</code> is a wrapper of SuperLearner::SuperLearner (ensemble model).
<code>q_xgboost()</code> is a wrapper of xgboost::xgboost.
</p>


<h3>Value</h3>

<p>q_model object: function with arguments 'AH'
(combined action and history matrix) and 'V_res' (residual value/expected
utility).
</p>


<h3>See Also</h3>

<p><code>get_history_names()</code>, <code>get_q_functions()</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library("polle")
### Single stage case
d1 &lt;- sim_single_stage(5e2, seed=1)
pd1 &lt;- policy_data(d1,
                   action="A",
                   covariates=list("Z", "B", "L"),
                   utility="U")
pd1

# available history variable names for the outcome regression:
get_history_names(pd1)

# evaluating the static policy a=1 using inverse
# propensity weighting based on the given Q-model:
pe1 &lt;- policy_eval(type = "or",
                   policy_data = pd1,
                   policy = policy_def(1, name = "A=1"),
                   q_model = q_glm(formula = ~A*.))
pe1

# getting the fitted Q-function values
head(predict(get_q_functions(pe1), pd1))

### Two stages:
d2 &lt;- sim_two_stage(5e2, seed=1)
pd2 &lt;- policy_data(d2,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd2

# available full history variable names at each stage:
get_history_names(pd2, stage = 1)
get_history_names(pd2, stage = 2)

# evaluating the static policy a=1 using outcome
# regression based on a glm model for each stage:
pe2 &lt;- policy_eval(type = "or",
            policy_data = pd2,
            policy = policy_def(1, reuse = TRUE, name = "A=1"),
            q_model = list(q_glm(~ A * L_1),
                           q_glm(~ A * (L_1 + L_2))),
            q_full_history = TRUE)
pe2

# getting the fitted Q-function values
head(predict(get_q_functions(pe2), pd2))
</code></pre>


</div>