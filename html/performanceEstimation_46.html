<div class="container">

<table style="width: 100%;"><tr>
<td>regressionMetrics</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Calculate some standard regression evaluation metrics of predictive performance
</h2>

<h3>Description</h3>

<p>This function is able to calculate a series of regression evaluation
statistics given two vectors: one with the true target variable values,
and the other with the predicted target variable values. Some of the
metrics may require additional information to be given (see Details section).		
</p>


<h3>Usage</h3>

<pre><code class="language-R">regressionMetrics(trues, preds, metrics = NULL, train.y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>trues</code></td>
<td>

<p>A numeric vector with the true values of the target variable.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>preds</code></td>
<td>

<p>A numeric vector with the predicted values of the target variable.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>

<p>A vector with the names of the evaluation statistics to
calculate (see Details section). If none is indicated (default) it will
calculate all available metrics of this function. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.y</code></td>
<td>

<p>An optional numeric vector with the values
of the target variable on the set of data used to obtain the model
whose performance is being tested.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>In the
following description of the currently available metrics we denote the
vector of true target variable values as t, the vector of predictions
by p, while N denotes the size of these two vectors, i.e. the number
of test cases.
</p>
<p>The regression evaluation statistics calculated by this function belong
to two different groups of measures: absolute and relative. In terms of
absolute error metrics the function includes currently the following:
</p>
<p>"mae": mean absolute error, which is calculated as sum(|t_i - p_i|)/N
</p>
<p>"mse": mean squared error, which is calculated as sum( (t_i - p_i)^2
)/N
</p>
<p>"rmse": root mean squared error that is calculated as sqrt(mse)
</p>
<p>The remaining measures ("mape", "nmse", "nmae" and "theil") are relative
measures, the three later
comparing the performance of the model with a baseline. They are
unit-less measures with values always greater than 0. In the case of
"nmse", "nmae" and "theil" the values are expected to be in the interval [0,1]
though occasionaly scores can overcome 1, which means that your model
is performing worse than the baseline model. The baseline used in both
"nmse" and "nmae" is a constant model that always predicts the average
target variable value, estimated using the values of this variable on
the training data (data used to obtain the model that generated the
predictions), which should be 
provided in the parameter <code>train.y</code>. The "theil" metric is
typically used in time series tasks and the used baseline is the last
observed value of the target variable. The relative error measure
"mape" does not require a baseline. It simply calculates the average
percentage difference between the true values and the
predictions.
</p>
<p>These measures are calculated as follows:
</p>
<p>"mape": sum(|(t_i - p_i) / t_i|)/N
</p>
<p>"nmse": sum( (t_i - p_i)^2 ) / sum( (t_i - AVG(Y))^2 ), where AVG(Y)
is the average of the values provided in vector <code>train.y</code>
</p>
<p>"nmae": sum(|t_i - p_i|) / sum(|t_i - AVG(Y)|)
</p>
<p>"theil": sum( (t_i - p_i)^2 ) / sum( (t_i - t_{i-1})^2 ), where
t_{i-1} is the last observed value of the target variable
</p>
<p>The user may also indicate the value "all" in the parameter
<code>metrics</code>. In this case all possible metrics will be
calculated. This will only include the "nmse", "nmae" and "theil" metrics if
the value of the <code>train.y</code> parameter is set, otherwise only the
other metrics will be returned.
</p>


<h3>Value</h3>

<p>A named vector with the calculated evaluation scores.
</p>


<h3>Note</h3>

<p>In case you require either "nmse", "nmae" or "theil" to be calculated you must
supply a vector of numeric values through the parameter
<code>train.y</code>, otherwise the function will return an error
message. These values are required to obtain a fair baseline against
which your model predictions will be compared to. 
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code>classificationMetrics</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
## Calculating several statistics of a regression tree on the Swiss data
data(swiss)
idx &lt;- sample(1:nrow(swiss),as.integer(0.7*nrow(swiss)))
train &lt;- swiss[idx,]
test &lt;- swiss[-idx,]
library(rpart)
model &lt;- rpart(Infant.Mortality ~ .,train)
preds &lt;- predict(model,test)
## by default only mse is calculated
regressionMetrics(test[,'Infant.Mortality'],preds)
## calculate mae and rmse
regressionMetrics(test[,'Infant.Mortality'],preds,metrics=c('mae','rmse'))
## calculate all statistics
regressionMetrics(test[,'Infant.Mortality'],preds,train.y=train[,'Infant.Mortality'])

## End(Not run)
</code></pre>


</div>