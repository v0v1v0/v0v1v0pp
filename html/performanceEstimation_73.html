<div class="container">

<table style="width: 100%;"><tr>
<td>topPerformers</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Obtain the best scores from a performance estimation experiment
</h2>

<h3>Description</h3>

<p>This function can be used to obtain the names of the workflows that obtained
the best scores (the top performers) on an experimental
comparison. This information will  be shown for each of the evaluation
metrics involved in the  comparison and also for all predictive tasks
that were used. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">topPerformers(compRes,
           maxs=rep(FALSE,dim(compRes[[1]][[1]]@iterationsScores)[2]),
           stat="avg",digs=3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>compRes</code></td>
<td>

<p>A <code>ComparisonResults</code> object with the results of your experimental comparison.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxs</code></td>
<td>

<p>A vector of booleans with as many elements are there are metrics estimated in
the experimental comparison. A <code>TRUE</code> value means the respective
statistic is to be maximized, while a <code>FALSE</code> means
minimization. Defaults to all <code>FALSE</code> values, i.e. all metrics are to
be minimized.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stat</code></td>
<td>

<p>The statistic to be used to obtain the ranks. The options are the
statistics produced by the function <code>summary</code> applied to objects
of class  <code>ComparisonResults</code>, i.e. "avg", "std",
"med", "iqr", "min", "max" or "invalid" (defaults to "avg").
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>digs</code></td>
<td>

<p>The number of digits (defaults to 3) used in the scores column of the results.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This is an utility function to check which were the top performers in a
comparative experiment for each data set and each evaluation
metric. The notion of <em>best performance</em> depends on the type of
evaluation metric, thus the need for the second argument. Some
evaluation statistics are to be maximized (e.g. accuracy), while
others are to be minimized (e.g. mean squared error). If you have a
mix of these types on your experiment then you can use the <code>maxs</code>
parameter to inform the function of which are to be maximized and
minimized. 
</p>


<h3>Value</h3>

<p>The function returns a list with named components. The components
correspond to the predictive tasks  used in the experimental comparison. For
each component you get a <code>data.frame</code>, where the rows represent the
statistics. For each statistic you get the name of the top performer
(1st column of the data frame) and the respective score on that
statistic (2nd column).
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code>performanceEstimation</code>,
<code>topPerformer</code>,
<code>rankWorkflows</code>,
<code>metricsSummary</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
## Estimating several evaluation metrics on different variants of a
## regression tree and of a SVM, on  two data sets, using one repetition
## of  10-fold CV

data(swiss)
data(mtcars)
library(e1071)

## run the experimental comparison
results &lt;- performanceEstimation(
               c(PredTask(Infant.Mortality ~ ., swiss),
                 PredTask(mpg ~ ., mtcars)),
               c(workflowVariants(learner='svm',
                                  learner.pars=list(cost=c(1,5),gamma=c(0.1,0.01))
                                 )
               ),
               EstimationTask(metrics=c("mse","mae"),method=CV(nReps=2,nFolds=5))
                                 )
## get the top performers for each task and evaluation metric
topPerformers(results)

## End(Not run)
</code></pre>


</div>