<div class="container">

<table style="width: 100%;"><tr>
<td>ridge</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Ridge regression with penalty parameter selection</h2>

<h3>Description</h3>

<p>Fit ridge regression models and select the penalty parameter by estimating
the respective prediction error via (repeated) <code class="reqn">K</code>-fold
cross-validation, (repeated) random splitting (also known as random
subsampling or Monte Carlo cross-validation), or the bootstrap.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ridge(
  x,
  y,
  lambda,
  standardize = TRUE,
  intercept = TRUE,
  splits = foldControl(),
  cost = rmspe,
  selectBest = c("hastie", "min"),
  seFactor = 1,
  ncores = 1,
  cl = NULL,
  seed = NULL,
  ...
)

ridge.fit(x, y, lambda, standardize = TRUE, intercept = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric matrix containing the predictor variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a numeric vector containing the response variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>a numeric vector of non-negative values to be used as penalty
parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>a logical indicating whether the predictor variables
should be standardized to have unit variance (the default is <code>TRUE</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>a logical indicating whether a constant term should be
included in the model (the default is <code>TRUE</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>splits</code></td>
<td>
<p>an object giving data splits to be used for prediction error
estimation (see <code>perryTuning</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cost</code></td>
<td>
<p>a cost function measuring prediction loss (see
<code>perryTuning</code> for some requirements).  The
default is to use the root mean squared prediction error (see
<code>cost</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>selectBest, seFactor</code></td>
<td>
<p>arguments specifying a criterion for selecting
the best model (see <code>perryTuning</code>).  The default is to
use a one-standard-error rule.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncores, cl</code></td>
<td>
<p>arguments for parallel computing (see
<code>perryTuning</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>optional initial seed for the random number generator (see
<code>.Random.seed</code> and <code>perryTuning</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>for <code>ridge</code>, additional arguments to be passed to the
prediction loss function <code>cost</code>.  For <code>ridge.fit</code>, additional
arguments are currently ignored.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>For <code>ridge</code>, an object of class <code>"perryTuning"</code>, see
<code>perryTuning</code>).  It contains information on the
prediction error criterion, and includes the final model with the optimal
tuning paramter as component <code>finalModel</code>.
</p>
<p>For <code>ridge.fit</code>, an object of class <code>ridge</code> with the following
components:
</p>

<dl>
<dt><code>lambda</code></dt>
<dd>
<p>a numeric vector containing the values of the penalty
parameter.</p>
</dd>
<dt><code>coefficients</code></dt>
<dd>
<p>a numeric vector or matrix containing the
coefficient estimates.</p>
</dd>
<dt><code>fitted.values</code></dt>
<dd>
<p>a numeric vector or matrix containing the
fitted values.</p>
</dd>
<dt><code>residuals</code></dt>
<dd>
<p>a numeric vector or matrix containing the
residuals.</p>
</dd>
<dt><code>standardize</code></dt>
<dd>
<p>a logical indicating whether the predictor
variables were standardized to have unit variance.</p>
</dd>
<dt><code>intercept</code></dt>
<dd>
<p>a logical indicating whether the model includes a
constant term.</p>
</dd>
<dt><code>muX</code></dt>
<dd>
<p>a numeric vector containing the means of the predictors.</p>
</dd>
<dt><code>sigmaX</code></dt>
<dd>
<p>a numeric vector containing the standard deviations
of the predictors.</p>
</dd>
<dt><code>muY</code></dt>
<dd>
<p>numeric; the mean of the response.</p>
</dd>
<dt><code>call</code></dt>
<dd>
<p>the matched function call.</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Andreas Alfons
</p>


<h3>References</h3>

<p>Hoerl, A.E. and Kennard, R.W. (1970) Ridge regression: biased estimation for
nonorthogonal problems.  <em>Technometrics</em>, <b>12</b>(1), 55â€“67.
</p>


<h3>See Also</h3>

<p><code>perryTuning</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## load data
data("Bundesliga")
Bundesliga &lt;- Bundesliga[, -(1:2)]
f &lt;- log(MarketValue) ~ Age + I(Age^2) + .
mf &lt;- model.frame(f, data=Bundesliga)
x &lt;- model.matrix(terms(mf), mf)[, -1]
y &lt;- model.response(mf)

## set up repeated random splits
splits &lt;- splitControl(m = 40, R = 10)

## select optimal penalty parameter
lambda &lt;- seq(600, 0, length.out = 50)
fit &lt;- ridge(x, y, lambda = lambda, splits = splits, seed = 2014)
fit

## plot prediction error results
plot(fit, method = "line")
</code></pre>


</div>