<div class="container">

<table style="width: 100%;"><tr>
<td>ppclust-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Probabilistic and Possibilistic Cluster Analysis</h2>

<h3>Description</h3>

<p>Partitioning clustering simply divides the objects in a data set into non-overlapping subsets or clusters by using the prototype-based probabilistic and possibilistic clustering algorithms. The package covers various functions for K-Means (MacQueen, 1967), Fuzzy C-Means (Bezdek, 1974), Possibilitic C-Means (Krishnapuram &amp; Keller, 1993;1996), Possibilistic and Fuzzy C-Means (Pal et al, 2005), Possibilistic Clustering Algorithm (Yang et al, 2006), Possibilistic C-Means with Repulsion (Wachs et al, 2006), Unsupervised Possibilistic Fuzzy C-Means (Wu et al, 2010) and the other variant algorithms which produce hard, fuzzy and possibilistic partitions of numeric data sets. The cluster prototypes and membership matrices required by the partitioning algorithms can be initialized with many initialization techniques that are available in the package ‘<span class="pkg">inaparc</span>’. As the distance metrics, not only the Euclidean distance but also a set of the commonly used distance metrics are available to use with some of the algorithms in the package.
</p>


<h3>Details</h3>

<p>The goal of prototype-based algorithms as the most widely-used group of partitioning clustering algorithms is to partition a data set of <em>n</em> objects with <em>p</em> features into <em>k</em>, a pre-defined number of clusters which are the non-hierchical subsets of data. On the clustering context, a prototype is a data item that represents or characterizes a cluster. Usually, a prototype can be regarded as the most central point in a data subspace (Tan et al. 2006). 
</p>
<p>Among the partitioning-based algorithms, the hard clustering algorithms, i.e. K-means, assume that each data point belongs to one cluster;  however, in practice clusters may overlap and data points may belong to more than one cluster.  In this case the membership degrees of a data point to clusters should be a value between zero and one. This idea has been modelled with the fuzzy clustering algorithms. The Fuzzy C-means proposed by Bezdek (FCM) (Bezdek, 1981), is the well-known partitioning based fuzzy algorithm. It assigns a fuzzy membership degree to each data point based on its distances to the cluster centers. If a data point is closer to a cluster center, its membership to this cluster be higher than its memberships to the other clusters. 
</p>
<p>As an extension of the basic FCM algorithm, Gustafson and Kessel (GK) clustering algorithm employs an adaptive distance norm in order to detect clusters with different geometrical shapes (Babuska, 2001; Correa et al, 2011). Gath and Geva (1989) revealed that the fuzzy maximum likelihood estimates (FMLE) clustering algorithm can be used to detect clusters of varying shapes, sizes and densities. 
</p>
<p>In the related literature it has been revealed that FCM is sensitive to noise and outliers in the data sets. Krishnapuram and Keller proposed and later improved the Possibilistic C-means (PCM) algorithm in order to avoid the effect of outliers or noises on the clustering performance (Krishnapuram &amp; Keller, 1993;1996). Althouh PCM solves the problems due to noise and outliers by relaxing the probabilistic constraint of FCM, it has the disadvantage to  generate coincident clusters with poor initializations. In order to overcome this problem, several variants of FCM and PCM have been proposed. Fuzzy Possibilistic C-means (FPCM) (Pal et al, 1997) is one of the mixed algorithms for simultaneously constructing memberships and typicalities. However FPCM has some problems because of the row sum constraint with the typicality values that produces unrealistic typicality values for large data sets. By adding a repulsion term forcing clusters far away from each other, an extension of PCM with repulsion has been introduced by Timm et al (2004). Pal et al (2005) proposed the Possibilistic Fuzzy C-means (PFCM) to overcome the noise sensitivity defect of FCM, to eliminate the coincident clusters problem of PCM and to eliminate the row sum constraints of FPCM.
</p>
<p>Possibilistic Clustering Algorithm (PCA) proposed by Yang and Wu (2006) was in the front lines of another direction of algorithms improving FCM and PCM. The authors argued that the resulting membership of their proposed algorithm becomes an exponential function, so that it is robust to noise and outliers. Recently, Wu et al (2010) introduced the Unsupervised Possibilistic Fuzzy Clustering (UPFC) algorithm. UPFC is an extension of PCA by combining FCM and PCA algorithms in order to overcome the noise sensitivity problem of FCM and the coincident clusters problem of PCM. When compared to previous algorithms, UPFC seems promising algorithm for clustering in fuzzy and possibilistic environments since it needs not a probabilistic initialization.
</p>


<h3>Basic Notations</h3>

<p>Given a data set <code class="reqn">\mathbf{X}</code> describing <code class="reqn">n</code> data objects, the probabilistic and possibilistic clustering algorithms partition data into <code class="reqn">k</code>, a predefined number of clusters through the minimization of their related objective functions with some probabilistic or possibilistic constraints. 
</p>
<p><code class="reqn">\mathbf{X} = \{\vec{x}_1, \vec{x}_2,\dots, \vec{x}_n\} \subseteq \Re^p</code> is the data set for <code class="reqn">n</code> objects in the <em>p</em>-dimensional data space <code class="reqn">\Re</code>, 
where:
</p>

<ul>
<li> <p><code class="reqn">n</code> is the number of objects in the data set, <code class="reqn">1\leq n\leq \infty</code> 
</p>
</li>
<li> <p><code class="reqn">p</code> is the number of features or variables which describes the data objects,
</p>
</li>
<li> <p><code class="reqn">\vec{x}_i</code> is <em>p</em>-length data vector for the i<em>th</em> data object. 
</p>
</li>
</ul>
<p>On the clustering context, clusters are mostly represented by their prototypes. The prototypes are generally the centers of clusters which can be either centroids or medoids. The probabilistic and possibilistic partitioning clustering algorithms start with initialization of a cluster prototype matrix <code class="reqn">\mathbf{V}</code>, and updates it through the iteration steps until it is stabilized. 
</p>
<p><code class="reqn">\mathbf{V} = \{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k\} \subseteq\Re^n</code> is the protoype matrix of the clusters, where:
</p>

<ul>
<li> <p><code class="reqn">k</code> is the number of clusters, <code class="reqn">1\leq k\leq n</code> 
</p>
</li>
<li> <p><code class="reqn">\vec{v}_j</code> is the <em>p</em>-length prototype vector for the j<em>th</em> cluster
</p>
</li>
</ul>
<p>The clustering algorithms compute the membership degrees of data objects by using some distance metrics for calculation of their proximities to the cluster centers. 
</p>
<p><code class="reqn">d(\vec{x}_i, \vec{v}_j)</code> is the distance measure between the data object <code class="reqn">\vec{x}_i</code> and cluster prototype <code class="reqn">\vec{v}_j</code>. In general, the squared Euclidean distance metric are used in most of the applications: 
</p>
<p><code class="reqn">d_{sq.euclidean}(\vec{x}_i, \vec{v}_j) = d^2(\vec{x}_i, \vec{v}_j) = \mid\mid \vec{x}_i - \vec{v}_j\mid \mid^2 \; = \; (\vec{x}_i - \vec{v}_j)^T \cdot (\vec{x}_i - \vec{v}_j)</code> 
</p>
<p>The clustering algorithms usually are run with the standard and squared Euclidean distance norms, which induce hyperspherical clusters. Therefore they are able find the clusters with the same shape and orientation because the norm inducing matrix is an identity matrix <code class="reqn">\mathbf{A} = \mathbf{I}</code>. On the other hand, the distance metrics can be employed with a <code class="reqn">n \times n</code> diagonal norm inducing matrix <code class="reqn">\mathbf{A} = \mathbf{I} \; 1/\sigma_j^2</code> which modifies the distances depending on the direction in which the distance is measured (Timm et al, 2004; Balasko et al 2005). In this case, the squared Euclidean distance with the norm matrix <code class="reqn">\mathbf{A}</code> becomes:
</p>
<p><code class="reqn">d_{sq.euclidean}(\vec{x}_i, \vec{v}_j) = d_{A}^2(\vec{x}_i, \vec{v}_j) = \mid\mid \vec{x}_i - \vec{v}_j\mid \mid_A^2 \; = \; (\vec{x}_i - \vec{v}_j)^T \mathbf{A} (\vec{x}_i - \vec{v}_j)</code> 
</p>
<p><code class="reqn">\mathbf{A}</code> can be also formed as the inverse of the <code class="reqn">n \times n</code> covariance matrix <code class="reqn">\mathbf{F}</code>: 
</p>
<p><code class="reqn">\mathbf{A} = \mathbf{F}^{-1}</code>, where:
</p>
<p><code class="reqn">\mathbf{F} = \frac{1}{n} \sum\limits_{i=1}^n (\vec{x}_i - \bar{x})^T (\vec{x}_i - \bar{x})</code>.  
</p>
<p>Where <code class="reqn">\bar{x}</code> stands for the sample mean of the data. When the distance is induced with the norm matrix <code class="reqn">\mathbf{A}</code> the Mahalanobis norm on <code class="reqn">\Re^n</code> can be written as follows: 
</p>
<p><code class="reqn">d_{mahalanobis}(\vec{x}_i, \vec{v}_j)_A = \; (\vec{x}_i - \vec{v}_j)^T \mathbf{A}_j (\vec{x}_i - \vec{v}_j)</code> 
</p>
<p>The membership degrees are the measures specifying the amount of belongingness of the data objects to the different clusters. A data point nearer to the center of a cluster has a higher degree of membership to this cluster. 
</p>
<p><code class="reqn">\mathbf{U} = [u_{ij}]</code> is the matrix for an hard, fuzzy or possibilistic partition of <code class="reqn">\mathbf{X}</code>, where:
</p>

<ul><li> <p><code class="reqn">u_{ij} = u_{i}(\vec{x}_{j})</code> is the membership degree of <code class="reqn">\vec{x}_i</code> to the j<em>th</em> cluster 
</p>
</li></ul>
<h3>Author(s)</h3>

<p>Zeynel Cebeci, Figen Yildiz, A. Tuna Kavlak, Cagatay Cebeci &amp; Hasan Onder</p>


<h3>References</h3>

<p>Babuska, R. (2001). Fuzzy and neural control. DISC Course Lecture Notes. Delft University of Technology. Delft, the Netherlands. &lt;<a href="https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control">https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control</a>&gt;.
</p>
<p>Balasko, B., Abonyi, J. &amp; Feil, B. (2005). Fuzzy clustering and data analysis toolbox. Department of Process Eng., Univ. of Veszprem, Veszprem. 
</p>
<p>Bezdek, J.C. (1981). <em>Pattern recognition with fuzzy objective function algorithms</em>. Plenum, NY. &lt;<a href="https://isbnsearch.org/isbn/0306406713">ISBN:0306406713</a>&gt;
</p>
<p>Cebeci, Z. &amp; Yildiz, F. (2015). Bulanik C-Ortalamalar algoritmasýnýn farklý kume buyuklukleri icin hesaplama performansi ve kumeleme gecerliliginin karsilastirilmasi, In Proc.pf <em>9. Ulusal Zootekni Bilim Kongresi</em>, Sep. 2015, Konya. pp. 227-239. <a href="https://doi.org/10.13140/RG.2.1.2909.9288">doi:10.13140/RG.2.1.2909.9288</a>
</p>
<p>Cebeci, Z., Kavlak, A.T. &amp; Yildiz, F. (2017). Validation of fuzzy and possibilistic clustering results, in Proc. of <em>2017 Int. Artificial Intelligence &amp; Data Processing Symposium</em>, IEEE. pp. 1-7. <a href="https://doi.org/10.1109/IDAP.2017.8090183">doi:10.1109/IDAP.2017.8090183</a>
</p>
<p>Cebeci, Z. (2018). Initialization of membership degree matrix for fast convergence of Fuzzy C-Means clustering, in Proc. of <em>2018 Int.Conf.on Artificial Intelligence &amp; Data Processing</em>, pp. 1-5. IEEE. <a href="https://doi.org/10.1109/IDAP.2018.8620920">doi:10.1109/IDAP.2018.8620920</a>
</p>
<p>Cebeci, Z. &amp; Cebeci, C. (2018) kpeaks: an R package for quick selection of k for cluster analysis, in Proc. of <em>2018 Int. Conf. on Artificial Intelligence &amp; Data Processing</em>, pp. 1-7, IEEE. <a href="https://doi.org/10.1109/IDAP.2018.8620896">doi:10.1109/IDAP.2018.8620896</a>
</p>
<p>Cebeci, Z. (2019). Comparison of internal validity indices for fuzzy clustering. <em>Journal of Agricultural Informatics</em>, 10(2):1-14. <a href="https://doi.org/10.17700/jai.2019.10.2.537">doi:10.17700/jai.2019.10.2.537</a>
</p>
<p>Correa, C., Valero, C., Barreiro, P., Diago, M. P., &amp; Tardáguila, J. (2011). A comparison of fuzzy clustering algorithms applied to feature extraction on vineyard. In <em>Proc. of the 14th Conf. of the Spanish Assoc. for Artificial Intelligence</em>. &lt;<a href="http://oa.upm.es/9246/">http://oa.upm.es/9246/</a>&gt;.
</p>
<p>Gath, I. &amp; Geva, A.B. (1989). Unsupervised optimal fuzzy clustering. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 11 (7): 773-781. &lt;doi:10.1109/34.192473&gt;
</p>
<p>Gustafson, D. E. &amp; Kessel, W. C. (1979). Fuzzy clustering with a fuzzy covariance matrix. In <em>Proc. of IEEE Conf. on Decision and Control including the 17th Symposium on Adaptive Processes</em>, San Diego. pp. 761-766. &lt;doi:10.1109/CDC.1978.268028&gt;
</p>
<p>Pal, N.R., Pal, K., &amp; Bezdek, J.C. (1997). A mixed c-means clustering model. In <em>Proc. of the 6th IEEE Int. Conf. on Fuzzy Systems</em>, 1, pp. 11-21. &lt;doi:10.1109/FUZZY.1997.616338&gt;
</p>
<p>Pal, N.R., Pal, S.K., Keller,J.M. &amp; Bezdek, J.C. (2005). A possibilistic fuzzy c-means clustering algorithm. <em>IEEE Transactions on Fuzzy Systems</em>, 13 (4): 517-530. &lt;doi: 10.1109/TFUZZ.2004.840099&gt;
</p>
<p>Tan, P. N., Steinbach, M., &amp; Kumar, V. (2006). Cluster analysis: Basic concepts and algorithms. In <em>Introduction to Data Mining</em>. Pearson Addison Wesley. &lt;<a href="http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf">http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf</a>&gt;
</p>
<p>Timm, H., Borgelt, C., Doring, C. &amp; Kruse, R. (2004). An extension to possibilistic fuzzy cluster analysis. <em>Fuzzy Sets and Systems</em>, 147 (1): 3-16. &lt;doi:10.1016/j.fss.2003.11.009&gt;
</p>
<p>Yang, M. S. &amp; Wu, K. L. (2006). Unsupervised possibilistic clustering. <em>Pattern Recognition</em>, 39(1): 5-21. &lt;doi:10.1016/j.patcog.2005.07.005&gt;
</p>
<p>Wu, X., Wu, B., Sun, J. &amp; Fu, H. (2010). Unsupervised possibilistic fuzzy clustering. <em>J. of Information &amp; Computational Sci.</em>, 7 (5): 1075-1080.
</p>


<h3>See Also</h3>

<p><code>as.ppclust</code>,
<code>comp.omega</code>,
<code>fcm</code>,
<code>fcm2</code>,
<code>fpcm</code>,
<code>fpppcm</code>,
<code>gg</code>,
<code>gk</code>,
<code>gkpfcm</code>,
<code>hcm</code>,
<code>is.ppclust</code>
<code>pca</code>,
<code>ppclust2</code>
<code>pcm</code>,
<code>pcmr</code>,
<code>pfcm</code>,
<code>summary.ppclust</code>,
<code>upfc</code>
</p>


</div>