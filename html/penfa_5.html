<div class="container">

<table style="width: 100%;"><tr>
<td>penfa</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Single- and multiple-group penalized factor analysis</h2>

<h3>Description</h3>

<p>The function <code>penfa</code> fits single- and multiple-group
<em>PENalized Factor Analysis</em> models via a trust-region algorithm with
integrated automatic multiple tuning parameter selection.
</p>
<p>In a single-group analysis, <code>penfa</code> can automatically shrink a subset
of the factor loadings to zero. In a multiple-group analysis, it can
encourage sparse loading matrices and invariant factor loadings and
intercepts. The currently supported penalty functions are lasso, adaptive
lasso, scad, mcp, and ridge. Except for the latter, all penalties can
achieve sparsity.
</p>


<h3>Usage</h3>

<pre><code class="language-R">penfa(
  model = NULL,
  data = NULL,
  group = NULL,
  pen.shrink = "alasso",
  pen.diff = "none",
  eta = list(shrink = c(lambda = 0.01), diff = c(none = 0)),
  strategy = "auto",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A description of a user-specified model. It takes the form of a
lavaan-like model syntax. See below for additional details on how to
specify a model syntax.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>A data frame containing the (continuous) observed variables used
in the model. Except for the <code>group</code> variable, all variables are
treated as numeric.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>group</code></td>
<td>
<p>Character. An optional variable name in the data frame defining
the groups in a multiple-group analysis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pen.shrink</code></td>
<td>
<p>Character. The type of penalty function used for shrinking
a subset of the model parameters (see the <code>eta</code> argument for details
on how to specify which model parameters shall be penalized). Possible
values for <code>pen.shrink</code> are "lasso", "alasso" (i.e., adaptive lasso),
"scad" (i.e., smoothly clipped absolute deviation), "mcp" (i.e., minimax
concave penalty), "ridge", and "none" in case of no shrinkage penalization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pen.diff</code></td>
<td>
<p>Character. The type of penalty function used for shrinking
certain parameter differences across groups, and thus encouraging parameter
equivalence across groups (see the <code>eta</code> argument for details on how
to specify which model parameters shall be encouraged to be equivalent).
Possible values for <code>pen.diff</code> are "lasso", "alasso" (i.e., adaptive
lasso), "scad" (i.e., smoothly clipped  absolute deviation), "mcp" (i.e.,
minimax concave penalty), "ridge", and "none" in case of no difference
penalization. Note that the specification of <code>pen.diff</code> is only valid
for multiple-group factor analyses when a <code>group</code> variable is defined.
If a difference penalty is requested, the groups must have the same
parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>A named list containing the starting value(s) of the tuning
parameter(s) if the automatic procedure is requested (<code>strategy =
  "auto"</code>) or the fixed value(s) of the tuning parameter(s) to be used during
optimization if <code>strategy = "fixed"</code>. The list has two components with
names "shrink" and "diff", which refer to the tuning parameters to be used
for shrinkage and group equivalence, respectively. The components of the
list are, in turn, named vectors specifying the type of parameter matrices
or vectors to be penalized. Common choices are "lambda" for the loading
matrix and "tau" for the intercept vector of the observed variables. Other
possible values are "phi" for the factor covariance matrix, "psi" for the
covariance matrix of the unique factors, and "kappa" for the factor means.
All non-fixed elements of the specified matrix/vector are penalized. When
<code>strategy = "fixed"</code> and the tuning values in <code>eta</code> are equal to
zero, specifying both list names as "none" results in ordinary maximum
likelihood estimation (no penalization).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>strategy</code></td>
<td>
<p>Character. The strategy used for the selection of the tuning
parameter(s). If <code>strategy = "auto"</code>, the optimal values of the tuning
parameters are determined via an automatic tuning parameter procedure; if
<code>strategy = "fixed"</code>, a penalized factor model with the values of the
tuning parameters stored in the option <code>eta</code> is estimated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional options that can be defined using <code>name =
"value"</code>. For a complete list, please refer to <code>penfaOptions</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object of class <code>penfa</code>, for which several
methods are available. See the manual pages of <code>summary,penfa-method</code>,
<code>show,penfa-method</code>, <code>coef,penfa-method</code>, and
<code>fitted,penfa-method</code> for details.
</p>


<h3>Data set vs Sample Moments</h3>

<p>The <code>penfa</code> function currently takes as input a data set, as opposed to
the sample moments (i.e., covariance matrices and mean vectors). Future
implementations will allow <code>penfa</code> to additionally take as input sample
covariance matrices and sample means. For now, if only sample moments are
available, users can generate multivariate data from those sample moments,
and apply the <code>penfa</code> function on the generated data.  <br> All variables
(except for the <code>group</code> variable in multiple-group analyses) are treated
as continuous. <br> Categorical items are not currently supported. <br></p>


<h3>Model syntax</h3>

<p>The model syntax in the <code>model</code> argument describes the factor analysis
model to be estimated, and specifies the relationships between the observed
and latent variables (i.e., the common factors). To facilitate its
formulation, the rules for the syntax specification broadly follow the ones in
the <a href="https://CRAN.R-project.org/package=lavaan"><code>lavaan</code></a> package.
</p>
<p>The model syntax is composed of one or multiple formula-like expressions
describing specific parts of the model. The model syntax can be specified as
a literal string enclosed by single quotes as in the example below.
</p>
<pre>model_syntax &lt;- '
                # Common factors
                factor1 =~ x1 + x2 + x3 + x4 + x5 + x6
                factor2 =~ x1 + x2 + x3 + x4 + x5 + x6

                # Factor variances and covariances
                factor1 ~~ factor1
                factor1 ~~ factor2

                # Unique variances and covariances
                x1 ~~ x1
                x1 ~~ x2

                # Intercepts and factor means
                x1 ~ 1
                factor1 ~ 1
                '
</pre>
<p>Blank lines and comments can be used in between formulas, and formulas can be
split over multiple lines. Multiple formulas can be placed on
a single line if they are separated by a semicolon (;).
</p>
<p>The current implementation allows for the following types of formula-like
expressions in the model syntax: </p>
 <ol>
<li>
<p> Common factors: The
<code>"=~"</code> operator can be used to define the continuous common factors
(latent variables). The name of the factor (e.g., factor1) is on the left of
the <code>"=~"</code> operator, whereas the terms on the right (e.g., <code>x1 + x2
+ x3 + x4 + x5 + x6</code>), separated by <code>"+"</code> operators, are the indicators
of the factor. The operator <code>"=~"</code> can be read as "is measured by".
</p>
</li>
<li>
<p> Variances and covariances: The <code>"~~"</code> ("double tilde") operator
specifies the (residual) variance of an observed or latent variable, or a set
of covariances between one variable, and several other variables (either
observed or latent). The distinction between variances and residual variances
is made automatically. Covariances between unique factors are currently only
allowed when <code>information = "fisher"</code>.
</p>
</li>
<li>
<p> Intercepts and factor means: We can specify an intercept for an
observed variable (<code>x1 ~ 1</code>) or a common factor (<code>factor1 ~ 1</code>).
The variable name appears on the left of the <code>"~"</code> operator. On the
right-hand side, there is the number "1", which stands for the
intercept/mean. Including an intercept/mean formula in the model
automatically implies <code>meanstructure = TRUE</code>. The distinction between
observed variable intercepts and factor means is made automatically.
</p>
</li>
</ol>
<p>Usually, only a single variable name appears on the left side of an operator.
However, if multiple variable names are specified, separated by the "+"
operator, the formula is repeated for each element on the left side. For
instance, the formula
</p>
<pre>
 x1 + x2 + x3 + x4 ~ 1</pre>
<p>specifies an intercept for variables <code>x1, x2, x3</code> and <code>x4</code>.
</p>
<p>On the right-hand side of these formula-like expressions, each element can be
modified (using the <code>"*"</code> operator) by a numeric constant or the special
function start(). This provides the user with a mechanism to fix
parameters and provide alternative starting values, respectively. All
<code>"*"</code> expressions are referred to as modifiers, and are explained in
detail in the sections below.
</p>
<p>Each parameter in a model is automatically given a name consisting of three
parts, that are coerced to a single character vector. The first part is the
name of the variable on the left-hand side of the formula where the parameter
is implied. The middle part is based on the special "operator" used in the
formula (e.g., <code>"=~"</code>, <code>"~"</code> or <code>"~~"</code>). The third part is the
name of the variable on the right-hand side of the formula where the
parameter is implied, or "1" if it is an intercept. The three parts are
pasted together in a single string. For example, the name of the factor
loading of <code>x2</code> on <code>factor1</code> is the string <code>"factor1~x2"</code>.
The name of the parameter corresponding to the factor covariance between
<code>factor1</code> and <code>factor2</code> is the string <code>"factor1~~factor2"</code>.
</p>


<h4>Fixing parameters</h4>

<p>It is often desirable to fix a model parameter that is otherwise (by default)
estimated. Any parameter in a model can be fixed by using a modifier
resulting in a numerical constant. For instance:
</p>

<ul>
<li>
<p> Fixing factor loadings for scale setting or identification
restrictions:
</p>
<pre>
factor1 ~ 0.8*x1 + x2 + x3 +   0*x4 + x5 + x6
factor2 ~   0*x1 + x2 + x3 + 0.8*x4 + x5 + x6</pre>
</li>
<li>
<p> Specifying an orthogonal (zero) covariance between two factors:
</p>
<pre>factor1 ~~ 0*factor2</pre>
</li>
</ul>
<p>Notice that multiplying a certain parameter by <code>NA</code> forces it to be
estimated.
</p>



<h4>Starting values</h4>

<p>User-defined starting values can be provided through the special function
start(), containing a numeric constant. For instance, the formula below
provides a starting value equal to 0.8 to the loading of <code>x2</code> on
<code>factor1</code>.
</p>
<pre>
factor1 ~ x1 + start(0.8)*x2 + x3 + x4 + x5 + x6</pre>



<h4>Multiple groups</h4>

<p>In a multiple group factor analysis, the modifiers containing a single element
should be replaced by a vector of the same length as the number of groups.
If a single element is provided, it is used for all groups. In the
example below with two groups, the factor loadings of <code>x1</code> on
<code>factor1</code> are fixed to 0.8 in both groups, whereas the factor loadings
of <code>x4</code> are fixed to 0.75 and 0.85 in the first and second group,
respectively.
</p>
<pre>
multigroup_syntax &lt;- '
 factor1 ~  0.8*x1 + x2 + x3 +               x4 + x5 + x6
 factor2 ~      x1 + x2 + x3 + c(0.75, 0.85)*x4 + x5 + x6 '</pre>



<h3>Algorithm</h3>

<p>Penalized factor analysis allows to produce parsimonious models using largely
an automated procedure. The use of sparsity-inducing penalty functions leads
to optimally sparse factor structures supported by the data. The resulting
models are less prone to instability in the estimation process and are easier
to interpret and generalize than their unpenalized counterparts.
Multiple-group penalized factor analysis can be used to automatically
ascertain the differences and similarities of parameter estimates across
groups.
</p>
<p>In <code>penfa</code>, estimation is achieved via a penalized likelihood-based
framework that builds upon differentiable approximations of
non-differentiable penalties, a theoretically founded definition of degrees
of freedom, and an algorithm with automatic multiple tuning parameter
selection (see section below for details).
</p>
<p>The <code>penfa</code> function uses a
<a href="https://CRAN.R-project.org/package=trust"><code>trust-region</code></a>
algorithm approach. This strategy constructs a model function whose behavior
near the current point and within a trust-region (usually a ball) is similar
to that of the actual objective function. The algorithm exploits second-order
analytical derivative information. This can come in the form of the penalized
Hessian matrix (if <code>information = "hessian"</code>) or the penalized Fisher
information matrix (if <code>information = "fisher"</code>). Models with a
mean structure can be only estimated with the penalized Fisher information
matrix, which exhibits similar performances to the penalized Hessian at a
reduced computational cost. <br></p>


<h3>Tuning parameter selection</h3>

<p>The selection of the tuning parameters is a crucial issue in penalized
estimation strategies, as the tuning parameters are responsible for the
optimal balance between goodness of fit and sparsity.
</p>


<h4>Automatic procedure</h4>

<p>The penalized framework discussed above is easily integrated with automatic
multiple tuning parameter selection (if <code>strategy = "auto"</code>). The tuning
parameters are chosen to minimize an approximate AIC. See below for
additional details on how to introduce more sparsity, if desired. The
automatic procedure is fast, efficient, and scales well with the number of
tuning parameters. It also eliminates the need for time-consuming and
computationally intensive grid-searches.
</p>
<p><strong>Note:</strong> Only lasso, adaptive lasso and ridge penalties can be used
with the automatic procedure.
</p>
<p>The automatic procedure returns the optimal value of the tuning parameter.
Notice, however, that the parameter estimates from this model will slightly
differ from the ones one would obtain by setting <code>strategy = "fixed"</code> and
<code>eta</code> equal to that optimal tuning value. This is due to the different
starting values employed in the two scenarios. In the automatic procedure,
the starting values of the final model come from the ones of the previous
model in the optimization loop; in the fixed-tuning context, the starting
values come from the default ones in <code>penfa</code>.
</p>



<h4>Grid-search</h4>

<p>If <code>strategy = "fixed"</code>, <code>penfa</code> estimates a penalized factor model
with the value of the tuning parameter stored in <code>eta</code>. This is useful
if users wish to make multiple calls to the <code>penfa</code> function using a
range of values for the tuning parameter. Then, the optimal penalized model
can be picked on the basis of information criteria, which are easily computed
by calling the <code>AIC</code> and <code>BIC</code> functions. It is often convenient
to use the (Generalized) Bayesian Information Criterion as a selector, due to
its recurrent use in sparse settings.
</p>
<p>These information criteria use the theoretical definition of the effective
degrees of freedom (<em>edf</em>) as their bias terms. This is because the use
of differentiable penalty approximations make the objective function
twice-continuously differentiable. The total <code>edf</code> are as the sum of the
effective degree of freedom for each model parameter, which in turn ranges
from 0 to 1 and quantifies the extend to which each parameter has been
penalized. <br></p>



<h3>Penalization</h3>

<p>The <code>penfa</code> function penalizes every element in the parameter
matrix/vector specified in the <code>eta</code> argument. For instance, if
<code>eta = list("shrink" = c("lambda" = 0.01), "diff" = c("none" = 0))</code> all
factor loadings are penalized through a shrinkage penalty.
</p>


<h4>Choosing the penalty function</h4>

<p>It may be beneficial to try out different penalties, and see which one works
best for the problem at hand. It is also useful to keep the following in mind:
</p>

<ul>
<li> <p><strong>Shrinkage</strong>: lasso, alasso, scad, and mcp are able to shrink parameters
to zero, contrarily to the ridge penalty whose purpose is just regularizing
the estimation process.
</p>
</li>
<li> <p><strong>Unbiasedness</strong>: alasso, scad, and mcp enjoy the so-called "oracle"
property. On the contrary, the lasso is biased since it applies the same
penalization to all parameters.
</p>
</li>
<li> <p><strong>Automatic procedure:</strong> only lasso, alasso, and ridge are supported by the
automatic procedure. This means that these penalties are a convenient choice
with all the analyses requiring multiple penalty terms (e.g., multiple-group
analyses), for which the automatic procedure is the only feasible alternative
to otherwise computationally intensive multi-dimensional grid-searches.
</p>
</li>
</ul>
<p>Geminiani, Marra, and Moustaki (2021) performed numerical and
empirical examples to evaluate and compare the performance of single- and
multiple-group penalized factor models under different penalty functions. The
alasso penalty generally produced the best trade-off between sparsity and
goodness of fit. However, unlike other penalties, the alasso requires a set
of adaptive weights. In some situations, the weights might not be available,
or might be difficult to obtain. If this is the case, users are encouraged to
resort to simpler penalties. <br></p>



<h4>More sparsity</h4>

<p>The penalized model automatically tries to generate the optimal trade-off
between goodness of fit and model complexity (if <code>strategy = "auto"</code>).
As a result of this delicate balance, it may not provide the sparsest factor
solution. If users desire more sparsity, they can follow the guidelines
below.
</p>

<ul>
<li>
<p> Influence factor: increase the value of the influence factor stored in
the option <code>gamma</code>. As a rule of thumb, in our experience, common values
for obtaining sparse solutions usually range between 3.5 and 4.5.
</p>
</li>
<li>
<p> Penalties: some penalties rely on a second tuning parameter. It may be
helpful to try out different values for it, and see which one performs best.
For instance, increasing the value or the exponent of the alasso (by
specifying, for instance, <code>a.alasso = 2</code>) leads to sparser solutions. </p>
</li>
</ul>
<p>In case users fitted a penalized model with a fixed tuning parameter
(<code>strategy = "fixed"</code>), they can manually and subjectively increase its
value in the option <code>eta</code> to encourage more sparsity. When doing so, it
is helpful to first do some trials and understand a reasonable range of
values that the tuning parameter can take. <br></p>



<h4>Ordinary Maximum Likelihood</h4>

<p>If <code>strategy = "fixed"</code>, <code>pen.shrink = "none"</code>, <code>pen.diff =
"none"</code>, and <code>eta = list("shrink" = c("none" = 0), "diff" = c("none" =
0))</code>, no penalization is applied, and the model is estimated through ordinary
maximum likelihood. <br></p>



<h3>Convergence &amp; Admissibility</h3>

<p>The function <code>penfa</code> internally assesses the convergence of the fitted
model, and the admissibility of the final solution.
</p>


<h4>Convergence</h4>

<p>The convergence checks assess whether the penalized gradient vector is close
to zero and the penalized Hessian/Fisher information matrix is positive
definite. In case of convergence issues, <code>penfa</code> warns the users with
explanatory messages. <br><strong>Note:</strong> Due to the presence of possibly
multiple penalty terms, our experiments highlighted that the penalized
gradient need not be strictly close to zero to obtain meaningful results. It
is enough that its elements do not exceed a pre-specified threshold, whose
value can be changed through the <code>optim.dx.tol</code> option.
</p>



<h4>Admissibility</h4>

<p>The admissibility checks are carried out to determine whether the final
solution is <em>admissible</em>. Specifically, the <code>penfa</code> function
sequentially checks whether:
</p>

<ol>
<li>
<p> The final model includes any negative unique variances (Heywood cases);
</p>
</li>
<li>
<p> The final model includes any negative factor variances;
</p>
</li>
<li>
<p> The estimated common factor covariance matrix is positive definite;
</p>
</li>
<li>
<p> The estimated unique factor covariance matrix is positive definite;
</p>
</li>
<li>
<p> The estimated factor loading matrix is of full column rank;
</p>
</li>
<li>
<p> The estimated factor loading matrix does not contain any null rows.
</p>
</li>
</ol>
<p>In case of multiple-group analyses, the function checks the admissibility of
the parameter matrices of each group. If any of the above conditions are not
satisfied, the <code>penfa</code> function warns the user with explanatory
messages on the reasons why. <br></p>



<h3>Warnings &amp; Errors</h3>

<p>Occasionally the <code>penfa</code> function may print out warnings or produce
errors. If the errors concern convergence issues, it may be helpful to go
through the following steps:
</p>
 <ol>
<li>
<p> Identification: please make sure that at least the
minimum identification restrictions are satisfied. This implies fixing the
scale and the origin of every factor in each group. In addition, other
constraints - which usually come in the form of zero-restricted loadings -
are necessary due to rotational freedom.
</p>
</li>
<li>
<p> Starting values: the choice of the starting values is of paramount
importance when it comes to convergence. The starting values internally used
by <code>penfa</code> correspond to the ones used by the
<a href="https://CRAN.R-project.org/package=lavaan"><code>lavaan</code></a> package for
<code>confirmatory factor analysis</code>. If users have some prior knowledge or
intuition about possible values for some of the parameters, it might be
beneficial to include this information by providing the starting values for
those parameters in the syntax specification (see below for additional
details). For instance, depending on the case, specifying the starting values
of the primary loadings equal to 1 (<code>start(1)*x1 + ...</code>) often results
in more stable optimization processes, especially when dealing with
complicated models that require the estimation of many parameters, as in
multiple-group penalized factor analysis.
</p>
</li>
<li>
<p> Sample size: the penalized models fitted by <code>penfa</code> have
a larger number of parameters than confirmatory factor analytic applications.
This complexity should be accompanied by a reasonable sample size. If the
sample size is too small for the complexity of the model, convergence issues
will arise. In case of small sample sizes, it might in principle be more
reliable to select the tuning parameter through a grid-search with the GBIC
instead of using the automatic procedure.
</p>
</li>
<li>
<p> Automatic procedure: if the starting values of the tuning parameters
prevent the automatic procedure from finding the optimal estimates of the
tuning parameters, the procedure is repeated with different starting values.
If this fails, an error is printed out.
</p>
</li>
<li>
<p> Adaptive weights: when using the alasso penalty, it is suggested to
manually provide a vector of adaptive weights, especially for complex models.
The adaptive weights often come in the form of (unpenalized) maximum
likelihood estimates. If no vector of weights is provided, the <code>penfa</code>
function internally estimates an unpenalized MLE model whose parameter
estimates will serve as weights. If the unpenalized model does not converge,
the <code>penfa</code> function internally estimates a ridge-regularized factor
model and uses the resulting estimates as weights. If even this estimation
fails, an error is printed out. <br></p>
</li>
</ol>
<p>Ultimately, if none of the above succeeds, users shall consider re-specifying
the model, either by simplifying the hypothesized factor structure or
considering a subset of the observed variables. Increasing the number of
restrictions (for instance, by specifying some additional fixed loadings)
might be advantageous. Also, as a general practice, when conducting a
multiple-group analysis, make sure beforehand that the groups share similar
factor structures: if the groups have different factor configurations, the
final results will be distorted.
</p>
<p>It is always important to assess whether the distributional assumptions of
the normal linear factor model hold. The <code>penfa</code> function fits penalized
factor models to continuous observed variables; this excludes categorical
items or items with a few number of categories that would instead require
tailored approaches that specifically take into account the qualitative
nature of the data. <br></p>


<h3>Standard Errors</h3>

<p>The standard errors are derived from the inverse of the penalized Fisher
information matrix (if <code>information = "fisher"</code>) or penalized Hessian
(if <code>information = "hessian"</code>), which relies on the Bayesian result for
the covariance matrix of the estimated parameters. The implemented framework
allows to have a standard error for every model parameter. However, users
should take extra caution when using the standard errors associated with the
penalized parameters that were shrunken to zero. <br></p>


<h3>Vignettes and Tutorials</h3>

<p>To learn more about <code>penfa</code>, start with the vignettes and tutorials at
<code>browseVignettes(package = "penfa")</code> and
<a href="https://egeminiani.github.io/penfa/articles/">https://egeminiani.github.io/penfa/articles/</a>.
</p>


<h3>Author(s)</h3>

<p>Elena Geminiani <a href="mailto:geminianielena@gmail.com">geminianielena@gmail.com</a>.
</p>


<h3>References</h3>

<p>Geminiani, E., Marra, G., &amp; Moustaki, I. (2021). "Single- and Multiple-Group
Penalized Factor Analysis: A Trust-Region Algorithm Approach with Integrated
Automatic Multiple Tuning Parameter Selection." Psychometrika, 86(1), 65-95.
doi: <a href="https://doi.org/10.1007/s11336-021-09751-8">10.1007/s11336-021-09751-8</a>
</p>
<p>Geminiani E. (2020), "A penalized likelihood-based framework for single and
multiple-group factor analysis models" (Doctoral dissertation, University of
Bologna). Available at <a href="http://amsdottorato.unibo.it/9355/">http://amsdottorato.unibo.it/9355/</a>.
</p>


<h3>See Also</h3>

<p><code>penfa-class</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data(ccdata)

### Single-group analysis (no mean-structure, unit factor variances)
syntax = 'help  =~   h1 + h2 + h3 + h4 + h5 + h6 + h7 + 0*v1 + v2 + v3 + v4 + v5
          voice =~ 0*h1 + h2 + h3 + h4 + h5 + h6 + h7 +   v1 + v2 + v3 + v4 + v5'

alasso_fit &lt;- penfa(## factor model
                    model  = syntax,
                    data   = ccdata,
                    std.lv = TRUE,
                    ## penalization
                    pen.shrink = "alasso",
                    eta = list(shrink = c("lambda" = 0.01), diff = c("none" = 0)),
                    ## automatic procedure
                    strategy = "auto",
                    gamma = 4)



### Multiple-group analysis (mean structure, marker-variable approach, starting values)
syntax_mg = '
help  =~ 1*h1 +          h2 +          h3 + h4 + h5 + h6 + h7 + 0*v1 + v2 + v3 + v4 + v5
voice =~ 0*h1 + start(0)*h2 + start(0)*h3 + h4 + h5 + h6 + h7 + 1*v1 + v2 + v3 + v4 + v5
h1 + v1 ~ 0*1 '

# Compute weights for alasso from unpenalized model
mle_fitMG &lt;- penfa(model = syntax_mg,
                   data  = ccdata,
                   group = "country",
                   int.ov.free = TRUE,
                   int.lv.free = TRUE,
                   pen.shrink = "none",
                   pen.diff = "none",
                   eta = list(shrink = c("lambda" = 0), diff = c("none" = 0)),
                   strategy = "fixed")
mle_weightsMG &lt;- coef(mle_fitMG)

# Fit model
alasso_fitMG &lt;- penfa(## factor model
                      model = syntax_mg,
                      data = ccdata,
                      group = "country",
                      int.ov.free = TRUE,
                      int.lv.free = TRUE,
                      ## penalization
                      pen.shrink = "alasso",
                      pen.diff = "alasso",
                      eta = list(shrink = c("lambda" = 0.01),
                      diff = c("lambda" = 0.1, "tau" = 0.01)),
                      ## automatic procedure
                      strategy = "auto",
                      gamma = 4,
                      ## alasso
                      weights = mle_weightsMG)

### For additional examples, see the vignettes and tutorials at
### browseVignettes(package = "penfa") and https://egeminiani.github.io/penfa/articles/


</code></pre>


</div>