<div class="container">

<table style="width: 100%;"><tr>
<td>penAFT.cv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cross-validation function for fitting a regularized semiparametric accelerated failure time model</h2>

<h3>Description</h3>

<p>A function to perform cross-validation and compute the solution path for the regularized semiparametric accelerated failure time model estimator.</p>


<h3>Usage</h3>

<pre><code class="language-R">penAFT.cv(X, logY, delta, nlambda = 50, 
  lambda.ratio.min = 0.1, lambda = NULL, 
  penalty = NULL, alpha = 1,weight.set = NULL, 
  groups = NULL, tol.abs = 1e-8, tol.rel = 2.5e-4, 
  standardize = TRUE, nfolds = 5, cv.index = NULL, 
  admm.max.iter = 1e4,quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>An <code class="reqn">n \times p</code> matrix of predictors. Observations should be organized by row.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logY</code></td>
<td>
<p>An <code class="reqn">n</code>-dimensional vector of log-survival or log-censoring times.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>An <code class="reqn">n</code>-dimensional binary vector indicating whether the <code class="reqn">j</code>th component of <code>logY</code> is an observed log-survival time (<code class="reqn">\delta_j = 1</code>) or a log-censoring time (<code class="reqn">\delta_j = 0</code>) for <code class="reqn">j=1, \dots, n</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>The number of candidate tuning parameters to consider.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.ratio.min</code></td>
<td>
<p>The ratio of maximum to minimum candidate tuning parameter value. As a default, we suggest 0.1, but standard model selection procedures should be applied to select <code class="reqn">\lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>An optional (not recommended) prespecified vector of candidate tuning parameters. Should be in descending order. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>Either "EN" or "SG" for elastic net or sparse group lasso penalties.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The tuning parameter <code class="reqn">\alpha</code>. See documentation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight.set</code></td>
<td>
<p>A list of weights. For both penalties, <code class="reqn">w</code> is an <code class="reqn">n</code>-dimensional vector of nonnegative weights. For "SG" penalty, can also include <code class="reqn">v</code> â€“ a non-negative vector the length of the number of groups. See documentation for usage example.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>groups</code></td>
<td>
<p>When using penalty "SG", a <code class="reqn">p</code>-dimensional vector of integers corresponding the to group assignment of each predictor (i.e., column of <code>X</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol.abs</code></td>
<td>
<p>Absolute convergence tolerance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol.rel</code></td>
<td>
<p>Relative convergence tolerance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>Should predictors be standardized (i.e., scaled to have unit variance) for model fitting?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>
<p>The number of folds to be used for cross-validation. Default is five. Ten is recommended when sample size is especially small.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.index</code></td>
<td>
<p>A list of length <code>nfolds</code> of indices to be used for cross-validation. This is to be used if trying to perform cross-validation for both <code class="reqn">\alpha</code> and <code class="reqn">\lambda</code>. Use with extreme caution: this overwrites <code>nfolds</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>admm.max.iter</code></td>
<td>
<p>Maximum number of ADMM iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>quiet</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> variable indicating whether progress should be printed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Given <code class="reqn">(\log y_1 , x_1, \delta_1),\dots,(\log y_n , x_n, \delta_n)</code> where for subject <code class="reqn">i</code> (<code class="reqn">i = 1, \dots, n</code>), <code class="reqn">y_i</code> is the minimum of the survival time and censoring time, <code class="reqn">x_i</code> is a <code class="reqn">p</code>-dimensional predictor, and <code class="reqn">\delta_i</code> is the indicator of censoring,  <code>penAFT.cv</code> performs <code>nfolds</code> cross-validation for selecting the tuning parameter to be used in the argument minimizing
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \delta_i \{ \log y_i - \log y_j  - (x_i - x_j)'\beta \}^{-} + \lambda g(\beta)</code>
</p>

<p>where <code class="reqn">\{a \}^{-} := \max(-a, 0) </code>, <code class="reqn">\lambda &gt; 0</code>, and <code class="reqn">g</code> is either the weighted elastic net penalty (<code>penalty = "EN"</code>) or weighted sparse group lasso penalty (<code>penalty = "SG"</code>).
The weighted elastic net penalty is defined as 
</p>
<p style="text-align: center;"><code class="reqn">\alpha \| w \circ \beta\|_1 + \frac{(1-\alpha)}{2}\|\beta\|_2^2</code>
</p>

<p>where <code class="reqn">w</code> is a set of non-negative weights (which can be specified in the <code>weight.set</code> argument). The weighted sparse group-lasso penalty we consider is
</p>
<p style="text-align: center;"><code class="reqn">\alpha \| w \circ \beta\|_1 + (1-\alpha)\sum_{l=1}^G v_l\|\beta_{\mathcal{G}_l}\|_2</code>
</p>

<p>where again, <code class="reqn">w</code> is a set of non-negative weights and <code class="reqn">v_l</code> are weights applied to each of the <code class="reqn">G</code> groups. 
</p>
<p>Next, we define the cross-validation errors. 
Let <code class="reqn">\mathcal{V}_1, \dots, \mathcal{V}_K</code> be a random <code>nfolds</code> = <code class="reqn">K</code> element partition of <code class="reqn">[n]</code> (the subjects) with the cardinality of each <code class="reqn">\mathcal{V}_k</code> (the "kth fold"") approximately equal for <code class="reqn">k = 1, \dots, K</code>. 
Let <code class="reqn">{\hat{\beta}}_{\lambda(-\mathcal{V}_k)}</code> be the solution with tuning parameter <code class="reqn">\lambda</code> using only data indexed by <code class="reqn">[n] \setminus \{\mathcal{V}_k\}</code> (i.e., outside the kth fold). Then, definining <code class="reqn">e_i(\beta) := \log y_i - \beta'x_i</code> for <code class="reqn">i= 1, \dots, n</code>, we call 

</p>
<p style="text-align: center;"><code class="reqn">
\sum_{k=1}^K \left[\frac{1}{|\mathcal{V}_k|^2}  \sum_{i \in \mathcal{V}_k} \sum_{j \in \mathcal{V}_k} \delta_i \{e_i({\hat{\beta}}_{\lambda(-\mathcal{V}_k)}) - e_{j}({\hat{\beta}}_{\lambda(-\mathcal{V}_k)})\}^{-}\right],
</code>
</p>

<p>the cross-validated Gehan loss at <code class="reqn">\lambda</code> in the <code class="reqn">k</code>th fold, and refer to the sum over all <code>nfolds</code> = <code class="reqn">K</code> folds as the cross-validated Gehan loss. 
Similarly, letting 
letting
</p>
<p style="text-align: center;"><code class="reqn">
\tilde{e}_i({\hat{\beta}}_\lambda) =  \sum_{k = 1}^K (\log y_i - x_i'{\hat{\beta}}_{\lambda(-\mathcal{V}_k)}) \mathbf{1}(i \in \mathcal{V}_k)</code>
</p>
<p> for each <code class="reqn">i \in [n]</code>,
we call 
</p>
<p style="text-align: center;"><code class="reqn">\left[\sum_{i = 1}^n \sum_{j = 1}^n \delta_i \{\tilde{e}_i({\hat{\beta}}_\lambda) - \tilde{e}_j({\hat{\beta}}_\lambda)\}^{-}\right]</code>
</p>

<p>the cross-validated linear predictor score at <code class="reqn">\lambda</code>.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>full.fit</code></td>
<td>
<p>A model fit with the same output as a model fit using <code>penAFT</code>. See documentation for <code>penAFT</code> for more.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.err.linPred</code></td>
<td>
<p>A <code>nlambda</code>-dimensional vector of cross-validated linear predictor scores.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.err.obj</code></td>
<td>
<p>A <code>nfolds </code><code class="reqn">\times</code><code>nlambda</code> matrix of cross-valdiation Gehan losses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.index</code></td>
<td>
<p>A list of length <code>nfolds</code>. Each element contains the indices for subjects belonging to that particular fold.</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R"> # --------------------------------------
# Generate data  
# --------------------------------------
set.seed(1)
genData &lt;- genSurvData(n = 50, p = 50, s = 10, mag = 2,  cens.quant = 0.6)
X &lt;- genData$X
logY &lt;- genData$logY
delta &lt;- genData$status
p &lt;- dim(X)[2]

# -----------------------------------------------
# Fit elastic net penalized estimator
# -----------------------------------------------
fit.en &lt;- penAFT.cv(X = X, logY = logY, delta = delta,
                   nlambda = 10, lambda.ratio.min = 0.1,
                   penalty = "EN", nfolds = 5,
                   alpha = 1)
# ---- coefficients at tuning parameter minimizing cross-valdiation error
coef.en &lt;- penAFT.coef(fit.en)

# ---- predict at 8th tuning parameter from full fit
Xnew &lt;- matrix(rnorm(10*p), nrow=10)
predict.en &lt;- penAFT.predict(fit.en, Xnew = Xnew, lambda = fit.en$full.fit$lambda[8])


  # -----------------------------------------------
  # Fit sparse group penalized estimator
  # -----------------------------------------------
  groups &lt;- rep(1:5, each = 10)
  fit.sg &lt;- penAFT.cv(X = X, logY = logY, delta = delta,
                    nlambda = 50, lambda.ratio.min = 0.01,
                    penalty = "SG", groups = groups, nfolds = 5,
                    alpha = 0.5)
                     
  # -----------------------------------------------
  # Pass fold indices
  # -----------------------------------------------
  groups &lt;- rep(1:5, each = 10)
  cv.index &lt;- list()
  for(k in 1:5){
    cv.index[[k]] &lt;- which(rep(1:5, length=50) == k)
  }
  fit.sg.cvIndex &lt;- penAFT.cv(X = X, logY = logY, delta = delta,
                    nlambda = 50, lambda.ratio.min = 0.01,
                    penalty = "SG", groups = groups, 
                    cv.index = cv.index,
                    alpha = 0.5)
  # --- compare cv indices
  ## Not run: fit.sg.cvIndex$cv.index  == cv.index

</code></pre>


</div>