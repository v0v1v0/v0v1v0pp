<div class="container">

<table style="width: 100%;"><tr>
<td>lmtree</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Linear Model Trees</h2>

<h3>Description</h3>

<p>Model-based recursive partitioning based on least squares
regression.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lmtree(formula, data, subset, na.action, weights, offset, cluster, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>symbolic description of the model (of type
<code>y ~ z1 + ... + zl</code> or <code>y ~ x1 + ... + xk | z1 + ... + zl</code>;
for details see below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data, subset, na.action</code></td>
<td>
<p>arguments controlling formula processing
via <code>model.frame</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>optional numeric vector of weights. By default these are
treated as case weights but the default can be changed in
<code>mob_control</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p>optional numeric vector with an a priori known component to be
included in the model <code>y ~ x1 + ... + xk</code> (i.e., only when
<code>x</code> variables are specified).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cluster</code></td>
<td>
<p>optional vector (typically numeric or factor) with a
cluster ID to be employed for clustered covariances in the parameter
stability tests.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>optional control parameters passed to
<code>mob_control</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Convenience interface for fitting MOBs (model-based recursive partitions) via
the <code>mob</code> function. <code>lmtree</code> internally sets up a model
<code>fit</code> function for <code>mob</code>, using either <code>lm.fit</code>
or <code>lm.wfit</code> (depending on whether weights are used or not).
Then <code>mob</code> is called using the residual sum of squares as the objective
function.
</p>
<p>Compared to calling <code>mob</code> by hand, the implementation tries to avoid
unnecessary computations while growing the tree. Also, it provides a more
elaborate plotting function.
</p>


<h3>Value</h3>

<p>An object of class <code>lmtree</code> inheriting from <code>modelparty</code>.
The <code>info</code> element of the overall <code>party</code> and the individual
<code>node</code>s contain various informations about the models.
</p>


<h3>References</h3>

 
<p>Zeileis A, Hothorn T, Hornik K (2008).
Model-Based Recursive Partitioning.
<em>Journal of Computational and Graphical Statistics</em>, <b>17</b>(2), 492â€“514.
</p>


<h3>See Also</h3>

<p><code>mob</code>, <code>mob_control</code>, <code>glmtree</code></p>


<h3>Examples</h3>

<pre><code class="language-R">if(require("mlbench")) {

## Boston housing data
data("BostonHousing", package = "mlbench")
BostonHousing &lt;- transform(BostonHousing,
  chas = factor(chas, levels = 0:1, labels = c("no", "yes")),
  rad = factor(rad, ordered = TRUE))

## linear model tree
bh_tree &lt;- lmtree(medv ~ log(lstat) + I(rm^2) | zn +
  indus + chas + nox + age + dis + rad + tax + crim + b + ptratio,
  data = BostonHousing, minsize = 40)

## printing whole tree or individual nodes
print(bh_tree)
print(bh_tree, node = 7)

## plotting
plot(bh_tree)
plot(bh_tree, tp_args = list(which = "log(lstat)"))
plot(bh_tree, terminal_panel = NULL)

## estimated parameters
coef(bh_tree)
coef(bh_tree, node = 9)
summary(bh_tree, node = 9)

## various ways for computing the mean squared error (on the training data)
mean((BostonHousing$medv - fitted(bh_tree))^2)
mean(residuals(bh_tree)^2)
deviance(bh_tree)/sum(weights(bh_tree))
deviance(bh_tree)/nobs(bh_tree)

## log-likelihood and information criteria
logLik(bh_tree)
AIC(bh_tree)
BIC(bh_tree)
## (Note that this penalizes estimation of error variances, which
## were treated as nuisance parameters in the fitting process.)

## different types of predictions
bh &lt;- BostonHousing[c(1, 10, 50), ]
predict(bh_tree, newdata = bh, type = "node")
predict(bh_tree, newdata = bh, type = "response")
predict(bh_tree, newdata = bh, type = function(object) summary(object)$r.squared)

}


if(require("AER")) {

## Demand for economics journals data
data("Journals", package = "AER")
Journals &lt;- transform(Journals,
  age = 2000 - foundingyear,
  chars = charpp * pages)

## linear regression tree (OLS)
j_tree &lt;- lmtree(log(subs) ~ log(price/citations) | price + citations +
  age + chars + society, data = Journals, minsize = 10, verbose = TRUE)

## printing and plotting
j_tree
plot(j_tree)

## coefficients and summary
coef(j_tree, node = 1:3)
summary(j_tree, node = 1:3)

}


if(require("AER")) {

## Beauty and teaching ratings data
data("TeachingRatings", package = "AER")

## linear regression (WLS)
## null model
tr_null &lt;- lm(eval ~ 1, data = TeachingRatings, weights = students,
  subset = credits == "more")
## main effects
tr_lm &lt;- lm(eval ~ beauty + gender + minority + native + tenure + division,
  data = TeachingRatings, weights = students, subset = credits == "more")
## tree
tr_tree &lt;- lmtree(eval ~ beauty | minority + age + gender + division + native + tenure,
   data = TeachingRatings, weights = students, subset = credits == "more",
   caseweights = FALSE)

## visualization
plot(tr_tree)

## beauty slope coefficient
coef(tr_lm)[2]
coef(tr_tree)[, 2]

## R-squared
1 - deviance(tr_lm)/deviance(tr_null)
1 - deviance(tr_tree)/deviance(tr_null)
}

</code></pre>


</div>