<div class="container">

<table style="width: 100%;"><tr>
<td>distance</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Distances and Similarities between Probability Density Functions</h2>

<h3>Description</h3>

<p>This functions computes the distance/dissimilarity between two probability density functions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">distance(
  x,
  method = "euclidean",
  p = NULL,
  test.na = TRUE,
  unit = "log",
  epsilon = 1e-05,
  est.prob = NULL,
  use.row.names = FALSE,
  as.dist.obj = FALSE,
  diag = FALSE,
  upper = FALSE,
  mute.message = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric <code>data.frame</code> or <code>matrix</code> (storing probability vectors) or a numeric <code>data.frame</code> or <code>matrix</code> storing counts (if <code>est.prob</code> is specified).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a character string indicating whether the distance measure that should be computed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p>power of the Minkowski distance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test.na</code></td>
<td>
<p>a boolean value indicating whether input vectors should be tested for <code>NA</code> values. Faster computations if <code>test.na = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>a character string specifying the logarithm unit that should be used to compute distances that depend on log computations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>est.prob</code></td>
<td>
<p>method to estimate probabilities from input count vectors such as non-probability vectors. Default: <code>est.prob = NULL</code>. Options are:
</p>

<ul><li> <p><code>est.prob = "empirical"</code>: The relative frequencies of each vector are computed internally. For example an input matrix <code>rbind(1:10, 11:20)</code> will be transformed to a probability vector <code>rbind(1:10 / sum(1:10), 11:20 / sum(11:20))</code>
</p>
</li></ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use.row.names</code></td>
<td>
<p>a logical value indicating whether or not row names from
the input matrix shall be used as rownames and colnames of the output distance matrix. Default value is <code>use.row.names = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>as.dist.obj</code></td>
<td>
<p>shall the return value or matrix be an object of class <code>link[stats]{dist}</code>? Default is <code>as.dist.obj = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diag</code></td>
<td>
<p>if <code>as.dist.obj = TRUE</code>, then this value indicates whether the diagonal of the distance matrix should be printed. Default</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper</code></td>
<td>
<p>if <code>as.dist.obj = TRUE</code>, then this value indicates whether the upper triangle of the distance matrix should be printed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mute.message</code></td>
<td>
<p>a logical value indicating whether or not messages printed by <code>distance</code> shall be muted. Default is <code>mute.message = FALSE</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Here a distance is defined as a quantitative degree of how far two mathematical objects are apart from eachother (Cha, 2007).
</p>
<p>This function implements the following distance/similarity measures to quantify the distance between probability density functions:
</p>

<ul>
<li>
<p> L_p Minkowski family
</p>

<ul>
<li>
<p> Euclidean : <code class="reqn">d = sqrt( \sum | P_i - Q_i |^2)</code>
</p>
</li>
<li>
<p> Manhattan : <code class="reqn">d = \sum | P_i - Q_i |</code>
</p>
</li>
<li>
<p> Minkowski : <code class="reqn">d = ( \sum | P_i - Q_i |^p)^1/p</code>
</p>
</li>
<li>
<p> Chebyshev : <code class="reqn">d = max | P_i - Q_i |</code>
</p>
</li>
</ul>
</li>
<li>
<p> L_1 family
</p>

<ul>
<li>
<p> Sorensen : <code class="reqn">d = \sum | P_i - Q_i | / \sum (P_i + Q_i)</code>
</p>
</li>
<li>
<p> Gower : <code class="reqn">d = 1/d * \sum | P_i - Q_i |</code>
</p>
</li>
<li>
<p> Soergel : <code class="reqn">d = \sum | P_i - Q_i | / \sum max(P_i , Q_i)</code>
</p>
</li>
<li>
<p> Kulczynski d : <code class="reqn">d = \sum | P_i - Q_i | / \sum min(P_i , Q_i)</code>
</p>
</li>
<li>
<p> Canberra : <code class="reqn">d = \sum | P_i - Q_i | / (P_i + Q_i)</code>
</p>
</li>
<li>
<p> Lorentzian : <code class="reqn">d = \sum ln(1 + | P_i - Q_i |)</code>
</p>
</li>
</ul>
</li>
<li>
<p> Intersection family
</p>

<ul>
<li>
<p> Intersection : <code class="reqn">s = \sum min(P_i , Q_i)</code>
</p>
</li>
<li>
<p> Non-Intersection : <code class="reqn">d = 1 - \sum min(P_i , Q_i)</code>
</p>
</li>
<li>
<p> Wave Hedges : <code class="reqn">d = \sum | P_i - Q_i | / max(P_i , Q_i)</code>
</p>
</li>
<li>
<p> Czekanowski : <code class="reqn">d = \sum | P_i - Q_i | / \sum | P_i + Q_i |</code>
</p>
</li>
<li>
<p> Motyka : <code class="reqn">d = \sum min(P_i , Q_i) / (P_i + Q_i)</code>
</p>
</li>
<li>
<p> Kulczynski s : <code class="reqn">d = 1 / \sum | P_i - Q_i | / \sum min(P_i , Q_i)</code>
</p>
</li>
<li>
<p> Tanimoto : <code class="reqn">d = \sum (max(P_i , Q_i) - min(P_i , Q_i)) / \sum max(P_i , Q_i)</code> ; equivalent to Soergel
</p>
</li>
<li>
<p> Ruzicka : <code class="reqn">s = \sum min(P_i , Q_i) / \sum max(P_i , Q_i)</code> ; equivalent to 1 - Tanimoto = 1 - Soergel
</p>
</li>
</ul>
</li>
<li>
<p> Inner Product family
</p>

<ul>
<li>
<p> Inner Product : <code class="reqn">s = \sum P_i * Q_i</code>
</p>
</li>
<li>
<p> Harmonic mean : <code class="reqn">s = 2 * \sum (P_i * Q_i) / (P_i + Q_i)</code>
</p>
</li>
<li>
<p> Cosine : <code class="reqn">s = \sum (P_i * Q_i) / sqrt(\sum P_i^2) * sqrt(\sum Q_i^2)</code>
</p>
</li>
<li>
<p> Kumar-Hassebrook (PCE) : <code class="reqn">s = \sum (P_i * Q_i) / (\sum P_i^2 + \sum Q_i^2 - \sum (P_i * Q_i))</code>
</p>
</li>
<li>
<p> Jaccard : <code class="reqn">d = 1 - \sum (P_i * Q_i) / (\sum P_i^2 + \sum Q_i^2 - \sum (P_i * Q_i))</code> ; equivalent to 1 - Kumar-Hassebrook
</p>
</li>
<li>
<p> Dice : <code class="reqn">d = \sum (P_i - Q_i)^2 / (\sum P_i^2 + \sum Q_i^2)</code>
</p>
</li>
</ul>
</li>
<li>
<p> Squared-chord family
</p>

<ul>
<li>
<p> Fidelity : <code class="reqn">s = \sum sqrt(P_i * Q_i)</code>
</p>
</li>
<li>
<p> Bhattacharyya : <code class="reqn">d = - ln \sum sqrt(P_i * Q_i)</code>
</p>
</li>
<li>
<p> Hellinger : <code class="reqn">d = 2 * sqrt( 1 - \sum sqrt(P_i * Q_i))</code>
</p>
</li>
<li>
<p> Matusita : <code class="reqn">d = sqrt( 2 - 2 * \sum sqrt(P_i * Q_i))</code>
</p>
</li>
<li>
<p> Squared-chord : <code class="reqn">d = \sum ( sqrt(P_i) - sqrt(Q_i) )^2</code>
</p>
</li>
</ul>
</li>
<li>
<p> Squared L_2 family (<code class="reqn">X</code>^2 squared family)
</p>

<ul>
<li>
<p> Squared Euclidean : <code class="reqn">d = \sum ( P_i - Q_i )^2</code>
</p>
</li>
<li>
<p> Pearson <code class="reqn">X</code>^2 : <code class="reqn">d = \sum ( (P_i - Q_i )^2 / Q_i )</code>
</p>
</li>
<li>
<p> Neyman <code class="reqn">X</code>^2 : <code class="reqn">d = \sum ( (P_i - Q_i )^2 / P_i )</code>
</p>
</li>
<li>
<p> Squared <code class="reqn">X</code>^2 : <code class="reqn">d = \sum ( (P_i - Q_i )^2 / (P_i + Q_i) )</code>
</p>
</li>
<li>
<p> Probabilistic Symmetric <code class="reqn">X</code>^2 : <code class="reqn">d = 2 *  \sum ( (P_i - Q_i )^2 / (P_i + Q_i) )</code>
</p>
</li>
<li>
<p> Divergence : <code class="reqn">X</code>^2 : <code class="reqn">d = 2 *  \sum ( (P_i - Q_i )^2 / (P_i + Q_i)^2 )</code>
</p>
</li>
<li>
<p> Clark : <code class="reqn">d = sqrt ( \sum (| P_i - Q_i | / (P_i + Q_i))^2 )</code>
</p>
</li>
<li>
<p> Additive Symmetric <code class="reqn">X</code>^2 : <code class="reqn">d = \sum ( ((P_i - Q_i)^2 * (P_i + Q_i)) / (P_i * Q_i) ) </code>
</p>
</li>
</ul>
</li>
<li>
<p> Shannon's entropy family
</p>

<ul>
<li>
<p> Kullback-Leibler : <code class="reqn">d = \sum P_i * log(P_i / Q_i)</code>
</p>
</li>
<li>
<p> Jeffreys : <code class="reqn">d = \sum (P_i - Q_i) * log(P_i / Q_i)</code>
</p>
</li>
<li>
<p> K divergence : <code class="reqn">d = \sum P_i * log(2 * P_i / P_i + Q_i)</code>
</p>
</li>
<li>
<p> Topsoe : <code class="reqn">d = \sum ( P_i * log(2 * P_i / P_i + Q_i) ) + ( Q_i * log(2 * Q_i / P_i + Q_i) )</code>
</p>
</li>
<li>
<p> Jensen-Shannon :  <code class="reqn">d = 0.5 * ( \sum P_i * log(2 * P_i / P_i + Q_i) + \sum Q_i * log(2 * Q_i / P_i + Q_i))</code>
</p>
</li>
<li>
<p> Jensen difference : <code class="reqn">d = \sum ( (P_i * log(P_i) + Q_i * log(Q_i) / 2) - (P_i + Q_i / 2) * log(P_i + Q_i / 2) )</code>
</p>
</li>
</ul>
</li>
<li>
<p> Combinations
</p>

<ul>
<li>
<p> Taneja : <code class="reqn">d = \sum ( P_i + Q_i / 2) * log( P_i + Q_i / ( 2 * sqrt( P_i * Q_i)) )</code>
</p>
</li>
<li>
<p> Kumar-Johnson : <code class="reqn">d = \sum (P_i^2 - Q_i^2)^2 / 2 * (P_i * Q_i)^1.5</code>
</p>
</li>
<li>
<p> Avg(L_1, L_n) : <code class="reqn">d = \sum | P_i - Q_i| + max{ | P_i - Q_i |} / 2</code>
</p>
</li>
</ul>
<p>In cases where <code>x</code> specifies a count matrix, the argument <code>est.prob</code> can be selected to first estimate probability vectors
from input count vectors and second compute the corresponding distance measure based on the estimated probability vectors.
</p>
<p>The following probability estimation methods are implemented in this function:
</p>

<ul><li> <p><code>est.prob = "empirical"</code> : relative frequencies of counts.
</p>
</li></ul>
</li>
</ul>
<h3>Value</h3>

<p>The following results are returned depending on the dimension of <code>x</code>:
</p>

<ul>
<li>
<p> in case <code>nrow(x)</code> = 2 : a single distance value.
</p>
</li>
<li>
<p> in case <code>nrow(x)</code> &gt; 2 : a distance <code>matrix</code> storing distance values for all pairwise probability vector comparisons.
</p>
</li>
</ul>
<h3>Note</h3>

<p>According to the reference in some distance measure computations invalid computations can
occur when dealing with 0 probabilities.
</p>
<p>In these cases the convention is treated as follows:
</p>

<ul>
<li>
<p> division by zero - case <code>0/0</code>: when the divisor and dividend become zero, <code>0/0</code> is treated as <code>0</code>.
</p>
</li>
<li>
<p> division by zero - case <code>n/0</code>: when only the divisor becomes <code>0</code>, the corresponsning <code>0</code> is replaced by a small <code class="reqn">\epsilon = 0.00001</code>.
</p>
</li>
<li>
<p> log of zero - case <code>0 * log(0)</code>: is treated as <code>0</code>.
</p>
</li>
<li>
<p> log of zero - case <code>log(0)</code>: zero is replaced by a small <code class="reqn">\epsilon = 0.00001</code>.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Sung-Hyuk Cha. (2007). <em>Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions</em>. International Journal of Mathematical Models and Methods in Applied Sciences 4: 1.
</p>


<h3>See Also</h3>

<p><code>getDistMethods</code>, <code>estimate.probability</code>, <code>dist.diversity</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Simple Examples

# receive a list of implemented probability distance measures
getDistMethods()

## compute the euclidean distance between two probability vectors
distance(rbind(1:10/sum(1:10), 20:29/sum(20:29)), method = "euclidean")

## compute the euclidean distance between all pairwise comparisons of probability vectors
ProbMatrix &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29),30:39/sum(30:39))
distance(ProbMatrix, method = "euclidean")

# compute distance matrix without testing for NA values in the input matrix
distance(ProbMatrix, method = "euclidean", test.na = FALSE)

# alternatively use the colnames of the input data for the rownames and colnames
# of the output distance matrix
ProbMatrix &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29),30:39/sum(30:39))
rownames(ProbMatrix) &lt;- paste0("Example", 1:3)
distance(ProbMatrix, method = "euclidean", use.row.names = TRUE)

# Specialized Examples

CountMatrix &lt;- rbind(1:10, 20:29, 30:39)

## estimate probabilities from a count matrix
distance(CountMatrix, method = "euclidean", est.prob = "empirical")

## compute the euclidean distance for count data
## NOTE: some distance measures are only defined for probability values,
distance(CountMatrix, method = "euclidean")

## compute the Kullback-Leibler Divergence with different logarithm bases:
### case: unit = log (Default)
distance(ProbMatrix, method = "kullback-leibler", unit = "log")

### case: unit = log2
distance(ProbMatrix, method = "kullback-leibler", unit = "log2")

### case: unit = log10
distance(ProbMatrix, method = "kullback-leibler", unit = "log10")

</code></pre>


</div>