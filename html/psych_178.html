<div class="container">

<table style="width: 100%;"><tr>
<td>factor.scores</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Various ways to estimate factor scores for the factor analysis model</h2>

<h3>Description</h3>

<p>A fundamental problem with factor analysis is that although the model is defined at the structural level, it is indeterminate at the data level. This problem of factor indeterminancy leads to alternative ways of estimating factor scores, none of which is ideal.  Following Grice (2001) four different methods are available here.
</p>


<h3>Usage</h3>

<pre><code class="language-R">factor.scores(x, f, Phi = NULL, method = c("Thurstone", "tenBerge", "Anderson", 
       "Bartlett", "Harman","components"),rho=NULL,missing=FALSE,impute="none")</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Either a matrix of data if scores are to be found, or a correlation matrix if just the factor weights are to be found.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f</code></td>
<td>
<p>The output from the <code>fa</code> or <code>irt.fa</code> functions, or a factor loading matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Phi</code></td>
<td>
<p>If a pattern matrix is provided, then what were the factor intercorrelations.  Does not need to be specified if f is the output from the  <code>fa</code> or <code>irt.fa</code> functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Which of four factor score estimation procedures should be used. Defaults to "Thurstone" or regression based weights.  See details below for the other four methods.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>If x is a set of data and rho is specified, then find scores based upon the fa results and the correlations reported in rho.  Used when scoring fa.poly results.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>missing</code></td>
<td>
<p>If missing is TRUE, missing items  are imputed using either the median or mean.  If missing is FALSE, the default, scores are found based upon the mean of the available items for each subject.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>impute</code></td>
<td>
<p>By default, only complete cases are scored.  But, missing data can be imputed using "median" or "mean".  The number of missing by subject is reported.  If impute = "none", missing data are not scored.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Although the factor analysis model is defined at the structural level, it is undefined at the data level.  This is a well known but little discussed problem with factor analysis.  
</p>
<p>Factor scores represent estimates of common part of the variables and should not be thought of as identical to the factors themselves. If a factor is thought of as a chop stick stuck into the center of an ice cream cone and factor score estimates are represented by straws anywhere along the edge of the cone the problem of factor indeterminacy becomes clear, for depending on the shape of the cone, two straws can be negatively correlated with each other. (The imagery is taken from Niels Waller, adapted from Stanley Mulaik). In a very clear discussion of the problem of factor score indeterminacy, Grice (2001) reviews several alternative ways of estimating factor scores and considers weighting schemes that will produce uncorrelated factor score estimates as well as the effect of using coarse coded (unit weighted) factor weights.
</p>
<p><code>factor.scores</code> uses four different ways of estimate factor scores.  In all cases, the factor score estimates are based upon the data matrix, X, times a weighting matrix, W, which weights the observed variables.
</p>
<p>For polytomous or dichotmous data, factor scores can be estimated using Item Response Theory techniques (e.g., using <code>link{irt.fa}</code> and then <code>link{scoreIrt}</code>.  Such scores are still just factor score estimates, for the IRT model is a latent variable model equivalent to factor analysis.  
</p>

<ul>
<li>
<p> method="Thurstone" finds the regression based weights: <code class="reqn">W = R^{-1} F</code> where R is the correlation matrix and F is the factor loading matrix. 
</p>
</li>
<li>
<p> method="tenBerge" finds weights such that the correlation between factors for an oblique solution is preserved. Note that  formula 8 in Grice has a typo in the formula for C and should be:
<code class="reqn">L = F \Phi^(1/2) </code>
<code class="reqn">C = R^(-1/2) L (L' R^(-1) L)^(-1/2) </code>
<code class="reqn">W = R ^(-1/2) C \Phi^(1/2) </code>
</p>
</li>
<li>
<p> method="Anderson" finds weights such that the factor scores will be uncorrelated: <code class="reqn">W = U^{-2}F (F' U^{-2} R  U^{-2} F)^{-1/2}</code> where U is the diagonal matrix of uniquenesses. The Anderson method works for orthogonal factors only, while the tenBerge method works for orthogonal or oblique solutions.
</p>
</li>
<li>
<p> method = "Bartlett"  finds weights given <code class="reqn">W = U^{-2}F (F' U^{-2}F)^{-1}</code>
</p>
</li>
<li>
<p> method="Harman" finds weights based upon socalled "idealized" variables: <code class="reqn">W =  F (t(F) F)^{-1}</code>.
</p>
</li>
<li>
<p> method="components" uses weights that are just component loadings.  
</p>
</li>
</ul>
<h3>Value</h3>


<ul>
<li>
<p> scores (the factor scores if the raw data is given)
</p>
</li>
<li>
<p> weights (the factor weights) 
</p>
</li>
<li>
<p> r.scores  (The correlations of the factor score estimates.)
</p>
</li>
<li>
<p> missing   A vector of the number of missing observations per subject
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>William Revelle
</p>


<h3>References</h3>

<p>Grice, James W.,2001, Computing and evaluating factor scores,  Psychological Methods, 6,4, 430-450. (note the typo in equation 8)
</p>
<p>ten Berge, Jos M.F.,  Wim P. Krijnen, Tom Wansbeek and Alexander Shapiro (1999) Some new results on correlation-preserving factor scores prediction methods. Linear Algebra and its Applications, 289, 311-318.
</p>
<p>Revelle, William. (in prep) An introduction to psychometric theory with applications in R. Springer.  Working draft available at <a href="https://personality-project.org/r/book/">https://personality-project.org/r/book/</a> 
</p>


<h3>See Also</h3>

<p><code>fa</code>, <code>factor.stats</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">f3 &lt;- fa(Thurstone,3 )
f3$weights  #just the scoring weights
f5 &lt;- fa(bfi,5) #this does the factor analyis
my.scores &lt;- factor.scores(bfi,f5, method="tenBerge")
#compare the tenBerge factor score correlation to the factor correlations
cor(my.scores$scores,use="pairwise") - f5$Phi  #compare to the f5$Phi values
#compare the default (regression) score correlations to the factor correlations
cor(f5$scores,use="pairwise")  - f5$Phi
#compare to the f5 solution

</code></pre>


</div>