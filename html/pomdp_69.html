<div class="container">

<table style="width: 100%;"><tr>
<td>simulate_MDP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Simulate Trajectories in a MDP</h2>

<h3>Description</h3>

<p>Simulate trajectories through a MDP. The start state for each
trajectory is randomly chosen using the specified belief. The belief is used to choose actions
from an epsilon-greedy policy and then update the state.
</p>


<h3>Usage</h3>

<pre><code class="language-R">simulate_MDP(
  model,
  n = 100,
  start = NULL,
  horizon = NULL,
  epsilon = NULL,
  delta_horizon = 0.001,
  return_trajectories = FALSE,
  engine = "cpp",
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a MDP model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>number of trajectories.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>start</code></td>
<td>
<p>probability distribution over the states for choosing the
starting states for the trajectories. Defaults to "uniform".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>horizon</code></td>
<td>
<p>epochs end once an absorbing state is reached or after
the maximal number of epochs specified via <code>horizon</code>. If <code>NULL</code> then the
horizon for the model is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>the probability of random actions  for using an epsilon-greedy policy.
Default for solved models is 0 and for unsolved model 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta_horizon</code></td>
<td>
<p>precision used to determine the horizon for infinite-horizon problems.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_trajectories</code></td>
<td>
<p>logical; return the complete trajectories.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>engine</code></td>
<td>
<p><code>'cpp'</code> or <code>'r'</code> to perform simulation using a faster C++
or a native R implementation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>report used parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments are ignored.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A native R implementation is available (<code>engine = 'r'</code>) and the default is a
faster C++ implementation (<code>engine = 'cpp'</code>).
</p>
<p>Both implementations support parallel execution using the package
<span class="pkg">foreach</span>. To enable parallel execution, a parallel backend like
<span class="pkg">doparallel</span> needs to be available needs to be registered (see
<code>doParallel::registerDoParallel()</code>).
Note that small simulations are slower using parallelization. Therefore, C++ simulations
with n * horizon less than 100,000 are always executed using a single worker.
</p>


<h3>Value</h3>

<p>A list with elements:
</p>

<ul>
<li> <p><code>avg_reward</code>: The average discounted reward.
</p>
</li>
<li> <p><code>reward</code>: Reward for each trajectory.
</p>
</li>
<li> <p><code>action_cnt</code>: Action counts.
</p>
</li>
<li> <p><code>state_cnt</code>: State counts.
</p>
</li>
<li> <p><code>trajectories</code>: A data.frame with the trajectories. Each row
contains the <code>episode</code> id, the <code>time</code> step, the state <code>s</code>,
the chosen action <code>a</code>,
the reward <code>r</code>, and the next state <code>s_prime</code>. Trajectories are
only returned for <code>return_trajectories = TRUE</code>.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code>MDP()</code>,
<code>MDP2POMDP</code>,
<code>MDP_policy_functions</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>add_policy()</code>,
<code>gridworld</code>,
<code>reachable_and_absorbing</code>,
<code>regret()</code>,
<code>solve_MDP()</code>,
<code>transition_graph()</code>,
<code>value_function()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># enable parallel simulation 
# doParallel::registerDoParallel()

data(Maze)

# solve the POMDP for 5 epochs and no discounting
sol &lt;- solve_MDP(Maze, discount = 1)
sol

# U in the policy is and estimate of the utility of being in a state when using the optimal policy.
policy(sol)
gridworld_matrix(sol, what = "action")

## Example 1: simulate 100 trajectories following the policy, 
#             only the final belief state is returned
sim &lt;- simulate_MDP(sol, n = 100, horizon = 10, verbose = TRUE)
sim

# Note that all simulations start at s_1 and that the simulated avg. reward
# is therefore an estimate to the U value for the start state s_1.
policy(sol)[1,]

# Calculate proportion of actions taken in the simulation
round_stochastic(sim$action_cnt / sum(sim$action_cnt), 2)

# reward distribution
hist(sim$reward)

## Example 2: simulate starting following a uniform distribution over all
#             states and return all trajectories
sim &lt;- simulate_MDP(sol, n = 100, start = "uniform", horizon = 10, 
  return_trajectories = TRUE)
head(sim$trajectories)   
  
# how often was each state visited?
table(sim$trajectories$s)
</code></pre>


</div>