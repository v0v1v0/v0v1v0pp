<div class="container">

<table style="width: 100%;"><tr>
<td>train_wordvec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Train static word embeddings using the Word2Vec, GloVe, or FastText algorithm.</h2>

<h3>Description</h3>

<p>Train static word embeddings using the
<code>Word2Vec</code>,
<code>GloVe</code>, or
<code>FastText</code> algorithm
with multi-threading.
</p>


<h3>Usage</h3>

<pre><code class="language-R">train_wordvec(
  text,
  method = c("word2vec", "glove", "fasttext"),
  dims = 300,
  window = 5,
  min.freq = 5,
  threads = 8,
  model = c("skip-gram", "cbow"),
  loss = c("ns", "hs"),
  negative = 5,
  subsample = 1e-04,
  learning = 0.05,
  ngrams = c(3, 6),
  x.max = 10,
  convergence = -1,
  stopwords = character(0),
  encoding = "UTF-8",
  tolower = FALSE,
  normalize = FALSE,
  iteration,
  tokenizer,
  remove,
  file.save,
  compress = "bzip2",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>text</code></td>
<td>
<p>A character vector of text,
or a file path on disk containing text.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Training algorithm:
</p>

<ul>
<li>
<p><code>"word2vec"</code> (default):
using the <code>word2vec</code> package
</p>
</li>
<li>
<p><code>"glove"</code>:
using the <code>rsparse</code> and
<code>text2vec</code> packages
</p>
</li>
<li>
<p><code>"fasttext"</code>:
using the <code>fastTextR</code> package
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dims</code></td>
<td>
<p>Number of dimensions of word vectors to be trained.
Common choices include 50, 100, 200, 300, and 500.
Defaults to <code>300</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>window</code></td>
<td>
<p>Window size (number of nearby words behind/ahead the current word).
It defines how many surrounding words to be included in training:
[window] words behind and [window] words ahead ([window]*2 in total).
Defaults to <code>5</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min.freq</code></td>
<td>
<p>Minimum frequency of words to be included in training.
Words that appear less than this value of times will be excluded from vocabulary.
Defaults to <code>5</code> (take words that appear at least five times).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threads</code></td>
<td>
<p>Number of CPU threads used for training.
A modest value produces the fastest training.
Too many threads are not always helpful.
Defaults to <code>8</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / FastText&gt;</strong>
</p>
<p>Learning model architecture:
</p>

<ul>
<li>
<p><code>"skip-gram"</code> (default): Skip-Gram,
which predicts surrounding words given the current word
</p>
</li>
<li>
<p><code>"cbow"</code>: Continuous Bag-of-Words,
which predicts the current word based on the context
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / FastText&gt;</strong>
</p>
<p>Loss function (computationally efficient approximation):
</p>

<ul>
<li>
<p><code>"ns"</code> (default): Negative Sampling
</p>
</li>
<li>
<p><code>"hs"</code>: Hierarchical Softmax
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negative</code></td>
<td>
<p><strong>&lt;Only for Negative Sampling in Word2Vec / FastText&gt;</strong>
</p>
<p>Number of negative examples.
Values in the range 5~20 are useful for small training datasets,
while for large datasets the value can be as small as 2~5.
Defaults to <code>5</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subsample</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / FastText&gt;</strong>
</p>
<p>Subsampling of frequent words (threshold for occurrence of words).
Those that appear with higher frequency in the training data will be randomly down-sampled.
Defaults to <code>0.0001</code> (<code>1e-04</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / FastText&gt;</strong>
</p>
<p>Initial (starting) learning rate, also known as alpha.
Defaults to <code>0.05</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ngrams</code></td>
<td>
<p><strong>&lt;Only for FastText&gt;</strong>
</p>
<p>Minimal and maximal ngram length.
Defaults to <code>c(3, 6)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.max</code></td>
<td>
<p><strong>&lt;Only for GloVe&gt;</strong>
</p>
<p>Maximum number of co-occurrences to use in the weighting function.
Defaults to <code>10</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>convergence</code></td>
<td>
<p><strong>&lt;Only for GloVe&gt;</strong>
</p>
<p>Convergence tolerance for SGD iterations. Defaults to <code>-1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stopwords</code></td>
<td>
<p><strong>&lt;Only for Word2Vec / GloVe&gt;</strong>
</p>
<p>A character vector of stopwords to be excluded from training.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>encoding</code></td>
<td>
<p>Text encoding. Defaults to <code>"UTF-8"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tolower</code></td>
<td>
<p>Convert all upper-case characters to lower-case?
Defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>Normalize all word vectors to unit length?
Defaults to <code>FALSE</code>. See <code>normalize</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iteration</code></td>
<td>
<p>Number of training iterations.
More iterations makes a more precise model,
but computational cost is linearly proportional to iterations.
Defaults to <code>5</code> for Word2Vec and FastText
while <code>10</code> for GloVe.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokenizer</code></td>
<td>
<p>Function used to tokenize the text.
Defaults to <code>text2vec::word_tokenizer</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>remove</code></td>
<td>
<p>Strings (in regular expression) to be removed from the text.
Defaults to <code>"_|'|&lt;br/&gt;|&lt;br /&gt;|e\\.g\\.|i\\.e\\."</code>.
You may turn off this by specifying <code>remove=NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>file.save</code></td>
<td>
<p>File name of to-be-saved R data (must be .RData).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compress</code></td>
<td>
<p>Compression method for the saved file. Defaults to <code>"bzip2"</code>.
</p>
<p>Options include:
</p>

<ul>
<li> <p><code>1</code> or <code>"gzip"</code>: modest file size (fastest)
</p>
</li>
<li> <p><code>2</code> or <code>"bzip2"</code>: small file size (fast)
</p>
</li>
<li> <p><code>3</code> or <code>"xz"</code>: minimized file size (slow)
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Print information to the console? Defaults to <code>TRUE</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A <code>wordvec</code> (data.table) with three variables:
<code>word</code>, <code>vec</code>, <code>freq</code>.
</p>


<h3>Download</h3>

<p>Download pre-trained word vectors data (<code>.RData</code>):
<a href="https://psychbruce.github.io/WordVector_RData.pdf">https://psychbruce.github.io/WordVector_RData.pdf</a>
</p>


<h3>References</h3>

<p>All-in-one package:
</p>

<ul><li>
<p><a href="https://CRAN.R-project.org/package=wordsalad">https://CRAN.R-project.org/package=wordsalad</a>
</p>
</li></ul>
<p>Word2Vec:
</p>

<ul>
<li>
<p><a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a>
</p>
</li>
<li>
<p><a href="https://CRAN.R-project.org/package=word2vec">https://CRAN.R-project.org/package=word2vec</a>
</p>
</li>
<li>
<p><a href="https://github.com/maxoodf/word2vec">https://github.com/maxoodf/word2vec</a>
</p>
</li>
</ul>
<p>GloVe:
</p>

<ul>
<li>
<p><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>
</p>
</li>
<li>
<p><a href="https://text2vec.org/glove.html">https://text2vec.org/glove.html</a>
</p>
</li>
<li>
<p><a href="https://CRAN.R-project.org/package=text2vec">https://CRAN.R-project.org/package=text2vec</a>
</p>
</li>
<li>
<p><a href="https://CRAN.R-project.org/package=rsparse">https://CRAN.R-project.org/package=rsparse</a>
</p>
</li>
</ul>
<p>FastText:
</p>

<ul>
<li>
<p><a href="https://fasttext.cc/">https://fasttext.cc/</a>
</p>
</li>
<li>
<p><a href="https://CRAN.R-project.org/package=fastTextR">https://CRAN.R-project.org/package=fastTextR</a>
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>tokenize</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">review = text2vec::movie_review  # a data.frame'
text = review$review

## Note: All the examples train 50 dims for faster code check.

## Word2Vec (SGNS)
dt1 = train_wordvec(
  text,
  method="word2vec",
  model="skip-gram",
  dims=50, window=5,
  normalize=TRUE)

dt1
most_similar(dt1, "Ive")  # evaluate performance
most_similar(dt1, ~ man - he + she, topn=5)  # evaluate performance
most_similar(dt1, ~ boy - he + she, topn=5)  # evaluate performance

## GloVe
dt2 = train_wordvec(
  text,
  method="glove",
  dims=50, window=5,
  normalize=TRUE)

dt2
most_similar(dt2, "Ive")  # evaluate performance
most_similar(dt2, ~ man - he + she, topn=5)  # evaluate performance
most_similar(dt2, ~ boy - he + she, topn=5)  # evaluate performance

## FastText
dt3 = train_wordvec(
  text,
  method="fasttext",
  model="skip-gram",
  dims=50, window=5,
  normalize=TRUE)

dt3
most_similar(dt3, "Ive")  # evaluate performance
most_similar(dt3, ~ man - he + she, topn=5)  # evaluate performance
most_similar(dt3, ~ boy - he + she, topn=5)  # evaluate performance

</code></pre>


</div>