<div class="container">

<table style="width: 100%;"><tr>
<td>regret</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate the Regret of a Policy</h2>

<h3>Description</h3>

<p>Calculates the regret of a policy relative to a benchmark policy.
</p>


<h3>Usage</h3>

<pre><code class="language-R">regret(policy, benchmark, start = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>policy</code></td>
<td>
<p>a solved POMDP containing the policy to calculate the regret for.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>benchmark</code></td>
<td>
<p>a solved POMDP with the (optimal) policy. Regret is calculated relative to this
policy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>start</code></td>
<td>
<p>the used start (belief) state. If NULL then the start (belief) state of the <code>benchmark</code> is used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Regret is defined as <code class="reqn">V^{\pi^*}(s_0) - V^{\pi}(s_0)</code> with <code class="reqn">V^\pi</code> representing the expected long-term
state value (represented by the value function) given the policy <code class="reqn">\pi</code> and the start
state <code class="reqn">s_0</code>. For POMDPs the start state is the start belief <code class="reqn">b_0</code>.
</p>
<p>Note that for regret usually the optimal policy <code class="reqn">\pi^*</code> is used as the benchmark.
Since the optimal policy may not be known, regret relative to the best known policy can be used.
</p>


<h3>Value</h3>

<p>the regret as a difference of expected long-term rewards.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code>MDP2POMDP</code>,
<code>POMDP()</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>add_policy()</code>,
<code>plot_belief_space()</code>,
<code>projection()</code>,
<code>reachable_and_absorbing</code>,
<code>sample_belief_space()</code>,
<code>simulate_POMDP()</code>,
<code>solve_POMDP()</code>,
<code>solve_SARSOP()</code>,
<code>transition_graph()</code>,
<code>update_belief()</code>,
<code>value_function()</code>,
<code>write_POMDP()</code>
</p>
<p>Other MDP: 
<code>MDP()</code>,
<code>MDP2POMDP</code>,
<code>MDP_policy_functions</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>add_policy()</code>,
<code>gridworld</code>,
<code>reachable_and_absorbing</code>,
<code>simulate_MDP()</code>,
<code>solve_MDP()</code>,
<code>transition_graph()</code>,
<code>value_function()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(Tiger)

sol_optimal &lt;- solve_POMDP(Tiger)
sol_optimal

# perform exact value iteration for 10 epochs
sol_quick &lt;- solve_POMDP(Tiger, method = "enum", horizon = 10)
sol_quick

regret(sol_quick, benchmark = sol_optimal)
</code></pre>


</div>