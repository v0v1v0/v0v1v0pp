<div class="container">

<table style="width: 100%;"><tr>
<td>IntervalRegressionInternal</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>IntervalRegressionInternal</h2>

<h3>Description</h3>

<p>Solve the squared hinge loss interval regression problem for one
<code>regularization</code> parameter: w* = argmin_w L(w) + <code>regularization</code> *
||w||_1 where L(w) is the average squared hinge loss with respect
to the <code>targets</code>, and ||w||_1 is the L1-norm of the weight vector
(excluding the first element, which is the un-regularized
intercept or bias term). This function performs no scaling of
input <code>features</code>, and is meant for internal use only! To learn a
regression model, try <code>IntervalRegressionCV</code> or
<code>IntervalRegressionUnregularized</code>.</p>


<h3>Usage</h3>

<pre><code class="language-R">IntervalRegressionInternal(features, 
    targets, initial.param.vec, 
    regularization, threshold = 0.001, 
    max.iterations = 1000, 
    weight.vec = NULL, 
    Lipschitz = NULL, 
    verbose = 2, margin = 1, 
    biggest.crit = 100)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>features</code></td>
<td>
<p>Scaled numeric feature matrix (problems x <code>features</code>). The first
column/feature should be all ones and will not be regularized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>targets</code></td>
<td>
<p>Numeric target matrix (problems x 2).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial.param.vec</code></td>
<td>
<p>initial guess for weight vector (<code>features</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>regularization</code></td>
<td>
<p>Degree of L1-regularization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>When the stopping criterion gets below this <code>threshold</code>, the
algorithm stops and declares the solution as optimal.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iterations</code></td>
<td>
<p>If the algorithm has not found an optimal solution after this many
iterations, increase <code>Lipschitz</code> constant and <code>max.iterations</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight.vec</code></td>
<td>
<p>A numeric vector of weights for each training example.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Lipschitz</code></td>
<td>
<p>A numeric scalar or NULL, which means to compute <code>Lipschitz</code> as the
mean of the squared L2-norms of the rows of the feature matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Cat messages: for restarts and at the end if &gt;= 1, and for every
iteration if &gt;= 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>margin</code></td>
<td>
<p>Margin size hyper-parameter, default 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>biggest.crit</code></td>
<td>
<p>Restart FISTA with a bigger <code>Lipschitz</code> (smaller step size) if crit
gets larger than this.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Numeric vector of scaled weights w of the affine function f_w(X) =
X %*% w for a scaled feature matrix X with the first row entirely
ones.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking &lt;toby.hocking@r-project.org&gt; [aut, cre]</p>


</div>