<div class="container">

<table style="width: 100%;"><tr>
<td>gk</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Gustafson-Kessel Clustering
</h2>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Gustafson-Kessel (GK) clustering algorithm (Gustafson &amp; Kessel, 1979). Unlike FCM using the Euclidean distance, GK uses cluster specific Mahalanobis distance.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gk(x, centers, memberships, m=2,  
   dmetric="sqeuclidean", pw = 2, alginitv="kmpp", 
   alginitu="imembrand", nstart=1, iter.max=1e03, con.val=1e-09, 
   fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code>get.dmetrics</code> for the alternative options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code>get.algorithms</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>As an extension of basic FCM algorithm, Gustafson and Kessel (GK) clustering algorithm employs an adaptive distance norm in order to detect clusters with different geometrical shapes (Babuska, 2001; Correa et al, 2011). 
</p>
<p>The objective function of GK is:
</p>
<p><code class="reqn">J_{GK}(\mathbf{X}; \mathbf{V}, \mathbf{A}, \mathbf{U}) = \sum\limits_{i=1}^n \sum\limits_{j=1}^k  u_{ij}^m d_{A_j}(\vec{x}_i, \vec{v}_j)</code>
</p>
<p>In the above equation <code class="reqn">d_{A_j}(\vec{x}_i, \vec{v}_j)</code> is the Mahalanobis distance between the data object <code class="reqn">\vec{x}_j</code> and cluster prototype <code class="reqn">\vec{v}_i</code>.
</p>
<p><code class="reqn">d_{A_j}(\vec{x}_i, \vec{v}_j) = (\vec{x}_i - \vec{v}_j)^T \mathbf{A}_j (\vec{x}_i - \vec{v}_j)</code> 
</p>
<p>As it is understood from the above equation, each cluster has its own norm-inducing matrix in GK, so that the norm matrix <code class="reqn">\mathbf{A}</code> is a <em>k</em>-length tuples of the cluster-specific norm-inducing matrices:
</p>
<p><code class="reqn">\mathbf{A}=\{\mathbf{A}_1, \mathbf{A}_2, \dots, \mathbf{A}_k\}</code>. 
</p>
<p>The objective function of GK cannot be directly minimized with respect to <code class="reqn">\mathbf{A}_j</code> since it is
linear in <code class="reqn">\mathbf{A}_j</code>. For obtaining a feasible solution, <code class="reqn">\mathbf{A}_j</code> must be constrained in some way. One of the usual ways of accomplishing this is to constrain the determinant of <code class="reqn">\mathbf{A}_j</code> (Babuska, 2001):
</p>
<p><code class="reqn">\mid\mathbf{A}_j\mid=\rho_j \;,\; \rho_j &gt; 0 \;,\; 1 \leq j \leq k</code>
</p>
<p>Allowing the matrix <code class="reqn">\mathbf{A}_j</code> to vary with its determinant fixed corresponds to optimizing the shape of cluster while its volume remains constant. By using the Lagrange-multiplier method, the norm-inducing matrix for the cluster <code class="reqn">j</code> is defined as follows (Babuska, 2001):
</p>
<p><code class="reqn">\mathbf{A}_j = [\rho_i \; det(\mathbf{F}_j)]^{-1/n}{\mathbf{F}_j}^{-1}</code>,
</p>
<p>where:
</p>
<p><code class="reqn">n</code> is the number of data objects,
</p>
<p><code class="reqn">\rho_j</code> represents the volume of cluster <code class="reqn">j</code>,
</p>
<p><code class="reqn">\mathbf{F}_j</code> is the fuzzy covariance matrix for the cluster <code class="reqn">j</code>, and is calculated as follows:
</p>
<p><code class="reqn">\mathbf{F}_j = \frac{\sum\limits_{i=1}^n u_{ij}^m \; d^2(\vec{x}_i, \vec{v}_j)}{\sum\limits_{i=1}^n u_{ij}^m}</code>
</p>
<p><code class="reqn">m</code> is the fuzzifier to specify the amount of fuzziness for the clustering; <code class="reqn">1\leq m\leq \infty</code>. It is usually chosen as 2. 
</p>
<p>GK must satisfy the following constraints:
</p>
<p><code class="reqn">\sum\limits_{j=1}^k u_{ij} = 1 \;\;;\; 1 \leq i \leq n</code>
</p>
<p><code class="reqn">\sum\limits_{i=1}^n u_{ij} &gt; 0 \;\;;\; 1 \leq j \leq k</code>
</p>
<p>The objective function of GK is minimized by using the following update equations:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d_{A_j}(\vec{x}_i, \vec{v}_j)}{d_{A_l}(\vec{x}_i, \vec{v}_l)}\Big)^{1/(m-1)} \Bigg]^{-1} \;\;; 1 \leq i \leq n,\; 1 \leq l \leq k</code>
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n u_{ij}^m \vec{x}_i}{\sum\limits_{i=1}^n u_{ij}^m} \;\;; 1 \leq j \leq k</code>
</p>


<h3>Value</h3>

<p>an object of class ‘ppclust’, which is a list consists of the following items:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is ‘FCM’ with this function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>a string for the matched function call generating this ‘ppclust’ object.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Zeynel Cebeci &amp; Cagatay Cebeci
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, pp. 1027-1035. <a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>
</p>
<p>Gustafson, D. E. &amp; Kessel, W. C. (1979). Fuzzy clustering with a fuzzy covariance matrix. In <em>Proc. of IEEE Conf. on Decision and Control including the 17th Symposium on Adaptive Processes</em>, San Diego. pp. 761-766. <a href="https://doi.org/10.1109/CDC.1978.268028">doi:10.1109/CDC.1978.268028</a>
</p>
<p>Babuska, R. (2001). Fuzzy and neural control. DISC Course Lecture Notes. Delft University of Technology. Delft, the Netherlands. <a href="https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control">https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control</a>.
</p>
<p>Correa, C., Valero, C., Barreiro, P., Diago, M. P., &amp; Tardáguila, J. (2011). A comparison of fuzzy clustering algorithms applied to feature extraction on vineyard. In <em>Proc. of the 14th Conf. of the Spanish Assoc. for Artificial Intelligence</em>. <a href="http://oa.upm.es/9246/">http://oa.upm.es/9246/</a>
</p>


<h3>See Also</h3>

<p><code>ekm</code>,
<code>fcm</code>,
<code>fcm2</code>,
<code>fpcm</code>,
<code>fpppcm</code>,
<code>gg</code>,
<code>gkpfcm</code>,
<code>hcm</code>,
<code>pca</code>,
<code>pcm</code>,
<code>pcmr</code>,
<code>pfcm</code>,
<code>upfc</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=3)$v

# Initialize the membership degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=3)$u

# Run FCM with the initial prototypes and memberships
gk.res &lt;- gk(x, centers=v, memberships=u, m=2)

# Show the fuzzy membership degrees for the top 5 objects
head(gk.res$u, 5)

## End(Not run)
</code></pre>


</div>