<div class="container">

<table style="width: 100%;"><tr>
<td>results2table</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Obtains a dplyr data frame table object containing all the results of
an experiment
</h2>

<h3>Description</h3>

<p>This function produces a dplyr data frame table object with the
information on all iterations of an experiment. This type of objects
may be easier to manipulate in terms of querying these results,
particular for larger experiments involving lots of tasks, workflows
and metrics.
</p>


<h3>Usage</h3>

<pre><code class="language-R">results2table(res)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>res</code></td>
<td>

<p>This is a <code>ComparisonResults</code> object (type "class?ComparisonResults" for details)
that contains the results of a performance estimation  experiment obtained
through the <code>performanceEstimation()</code> function. 
</p>
</td>
</tr></table>
<h3>Value</h3>

<p>The function returns a dplyr data frame table object containing all
resutls of the experiment. The object has the columns: Task, Workflow,
nrIt, Metric and Score. Each row is one train+test cycle within the
experiment, i.e. contains the score of some metric obtained by some
workflow on one train+test iteration of a task.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code>getScores</code>, <code>performanceEstimation</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(swiss)

## running the estimation experiment
res &lt;- performanceEstimation(
  PredTask(Infant.Mortality ~ .,swiss,"Swiss"),
  workflowVariants(learner="svm",
                   learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
  EstimationTask(metrics=c("mse","nmae"),method=CV(nReps=2,nFolds=5))
  )

## Obtaining a table with the results
library(dplyr)
tbl &lt;- results2table(res)

## Mean and standard deviation of each workflow per task (only one in
## this example) and metric
group_by(tbl,Task,Workflow,Metric) 
     summarize_each_(funs(mean,sd),"Score")

## Top 2 workflows in terms of MSE for this task
filter(tbl,Task=="Swiss",Metric=="mse") 
    group_by(Workflow) 
      summarize_each_(funs(mean),"Score") 
        arrange(Score) 
          slice(1:2)

## End(Not run)
</code></pre>


</div>