<div class="container">

<table style="width: 100%;"><tr>
<td>p_logreg_xerrors</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Poolwise Logistic Regression with Normal Exposure Subject to Errors</h2>

<h3>Description</h3>

<p>Assumes normal linear model for exposure given covariates, and additive
normal processing errors and measurement errors acting on the poolwise mean
exposure. Manuscript fully describing the approach is under review.
</p>


<h3>Usage</h3>

<pre><code class="language-R">p_logreg_xerrors(g, y, xtilde, c = NULL, errors = "processing",
  nondiff_pe = TRUE, nondiff_me = TRUE, constant_pe = TRUE,
  prev = NULL, samp_y1y0 = NULL, approx_integral = TRUE,
  estimate_var = TRUE, start_nonvar_var = c(0.01, 1),
  lower_nonvar_var = c(-Inf, 1e-04), upper_nonvar_var = c(Inf, Inf),
  jitter_start = 0.01, hcubature_list = list(tol = 1e-08),
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)),
  nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Numeric vector with poolwise Y values, coded 0 if all members are
controls and 1 if all members are cases.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>c</code></td>
<td>
<p>Numeric matrix with poolwise <strong>C</strong> values (if any), with one
row for each pool. Can be a vector if there is only 1 covariate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"neither"</code>, <code>"processing"</code> for processing error
only, <code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nondiff_pe</code></td>
<td>
<p>Logical value for whether to assume the processing error
variance is non-differential, i.e. the same in case pools and control pools.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nondiff_me</code></td>
<td>
<p>Logical value for whether to assume the measurement error
variance is non-differential, i.e. the same in case pools and control pools.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constant_pe</code></td>
<td>
<p>Logical value for whether to assume the processing error
variance is constant with pool size. If <code>FALSE</code>, assumption is that
processing error variance increase with pool size such that, for example, the
processing error affecting a pool 2x as large as another has 2x the variance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prev</code></td>
<td>
<p>Numeric value specifying disease prevalence, allowing
for valid estimation of the intercept with case-control sampling. Can specify
<code>samp_y1y0</code> instead if sampling rates are known.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>samp_y1y0</code></td>
<td>
<p>Numeric vector of length 2 specifying sampling probabilities
for cases and controls, allowing for valid estimation of the intercept with
case-control sampling. Can specify <code>prev</code> instead if it's easier.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>approx_integral</code></td>
<td>
<p>Logical value for whether to use the probit
approximation for the logistic-normal integral, to avoid numerically
integrating X's out of the likelihood function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code>nlminb</code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hcubature_list</code></td>
<td>
<p>List of arguments to pass to
<code>hcubature</code> for numerical integration. Only used if
<code>approx_integral = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code>nlminb</code>
for log-likelihood maximization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code>hessian</code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlminb_object</code></td>
<td>
<p>Object returned from <code>nlminb</code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li>
<p> Numeric vector of parameter estimates.
</p>
</li>
<li>
<p> Variance-covariance matrix (if <code>estimate_var = TRUE</code>).
</p>
</li>
<li>
<p> Returned <code>nlminb</code> object from maximizing the
log-likelihood function.
</p>
</li>
<li>
<p> Akaike information criterion (AIC).
</p>
</li>
</ol>
<h3>References</h3>

<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) "Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers."
<em>Stat. Med.</em> <strong>29</strong>(5): 597–613.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (1999) "Using pooled exposure assessment to
improve efficiency in case-control studies." <em>Biometrics</em> <strong>55</strong>:
718–726.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (2014) "Correction to 'Using pooled exposure
assessment to improve efficiency in case-control studies' by Clarice R.
Weinberg and David M. Umbach; 55, 718–726, September 1999."
<em>Biometrics</em> <strong>70</strong>: 1061.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Load dataset containing (Y, Xtilde, C) values for pools of size 1, 2, and
# 3. Xtilde values are affected by processing error.
data(pdat1)

# Estimate log-OR for X and Y adjusted for C, ignoring processing error
fit1 &lt;- p_logreg_xerrors(
  g = pdat1$g,
  y = pdat1$allcases,
  xtilde = pdat1$xtilde,
  c = pdat1$c,
  errors = "neither"
)
fit1$theta.hat

# Repeat, but accounting for processing error. Closer to true log-OR of 0.5.
fit2 &lt;- p_logreg_xerrors(
  g = pdat1$g,
  y = pdat1$allcases,
  xtilde = pdat1$xtilde,
  c = pdat1$c,
  errors = "processing"
)
fit2$theta.hat


</code></pre>


</div>