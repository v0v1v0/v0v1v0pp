<div class="container">

<table style="width: 100%;"><tr>
<td>add_policy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Add a Policy to a POMDP Problem Description</h2>

<h3>Description</h3>

<p>Add a policy to a POMDP problem description allows the user to
test policies on modified problem descriptions or to test manually created
policies.
</p>


<h3>Usage</h3>

<pre><code class="language-R">add_policy(model, policy)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a POMDP or MDP model description.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>policy</code></td>
<td>
<p>a policy data.frame.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The model description with the added policy.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other POMDP: 
<code>MDP2POMDP</code>,
<code>POMDP()</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>plot_belief_space()</code>,
<code>projection()</code>,
<code>reachable_and_absorbing</code>,
<code>regret()</code>,
<code>sample_belief_space()</code>,
<code>simulate_POMDP()</code>,
<code>solve_POMDP()</code>,
<code>solve_SARSOP()</code>,
<code>transition_graph()</code>,
<code>update_belief()</code>,
<code>value_function()</code>,
<code>write_POMDP()</code>
</p>
<p>Other MDP: 
<code>MDP()</code>,
<code>MDP2POMDP</code>,
<code>MDP_policy_functions</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>gridworld</code>,
<code>reachable_and_absorbing</code>,
<code>regret()</code>,
<code>simulate_MDP()</code>,
<code>solve_MDP()</code>,
<code>transition_graph()</code>,
<code>value_function()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(Tiger)

sol &lt;- solve_POMDP(Tiger)
sol

# Example 1: Use the solution policy on a changed POMDP problem
#            where listening is perfect and simulate the expected reward

perfect_Tiger &lt;- Tiger
perfect_Tiger$observation_prob &lt;- list(
  listen = diag(1, length(perfect_Tiger$states), 
    length(perfect_Tiger$observations)), 
  `open-left` = "uniform",
  `open-right` = "uniform"
)

sol_perfect &lt;- add_policy(perfect_Tiger, sol)
sol_perfect

simulate_POMDP(sol_perfect, n = 1000)$avg_reward

# Example 2: Handcraft a policy and apply it to the Tiger problem

# original policy
policy(sol)
plot_value_function(sol)
plot_belief_space(sol)

# create a policy manually where the agent opens a door at a believe of 
#  roughly 2/3 (note the alpha vectors do not represent 
#  a valid value function)
p &lt;- list(
data.frame(
  `tiger-left` = c(1, 0, -2),
  `tiger-right` = c(-2, 0, 1), 
  action = c("open-right", "listen", "open-left"),
  check.names = FALSE
))
p

custom_sol &lt;- add_policy(Tiger, p)
custom_sol

policy(custom_sol)
plot_value_function(custom_sol)
plot_belief_space(custom_sol)

simulate_POMDP(custom_sol, n = 1000)$avg_reward
</code></pre>


</div>