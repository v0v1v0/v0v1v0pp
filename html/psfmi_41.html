<div class="container">

<table style="width: 100%;"><tr>
<td>pool_compare_models</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Compare the fit and performance of prediction models across Multipy Imputed data</h2>

<h3>Description</h3>

<p><code>pool_compare_model</code> Compares the fit and performance of prediction models 
in multiply imputed data sets by using clinical important performance measures
</p>


<h3>Usage</h3>

<pre><code class="language-R">pool_compare_models(
  pobj,
  compare.predictors = NULL,
  compare.group = NULL,
  cutoff = 0.5,
  boot_auc = FALSE,
  nboot = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pobj</code></td>
<td>
<p>An object of class <code>pmods</code> (pooled models), produced by a previous
call to <code>psfmi_lr</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compare.predictors</code></td>
<td>
<p>Character vector with the names of the predictors that are 
compared. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compare.group</code></td>
<td>
<p>Character vector with the names of the group of predictors that are 
compared. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cutoff</code></td>
<td>
<p>A numerical scalar. Cutoff used for the categorical NRI value. More than one
cutoff value can be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>boot_auc</code></td>
<td>
<p>If TRUE the standard error of the AUC is calculated with stratified
bootstrapping. If FALSE (is default), the standard error is calculated with De Long's
method.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nboot</code></td>
<td>
<p>A numerical scalar. The number of bootstrap samples for the AUC standard error, used 
when boot_auc is TRUE. Default is 1000.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The fit of the models are compared by using the D3 method for pooling Likelihood ratio 
statistics (method of Meng and Rubin). The pooled AIC difference is calculated according to
the formula <code>AIC = D - 2*p</code>, where D is the pooled likelihood ratio tests of 
constrained models (numerator in D3 statistic) and p is the difference in number of parameters 
between the full and restricted models that are compared. The pooled AUC difference  
is calculated, after the standard error is obtained in each imputed data set by method 
DeLong or bootstrapping. The NRI categorical and continuous and IDI are calculated in each 
imputed data set and pooled.
</p>


<h3>Value</h3>

<p>An object from which the following objects can be extracted: 
</p>

<ul>
<li>  <p><code>DR_stats</code> p-value of the D3 statistic, the D3 statistic, LRT fixed is the 
likelihood Ratio test value of the constrained models.
</p>
</li>
<li>  <p><code>stats_compare</code> Mean of LogLik0, LogLik1, AIC0, AIC1, AIC_diff values of the 
restricted (containing a 0) and full models (containing a 1).
</p>
</li>
<li>  <p><code>NRI</code> pooled values for the categorical and continuous Net Reclassification
improvement values and the Integrated Discrimination improvement.
</p>
</li>
<li>  <p><code>AUC_stats</code> Pooled Area Under the Curve of restricted and full models. 
</p>
</li>
<li>  <p><code>AUC_diff</code> Pooled difference in AUC. 
</p>
</li>
<li>  <p><code>formula_test</code> regression formula of full model.
</p>
</li>
<li>  <p><code>cutoff</code> Cutoff value used for reclassification values.
</p>
</li>
<li>  <p><code>formula_null</code> regression formula of null model
</p>
</li>
<li>  <p><code>compare_predictors</code> Predictors used in full model.
</p>
</li>
<li>  <p><code>compare_group</code> group of predictors used in full model.
</p>
</li>
</ul>
<h3>References</h3>

<p>Eekhout I, van de Wiel MA, Heymans MW. Methods for significance testing of categorical
covariates in logistic regression models after multiple imputation: power and applicability
analysis. BMC Med Res Methodol. 2017;17(1):129.
</p>
<p>Consentino F, Claeskens G. Order Selection tests with multiply imputed data
Computational Statistics and Data Analysis.2010;54:2284-2295.
</p>


<h3>Examples</h3>

<pre><code class="language-R"> pool_lr &lt;- psfmi_lr(data=lbpmilr, p.crit = 1, direction="FW", nimp=10, impvar="Impnr", 
 Outcome="Chronic", predictors=c("Radiation"), cat.predictors = ("Satisfaction"),
 int.predictors = NULL, spline.predictors="Tampascale", nknots=3, method="D1")

 res_compare &lt;- pool_compare_models(pool_lr, compare.predictors = c("Pain", "Duration", 
 "Function"), cutoff = 0.4)
 res_compare

 
</code></pre>


</div>