<div class="container">

<table style="width: 100%;"><tr>
<td>MDP2POMDP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Convert between MDPs and POMDPs</h2>

<h3>Description</h3>

<p>Convert a MDP into POMDP by adding an observation model or
a POMDP into a MDP by making the states observable.
</p>


<h3>Usage</h3>

<pre><code class="language-R">make_partially_observable(x, observations = NULL, observation_prob = NULL)

make_fully_observable(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a <code>MDP</code> object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>observations</code></td>
<td>
<p>a character vector specifying the names of the available
observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>observation_prob</code></td>
<td>
<p>Specifies the observation probabilities (see POMDP for details).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>make_partially_observable()</code> adds an observation model to an MDP. If no observations and
observation probabilities are provided, then an observation for each state is created
with identity observation matrices. This means we have a fully observable model
encoded as a POMDP.
</p>
<p><code>make_fully_observable()</code> removes the observation model from a POMDP and returns
an MDP.
</p>


<h3>Value</h3>

<p>a <code>MDP</code> or a <code>POMDP</code> object.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code>MDP()</code>,
<code>MDP_policy_functions</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>add_policy()</code>,
<code>gridworld</code>,
<code>reachable_and_absorbing</code>,
<code>regret()</code>,
<code>simulate_MDP()</code>,
<code>solve_MDP()</code>,
<code>transition_graph()</code>,
<code>value_function()</code>
</p>
<p>Other POMDP: 
<code>POMDP()</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>add_policy()</code>,
<code>plot_belief_space()</code>,
<code>projection()</code>,
<code>reachable_and_absorbing</code>,
<code>regret()</code>,
<code>sample_belief_space()</code>,
<code>simulate_POMDP()</code>,
<code>solve_POMDP()</code>,
<code>solve_SARSOP()</code>,
<code>transition_graph()</code>,
<code>update_belief()</code>,
<code>value_function()</code>,
<code>write_POMDP()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Turn the Maze MDP into a partially observable problem.
# Here each state has an observation, so it is still a fully observable problem
# encoded as a POMDP.
data("Maze")
Maze

Maze_POMDP &lt;- make_partially_observable(Maze)
Maze_POMDP

sol &lt;- solve_POMDP(Maze_POMDP)
policy(sol)
simulate_POMDP(sol, n = 1, horizon = 100, return_trajectories = TRUE)$trajectories

# Make the Tiger POMDP fully observable
data("Tiger")
Tiger

Tiger_MDP &lt;- make_fully_observable(Tiger)
Tiger_MDP

sol &lt;- solve_MDP(Tiger_MDP)
policy(sol)
# The result is not exciting since we can observe where the tiger is!
</code></pre>


</div>