<div class="container">

<table style="width: 100%;"><tr>
<td>JSD</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Jensen-Shannon Divergence</h2>

<h3>Description</h3>

<p>This function computes a distance matrix or distance value based on the Jensen-Shannon Divergence with equal weights.
</p>


<h3>Usage</h3>

<pre><code class="language-R">JSD(x, test.na = TRUE, unit = "log2", est.prob = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric <code>data.frame</code> or <code>matrix</code> (storing probability vectors) or a numeric <code>data.frame</code> or <code>matrix</code> storing counts (if <code>est.prob = TRUE</code>). See <code>distance</code> for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test.na</code></td>
<td>
<p>a boolean value specifying whether input vectors shall be tested for NA values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>est.prob</code></td>
<td>
<p>method to estimate probabilities from input count vectors such as non-probability vectors. Default: <code>est.prob = NULL</code>. Options are:
</p>

<ul><li> <p><code>est.prob = "empirical"</code>: The relative frequencies of each vector are computed internally. For example an input matrix <code>rbind(1:10, 11:20)</code> will be transformed to a probability vector <code>rbind(1:10 / sum(1:10), 11:20 / sum(11:20))</code>
</p>
</li></ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Function to compute the Jensen-Shannon Divergence JSD(P || Q) between two
probability distributions P and Q with equal weights <code class="reqn">\pi_1</code> =
<code class="reqn">\pi_2</code> = <code class="reqn">1/2</code>.
</p>
<p>The Jensen-Shannon Divergence JSD(P || Q) between two probability
distributions P and Q is defined as:
</p>
<p style="text-align: center;"><code class="reqn">JSD(P || Q) = 0.5 * (KL(P || R) + KL(Q || R))</code>
</p>

<p>where <code class="reqn">R = 0.5 * (P + Q)</code> denotes the mid-point of the probability
vectors P and Q, and KL(P || R), KL(Q || R) denote the Kullback-Leibler
Divergence of P and R, as well as Q and R.
</p>
<p><strong>General properties of the Jensen-Shannon Divergence:</strong>
</p>

<ul>
<li> <p><code>1)</code> JSD is non-negative.
</p>
</li>
<li> <p><code>2)</code> JSD is a symmetric measure JSD(P || Q) = JSD(Q || P).
</p>
</li>
<li> <p><code>3)</code> JSD = 0, if and only if P = Q.
</p>
</li>
</ul>
<h3>Value</h3>

<p>a distance value or matrix based on JSD computations.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Lin J. 1991. "Divergence Measures Based on the Shannon Entropy".
IEEE Transactions on Information Theory. (33) 1: 145-151.
</p>
<p>Endres M. and Schindelin J. E. 2003. "A new metric for probability
distributions". IEEE Trans. on Info. Thy. (49) 3: 1858-1860.
</p>


<h3>See Also</h3>

<p><code>KL</code>, <code>H</code>, <code>CE</code>, <code>gJSD</code>, <code>distance</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Jensen-Shannon Divergence between P and Q
P &lt;- 1:10/sum(1:10)
Q &lt;- 20:29/sum(20:29)
x &lt;- rbind(P,Q)
JSD(x)

# Jensen-Shannon Divergence between P and Q using different log bases
JSD(x, unit = "log2") # Default
JSD(x, unit = "log")
JSD(x, unit = "log10")

# Jensen-Shannon Divergence Divergence between count vectors P.count and Q.count
P.count &lt;- 1:10
Q.count &lt;- 20:29
x.count &lt;- rbind(P.count,Q.count)
JSD(x.count, est.prob = "empirical")

# Example: Distance Matrix using JSD-Distance

Prob &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29), 30:39/sum(30:39))

# compute the KL matrix of a given probability matrix
JSDMatrix &lt;- JSD(Prob)

# plot a heatmap of the corresponding JSD matrix
heatmap(JSDMatrix)

</code></pre>


</div>