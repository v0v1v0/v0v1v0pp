<div class="container">

<table style="width: 100%;"><tr>
<td>topPerformer</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Obtain the workflow that best performed in terms of a metric on a task
</h2>

<h3>Description</h3>

<p>This function can be used to obtain the workflow (an object of class
<code>Workflow</code>) that performed better in terms of a
given metric on a certain task. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">topPerformer(compRes,metric,task,max=FALSE,stat="avg") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>compRes</code></td>
<td>

<p>A <code>ComparisonResults</code> object with the results of your experimental comparison.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>

<p>A string with the name of a metric estimated in the comparison
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>task</code></td>
<td>

<p>A string with the name of a predictive task used in the comparison
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max</code></td>
<td>

<p>A boolean (defaulting to <code>FALSE</code>) indicating the meaning of
best performance for the selected metric. If this is <code>FALSE</code> it
means that the goal is to minimize this metric, otherwise it means
that the metric is to be maximized.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stat</code></td>
<td>

<p>The statistic to be used to obtain the ranks. The options are the
statistics produced by the function <code>summary</code> applied to objects
of class  <code>ComparisonResults</code>, i.e. "avg", "std",
"med", "iqr", "min", "max" or "invalid" (defaults to "avg").
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This is an utility function that can be used to obtain the workflow
(an object of class <code>Workflow</code>) that achieved the
best performance on a given predictive task in terms of a certain
evaluation metric.  The notion of <em>best performance</em> depends on
the type of evaluation metric, thus the need for the <code>max</code>
argument. Some evaluation statistics are to be maximized
(e.g. accuracy), while others are to be minimized (e.g. mean squared
error). For the former you should use <code>max=TRUE</code>, while the
latter require <code>max=FALSE</code> (the default).
</p>


<h3>Value</h3>

<p>The function returns an object of class <code>Workflow</code> .
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code>performanceEstimation</code>,
<code>topPerformers</code>,
<code>rankWorkflows</code>,
<code>metricsSummary</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
## Estimating several evaluation metrics on different variants of a
## regression tree and of a SVM, on  two data sets, using one repetition
## of  10-fold CV

data(swiss)
data(mtcars)
library(e1071)

## run the experimental comparison
results &lt;- performanceEstimation(
               c(PredTask(Infant.Mortality ~ ., swiss),
                 PredTask(mpg ~ ., mtcars)),
               c(workflowVariants(learner='svm',
                                  learner.pars=list(cost=c(1,5),gamma=c(0.1,0.01))
                                 )
               ),
               EstimationTask(metrics=c("mse","mae"),method=CV(nReps=2,nFolds=5))
                                 )

## get the top performer workflow for a metric and task
topPerformer(results,"mse","swiss.Infant.Mortality")

## End(Not run)
</code></pre>


</div>