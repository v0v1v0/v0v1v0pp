<div class="container">

<table style="width: 100%;"><tr>
<td>stability</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate the stability based statistics</h2>

<h3>Description</h3>

<p>Calculate the  the stability based statistics. We try with various tuning parameter values, obtaining their corresponding statbility based statistics average prediction strengths, then choose the set of the tuning parameters with the maximum average prediction stength.
</p>


<h3>Usage</h3>

<pre><code class="language-R">stability(data,rho,lambda,tau,
    loss.function = c("quadratic","L1","MCP","SCAD"),
    grouping.penalty = c("gtlp","tlp"), 
    algorithm = c("DCADMM","Quadratic"),
    epsilon = 0.001,n.times = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>Input matrix. Each column is an observation vector.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>

<p>Tuning parameter or step size: rho, typically set at 1 for quadratic penalty based algorithm; 0.4 for DC-ADMM. (Note that rho is the lambda1 in quadratic penalty based algorithm.)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>

<p>Tuning parameter: lambda, the magnitude of grouping penalty.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>

<p>Tuning parameter: tau, a nonnegative tuning parameter controll ing the trade-off between the model fit and the number of clusters.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss.function </code></td>
<td>

<p>The loss function. "L1" stands for <code class="reqn">L_1</code> loss function, while "quadratic" stands for the quadratic loss function.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>grouping.penalty</code></td>
<td>
<p>Grouping penalty. Character: may be abbreviated. "gtlp" means generalized group lasso is used for grouping penalty. "lasso" means lasso is used for grouping penalty. "SCAD" and "MCP" are two other non-convex penalty.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algorithm</code></td>
<td>

<p>Two algorithms for PRclust. "DC-ADMM" and "Quadratic" stand for the DC-ADMM and quadratic penalty based criterion respectively. "DC-ADMM" is much faster than "Quadratic" and thus recommend it here. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>The stopping critetion parameter corresponding to DC-ADMM. The default is 0.001.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.times</code></td>
<td>
<p>Repeat times. Based on our limited simulations, we find 10 is usually good enough.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A generalized degrees of freedom (GDF) together with generalized cross validation (GCV) was proposed for selection of tuning parameters for clustering (Pan et al., 2013). This method, while yielding good performance, requires extensive computation and specification of a hyper-parameter perturbation size. Here, we provide an alternative by modifying a stability-based criterion (Tibshirani and Walther, 2005; Liu et al., 2016) for determining the tuning parameters.
</p>
<p>The main idea of the method is based on cross-validation. That is, (1) randomly partition the entire data set into a training set and a test set with an almost equal size; (2) cluster the training and test sets separately via PRclust with the same tuning parameters; (3) measure how well the training set clusters predict the test clusters.
</p>
<p>We try with various tuning parameter values, obtaining their corresponding statbility based statistics average prediction strengths, then choose the set of the tuning parameters with the maximum average prediction stength.
</p>


<h3>Value</h3>

<p>Return value: the average prediction score.
</p>


<h3>Author(s)</h3>

<p>Chong Wu
</p>


<h3>References</h3>

<p>Wu, C., Kwon, S., Shen, X., &amp; Pan, W. (2016). A New Algorithm and Theory for Penalized Regression-based Clustering. <em>Journal of Machine Learning Research</em>, 17(188), 1-25.
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(1)
library("prclust")
data = matrix(NA,2,50)
data[1,1:25] = rnorm(25,0,0.33)
data[2,1:25] = rnorm(25,0,0.33)
data[1,26:50] = rnorm(25,1,0.33)
data[2,26:50] = rnorm(25,1,0.33)

#case 1
stab1 = stability(data,rho=1,lambda=1,tau=0.5,n.times = 2)
stab1

#case 2
stab2 = stability(data,rho=1,lambda=0.7,tau=0.3,n.times = 2)
stab2
# Note that the combination of tuning parameters in case 1 are better than 
# the combination of tuning parameters in case 2 since the value of GCV in case 1 is
# less than the value in case 2.
</code></pre>


</div>