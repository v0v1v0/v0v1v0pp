<div class="container">

<table style="width: 100%;"><tr>
<td>pedmod_sqn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Optimize the Log Marginal Likelihood Using a Stochastic Quasi-Newton Method</h2>

<h3>Description</h3>

<p>Optimizes <code>eval_pedigree_ll</code> and <code>eval_pedigree_grad</code>
using a stochastic quasi-Newton method.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pedmod_sqn(
  ptr,
  par,
  maxvls,
  abs_eps,
  rel_eps,
  step_factor,
  n_it,
  n_grad_steps,
  indices = NULL,
  minvls = -1L,
  n_grad = 50L,
  n_hess = 500L,
  do_reorder = TRUE,
  use_aprx = FALSE,
  n_threads = 1L,
  cluster_weights = NULL,
  fix = NULL,
  standardized = FALSE,
  minvls_hess = minvls,
  maxvls_hess = maxvls,
  abs_eps_hess = abs_eps,
  rel_eps_hess = rel_eps,
  verbose = FALSE,
  method = 0L,
  check_every = 2L * n_grad_steps,
  use_tilting = FALSE,
  vls_scales = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>ptr</code></td>
<td>
<p>object from <code>pedigree_ll_terms</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>par</code></td>
<td>
<p>starting values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxvls</code></td>
<td>
<p>maximum number of samples in the approximation for each
marginal likelihood term.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>abs_eps</code></td>
<td>
<p>absolute convergence threshold for
<code>eval_pedigree_ll</code> and <code>eval_pedigree_grad</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rel_eps</code></td>
<td>
<p>rel_eps convergence threshold for
<code>eval_pedigree_ll</code> and <code>eval_pedigree_grad</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step_factor</code></td>
<td>
<p>factor used for the step size. The step size is
<code>step_factor</code> divided by the iteration number.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_it</code></td>
<td>
<p>number of stochastic gradient steps to make.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_grad_steps</code></td>
<td>
<p>number of stochastic gradient steps to make between each
Hessian approximation update.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>indices</code></td>
<td>
<p>zero-based vector with indices of which log marginal
likelihood terms to include. Use <code>NULL</code> if all indices should be
used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minvls</code></td>
<td>
<p>minimum number of samples for each
marginal likelihood term. Negative values provides a
default which depends on the dimension of the integration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_grad</code></td>
<td>
<p>number of log marginal likelihood terms to include in the
stochastic gradient step.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_hess</code></td>
<td>
<p>number of log marginal likelihood terms to include in the
gradients used for the Hessian approximation update. This is set to the
entire sample (or <code>indices</code>) if this is greater than or equal to half
the number of log marginal likelihood terms.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>do_reorder</code></td>
<td>
<p><code>TRUE</code> if a heuristic variable reordering should
be used. <code>TRUE</code> is likely the best value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_aprx</code></td>
<td>
<p><code>TRUE</code> if a less precise approximation of
<code>pnorm</code> and <code>qnorm</code> should be used. This may
reduce the computation time while not affecting the result much.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_threads</code></td>
<td>
<p>number of threads to use.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cluster_weights</code></td>
<td>
<p>numeric vector with weights for each cluster. Use
<code>NULL</code> if all clusters have weight one.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fix</code></td>
<td>
<p>integer vector with indices of <code>par</code> to fix. This is useful
for computing profile likelihoods. <code>NULL</code> yields all parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardized</code></td>
<td>
<p>logical for whether to use the standardized or direct
parameterization. See <code>standardized_to_direct</code> and the vignette
at <code>vignette("pedmod", package = "pedmod")</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minvls_hess</code></td>
<td>
<p><code>minvls</code> argument to use when updating the Hessian
approximation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxvls_hess</code></td>
<td>
<p><code>maxvls</code> argument to use when updating the Hessian
approximation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>abs_eps_hess</code></td>
<td>
<p><code>abs_eps</code> argument to use when updating the Hessian
approximation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rel_eps_hess</code></td>
<td>
<p><code>rel_eps</code> argument to use when updating the Hessian
approximation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical for whether to print output during the estimation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>integer with the method to use. Zero yields randomized Korobov
lattice rules while one yields scrambled Sobol sequences.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>check_every</code></td>
<td>
<p>integer for the number of gradient steps between checking
that the likelihood did increase. If not, the iterations are reset and the
step-size is halved.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_tilting</code></td>
<td>
<p><code>TRUE</code> if the minimax tilting method suggested
by Botev (2017) should be used. See <a href="https://doi.org/10.1111/rssb.12162">doi:10.1111/rssb.12162</a>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vls_scales</code></td>
<td>
<p>can be a numeric vector with a positive scalar for each
cluster. Then <code>vls_scales[i] * minvls</code> and
<code>vls_scales[i] * maxvls</code> is used for cluster <code>i</code> rather than
<code>minvls</code> and <code>maxvls</code>. Set <code>vls_scales = NULL</code> if the latter
should be used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function uses a stochastic quasi-Newton method like suggested by
Byrd et al. (2016) with a few differences: Differences in gradients are
used rather than Hessian-vector products, BFGS rather than L-BFGS is used
because the problem is typically low dimensional, and damped BFGS updates
are used (see e.g. chapter 18 of Nocedal and Wright, 2006).
</p>
<p>Separate arguments for the gradient approximation in the Hessian update are
provided as one may want a more precise approximation for these gradients.
<code>step_factor</code> likely depends on the other parameters and the data set
and should be altered.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>par</code></td>
<td>
<p>estimated parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>omegas</code></td>
<td>
<p>parameter estimates after each iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>H</code></td>
<td>
<p>Hessian approximation in the quasi-Newton method. It should not
be treated as the Hessian.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Byrd, R. H., Hansen, S. L., Nocedal, J., &amp; Singer, Y. (2016).
<em>A stochastic quasi-Newton method for large-scale optimization</em>.
SIAM Journal on Optimization, 26(2), 1008-1031.
</p>
<p>Nocedal, J., &amp; Wright, S. (2006). <em>Numerical optimization</em>.
Springer Science &amp; Business Media.
</p>


<h3>See Also</h3>

<p><code>pedmod_opt</code> and <code>pedmod_start</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# we simulate outcomes with an additive genetic effect. The kinship matrix is
# the same for all families and given by
K &lt;- matrix(c(
  0.5  , 0    , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0    , 0.5  , 0.25 , 0   , 0.25 , 0   , 0.125 , 0.125 , 0.125 , 0.125 ,
  0.25 , 0.25 , 0.5  , 0   , 0.25 , 0   , 0.25  , 0.25  , 0.125 , 0.125 ,
  0    , 0    , 0    , 0.5 , 0    , 0   , 0.25  , 0.25  , 0     , 0     ,
  0.25 , 0.25 , 0.25 , 0   , 0.5  , 0   , 0.125 , 0.125 , 0.25  , 0.25  ,
  0    , 0    , 0    , 0   , 0    , 0.5 , 0     , 0     , 0.25  , 0.25  ,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.5   , 0.25  , 0.0625, 0.0625,
  0.125, 0.125, 0.25 , 0.25, 0.125, 0   , 0.25  , 0.5   , 0.0625, 0.0625,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.5   , 0.25  ,
  0.125, 0.125, 0.125, 0   , 0.25 , 0.25, 0.0625, 0.0625, 0.25  , 0.5
), 10)

# simulates a data set.
#
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameter.
sim_dat &lt;- function(n_fams, beta = c(-1, 1, 2), sig_sq = 3){
  # setup before the simulations
  Cmat &lt;- 2 * K
  n_obs &lt;- NROW(K)
  Sig &lt;- diag(n_obs) + sig_sq * Cmat
  Sig_chol &lt;- chol(Sig)

  # simulate the data
  out &lt;- replicate(
    n_fams, {
      # simulate covariates
      X &lt;- cbind(`(Intercept)` = 1, Continuous = rnorm(n_obs),
                 Binary = runif(n_obs) &gt; .5)

      # assign the linear predictor + noise
      eta &lt;- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)

      # return the list in the format needed for the package
      list(y = as.numeric(eta &gt; 0), X = X, scale_mats = list(Cmat))
    }, simplify = FALSE)

  # add attributes with the true values and return
  attributes(out) &lt;- list(beta = beta, sig_sq = sig_sq)
  out
}

# simulate the data
set.seed(1)
dat &lt;- sim_dat(100L)

# fit the model
ptr &lt;- pedigree_ll_terms(dat, max_threads = 1L)
start &lt;- pedmod_start(ptr = ptr, data = dat, n_threads = 1L)
fit &lt;- pedmod_sqn(ptr = ptr, par = start$par, n_threads = 1L, use_aprx = TRUE,
                  maxvls = 5000L, minvls = 1000L, abs_eps = 0, rel_eps = 1e-3,
                  n_grad_steps = 20L, step_factor = 1, n_grad = 10L,
                  n_hess = 50L, check_every = 50L, n_it = 1000L)
fit$par # maximum likelihood estimate
# the maximum likelihood
eval_pedigree_ll(ptr = ptr, fit$par, maxvls = 5000L, abs_eps = 0,
                 rel_eps = 1e-3, minvls = 1000L)

</code></pre>


</div>