<div class="container">

<table style="width: 100%;"><tr>
<td>KL</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler Divergence</h2>

<h3>Description</h3>

<p>This function computes the Kullback-Leibler divergence of two probability
distributions P and Q.
</p>


<h3>Usage</h3>

<pre><code class="language-R">KL(x, test.na = TRUE, unit = "log2", est.prob = NULL, epsilon = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric <code>data.frame</code> or <code>matrix</code> (storing probability vectors) or a numeric <code>data.frame</code> or <code>matrix</code> storing counts (if <code>est.prob = TRUE</code>). See <code>distance</code> for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test.na</code></td>
<td>
<p>a boolean value indicating whether input vectors should be tested for NA values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>est.prob</code></td>
<td>
<p>method to estimate probabilities from a count vector. Default: est.prob = NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>a small value to address cases in the KL computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">KL(P||Q) = \sum P(P) * log2(P(P) / P(Q)) = H(P,Q) - H(P)</code>
</p>

<p>where H(P,Q) denotes the joint entropy of the probability
distributions P and Q and H(P) denotes the entropy of
probability distribution P. In case P = Q then KL(P,Q) = 0 and in case P !=
Q then KL(P,Q) &gt; 0.
</p>
<p>The KL divergence is a non-symmetric measure of the directed divergence
between two probability distributions P and Q. It only fulfills the
<em>positivity</em> property of a <em>distance metric</em>.
</p>
<p>Because of the relation KL(P||Q) = H(P,Q) - H(P), the Kullback-Leibler
divergence of two probability distributions P and Q is also named
<em>Cross Entropy</em> of two probability distributions P and Q.
</p>


<h3>Value</h3>

<p>The Kullback-Leibler divergence of probability vectors.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Cover Thomas M. and Thomas Joy A. 2006. Elements of Information
Theory. <em>John Wiley &amp; Sons</em>.
</p>


<h3>See Also</h3>

<p><code>H</code>, <code>CE</code>, <code>JSD</code>, <code>gJSD</code>, <code>distance</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Kulback-Leibler Divergence between P and Q
P &lt;- 1:10/sum(1:10)
Q &lt;- 20:29/sum(20:29)
x &lt;- rbind(P,Q)
KL(x)

# Kulback-Leibler Divergence between P and Q using different log bases
KL(x, unit = "log2") # Default
KL(x, unit = "log")
KL(x, unit = "log10")

# Kulback-Leibler Divergence between count vectors P.count and Q.count
P.count &lt;- 1:10
Q.count &lt;- 20:29
x.count &lt;- rbind(P.count,Q.count)
KL(x, est.prob = "empirical")

# Example: Distance Matrix using KL-Distance

Prob &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29), 30:39/sum(30:39))

# compute the KL matrix of a given probability matrix
KLMatrix &lt;- KL(Prob)

# plot a heatmap of the corresponding KL matrix
heatmap(KLMatrix)


</code></pre>


</div>