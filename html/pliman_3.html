<div class="container">

<table style="width: 100%;"><tr>
<td>analyze_objects</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Analyzes objects in an image</h2>

<h3>Description</h3>


<ul>
<li> <p><code>analyze_objects()</code> provides tools for counting and extracting object
features (e.g., area, perimeter, radius, pixel intensity) in an image. See
more at the <strong>Details</strong> section.
</p>
</li>
<li> <p><code>analyze_objects_iter()</code> provides an iterative section to measure object
features using an object with a known area.
</p>
</li>
<li> <p><code>plot.anal_obj()</code> produces a histogram for the R, G, and B values when
argument <code>object_index</code> is used in the function <code>analyze_objects()</code>.
</p>
</li>
</ul>
<h3>Usage</h3>

<pre><code class="language-R">analyze_objects(
  img,
  foreground = NULL,
  background = NULL,
  pick_palettes = FALSE,
  segment_objects = TRUE,
  viewer = get_pliman_viewer(),
  reference = FALSE,
  reference_area = NULL,
  back_fore_index = "R/(G/B)",
  fore_ref_index = "B-R",
  reference_larger = FALSE,
  reference_smaller = FALSE,
  pattern = NULL,
  parallel = FALSE,
  workers = NULL,
  watershed = TRUE,
  veins = FALSE,
  sigma_veins = 1,
  ab_angles = FALSE,
  ab_angles_percentiles = c(0.25, 0.75),
  width_at = FALSE,
  width_at_percentiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
  haralick = FALSE,
  har_nbins = 32,
  har_scales = 1,
  har_band = 1,
  pcv = FALSE,
  pcv_niter = 100,
  resize = FALSE,
  trim = FALSE,
  fill_hull = FALSE,
  filter = FALSE,
  invert = FALSE,
  object_size = "medium",
  index = "NB",
  r = 1,
  g = 2,
  b = 3,
  re = 4,
  nir = 5,
  object_index = NULL,
  pixel_level_index = FALSE,
  return_mask = FALSE,
  efourier = FALSE,
  nharm = 10,
  threshold = "Otsu",
  k = 0.1,
  windowsize = NULL,
  tolerance = NULL,
  extension = NULL,
  lower_noise = 0.1,
  lower_size = NULL,
  upper_size = NULL,
  topn_lower = NULL,
  topn_upper = NULL,
  lower_eccent = NULL,
  upper_eccent = NULL,
  lower_circ = NULL,
  upper_circ = NULL,
  randomize = TRUE,
  nrows = 1000,
  plot = TRUE,
  show_original = TRUE,
  show_chull = FALSE,
  show_contour = TRUE,
  contour_col = "red",
  contour_size = 1,
  show_lw = FALSE,
  show_background = TRUE,
  show_segmentation = FALSE,
  col_foreground = NULL,
  col_background = NULL,
  marker = FALSE,
  marker_col = NULL,
  marker_size = NULL,
  save_image = FALSE,
  prefix = "proc_",
  dir_original = NULL,
  dir_processed = NULL,
  verbose = TRUE
)

## S3 method for class 'anal_obj'
plot(
  x,
  which = "measure",
  measure = "area",
  type = c("density", "histogram"),
  ...
)

analyze_objects_iter(pattern, known_area, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>img</code></td>
<td>
<p>The image to be analyzed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>foreground, background</code></td>
<td>
<p>A color palette for the foregrond and background,
respectively (optional). If a chacarceter is used (eg., <code>foreground = "fore"</code>), the function will search in the current working directory a valid
image named "fore".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pick_palettes</code></td>
<td>
<p>Logical argument indicating wheater the user needs to
pick up the color palettes for foreground and background for the image. If
<code>TRUE</code> <code>pick_palette()</code> will be called internally so that the user can sample
color points representing foreground and background.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>segment_objects</code></td>
<td>
<p>Segment objects in the image? Defaults to <code>TRUE</code>. In
this case, objects are segmented using the index defined in the <code>index</code>
argument, and each object is analyzed individually. If <code>segment_objects = FALSE</code> is used, the objects are not segmented and the entire image is
analyzed. This is useful, for example, when analyzing an image without
background, where an <code>object_index</code> could be computed for the entire image,
like the index of a crop canopy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>viewer</code></td>
<td>
<p>The viewer option. This option controls the type of viewer to
use for interactive plotting (eg., when <code>pick_palettes = TRUE</code>).  If not
provided, the value is retrieved using <code>get_pliman_viewer()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reference</code></td>
<td>
<p>Logical to indicate if a reference object is present in the
image. This is useful to adjust measures when images are not obtained with
standard resolution (e.g., field images). See more in the details section.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reference_area</code></td>
<td>
<p>The known area of the reference objects. The measures of
all the objects in the image will be corrected using the same unit of the
area informed here.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>back_fore_index</code></td>
<td>
<p>A character value to indicate the index to segment the
foreground (objects and reference) from the background. Defaults to
<code>"R/(G/B)"</code>. This index is optimized to segment white backgrounds from green
leaves and a blue reference object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fore_ref_index</code></td>
<td>
<p>A character value to indicate the index to segment
objects and the reference object. It can be either an available index in
<code>pliman</code> (see <code>pliman_indexes()</code> or an own index computed with the R, G, and
B bands. Defaults to <code>"B-R"</code>. This index is optimized to segment green
leaves from a blue reference object after a white background has been
removed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reference_larger, reference_smaller</code></td>
<td>
<p>Logical argument indicating when the
larger/smaller object in the image must be used as the reference object.
This only is valid when <code>reference</code> is set to <code>TRUE</code> and <code>reference_area</code>
indicates the area of the reference object. IMPORTANT. When
<code>reference_smaller</code> is used, objects with an area smaller than 1% of the
mean of all the objects are ignored. This is used to remove possible noise
in the image such as dust. So, be sure the reference object has an area that
will be not removed by that cutpoint.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pattern</code></td>
<td>
<p>A pattern of file name used to identify images to be imported.
For example, if <code>pattern = "im"</code> all images in the current working directory
that the name matches the pattern (e.g., img1.-, image1.-, im2.-) will be
imported as a list. Providing any number as pattern (e.g., <code>pattern = "1"</code>)
will select images that are named as 1.-, 2.-, and so on. An error will be
returned if the pattern matches any file that is not supported (e.g.,
img1.pdf).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>If <code>TRUE</code> processes the images asynchronously (in parallel) in
separate R sessions running in the background on the same machine. It may
speed up the processing time, especially when <code>pattern</code> is used is informed.
When <code>object_index</code> is informed, multiple sections will be used to extract
the RGB values for each object in the image. This may significantly speed up
processing time when an image has lots of objects (say &gt;1000).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>workers</code></td>
<td>
<p>A positive numeric scalar or a function specifying the number
of parallel processes that can be active at the same time. By default, the
number of sections is set up to 30% of available cores.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>watershed</code></td>
<td>
<p>If <code>TRUE</code> (default) performs watershed-based object
detection. This will detect objects even when they are touching one other.
If <code>FALSE</code>, all pixels for each connected set of foreground pixels are set
to a unique object. This is faster but is not able to segment touching
objects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>veins</code></td>
<td>
<p>Logical argument indicating whether vein features are computed.
This will call <code>object_edge()</code> and applies the Sobel-Feldman Operator to
detect edges. The result is the proportion of edges in relation to the
entire area of the object(s) in the image. Note that <strong>THIS WILL BE AN
OPERATION ON AN IMAGE LEVEL, NOT OBJECT!</strong>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma_veins</code></td>
<td>
<p>Gaussian kernel standard deviation used in the gaussian
blur in the edge detection algorithm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ab_angles</code></td>
<td>
<p>Logical argument indicating whether apex and base angles
should be computed. Defaults to <code>FALSE</code>. If <code>TRUE</code>, <code>poly_apex_base_angle()</code>
are called and the base and apex angles are computed considering the 25th
and 75th percentiles of the object height. These percentiles can be changed
with the argument <code>ab_angles_percentiles</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ab_angles_percentiles</code></td>
<td>
<p>The percentiles indicating the heights of the
object for which the angle should be computed (from the apex and the
bottom). Defaults to c(0.25, 0.75), which means considering the 25th and
75th percentiles of the object height.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>width_at</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the widths of the object at a given set
of quantiles of the height are computed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>width_at_percentiles</code></td>
<td>
<p>A vector of heights along the vertical axis of
the object at which the width will be computed. The default value is
c(0.05, 0.25, 0.5, 0.75, 0.95), which means the function will return the
width at the 5th, 25th, 50th, 75th, and 95th percentiles of the object's
height.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>haralick</code></td>
<td>
<p>Logical value indicating whether Haralick features are
computed. Defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>har_nbins</code></td>
<td>
<p>An integer indicating the number of bins using to compute the
Haralick matrix. Defaults to 32. See Details</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>har_scales</code></td>
<td>
<p>A integer vector indicating the number of scales to use to
compute the Haralick features. See Details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>har_band</code></td>
<td>
<p>The band to compute the Haralick features (1 = R, 2 = G, 3 =
B). Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pcv</code></td>
<td>
<p>Computes the Perimeter Complexity Value? Defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pcv_niter</code></td>
<td>
<p>An integer specifying the number of smoothing iterations for
computing the  Perimeter Complexity Value. Defaults to 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>resize</code></td>
<td>
<p>Resize the image before processing? Defaults to <code>FALSE</code>. Use a
numeric value of range 0-100 (proportion of the size of the original image).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trim</code></td>
<td>
<p>Number of pixels removed from edges in the analysis. The edges of
images are often shaded, which can affect image analysis. The edges of
images can be removed by specifying the number of pixels. Defaults to
<code>FALSE</code> (no trimmed edges).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fill_hull</code></td>
<td>
<p>Fill holes in the binary image? Defaults to <code>FALSE</code>. This is
useful to fill holes in objects that have portions with a color similar to
the background. IMPORTANT: Objects touching each other can be combined into
one single object, which may underestimate the number of objects in an
image.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>filter</code></td>
<td>
<p>Performs median filtering in the binary image? See more at
<code>image_filter()</code>. Defaults to <code>FALSE</code>. Use a positive integer to define the
size of the median filtering. Larger values are effective at removing noise,
but adversely affect edges.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>invert</code></td>
<td>
<p>Inverts the binary image if desired. This is useful to process
images with a black background. Defaults to <code>FALSE</code>. If <code>reference = TRUE</code>
is use, <code>invert</code> can be declared as a logical vector of length 2 (eg.,
<code style="white-space: pre;">⁠invert = c(FALSE, TRUE⁠</code>). In this case, the segmentation of objects and
reference from the foreground using <code>back_fore_index</code> is performed using the
default (not inverted), and the segmentation of objects from the reference
is performed by inverting the selection (selecting pixels higher than the
threshold).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object_size</code></td>
<td>
<p>The size of the object. Used to automatically set up
<code>tolerance</code> and <code>extension</code> parameters. One of the following. <code>"small"</code>
(e.g, wheat grains), <code>"medium"</code> (e.g, soybean grains), <code>"large"</code>(e.g, peanut
grains), and <code>"elarge"</code> (e.g, soybean pods)'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>index</code></td>
<td>
<p>A character value specifying the target mode for conversion to
binary image when <code>foreground</code> and <code>background</code> are not declared. Defaults
to <code>"NB"</code> (normalized blue). See <code>image_index()</code> for more details. User can
also calculate your own index using the bands names, e.g. <code>index = "R+B/G"</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r, g, b, re, nir</code></td>
<td>
<p>The red, green, blue, red-edge, and near-infrared bands
of the image, respectively. Defaults to 1, 2, 3, 4, and 5, respectively. If
a multispectral image is provided (5 bands), check the order of bands,
which are frequently presented in the 'BGR' format.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object_index</code></td>
<td>
<p>Defaults to <code>FALSE</code>. If an index is informed, the average
value for each object is returned. It can be the R, G, and B values or any
operation involving them, e.g., <code>object_index = "R/B"</code>. In this case, it
will return for each object in the image, the average value of the R/B
ratio. Use <code>pliman_indexes_eq()</code> to see the equations of available indexes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pixel_level_index</code></td>
<td>
<p>Return the indexes computed in <code>object_index</code> in the
pixel level? Defaults to <code>FALSE</code> to avoid returning large data.frames.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_mask</code></td>
<td>
<p>Returns the mask for the analyzed image? Defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>efourier</code></td>
<td>
<p>Logical argument indicating if Elliptical Fourier should be
computed for each object. This will call <code>efourier()</code> internally. It
<code>efourier = TRUE</code> is used, both standard and normalized Fourier coefficients
are returned.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nharm</code></td>
<td>
<p>An integer indicating the number of harmonics to use. Defaults to
10. For more details see <code>efourier()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>The theshold method to be used.
</p>

<ul>
<li>
<p> By default (<code>threshold = "Otsu"</code>), a threshold value based
on Otsu's method is used to reduce the grayscale image to a binary image. If
a numeric value is informed, this value will be used as a threshold.
</p>
</li>
<li>
<p> If <code>threshold = "adaptive"</code>, adaptive thresholding (Shafait et al. 2008)
is used, and will depend on the <code>k</code> and <code>windowsize</code> arguments.
</p>
</li>
<li>
<p> If any non-numeric value different than <code>"Otsu"</code> and <code>"adaptive"</code> is used,
an iterative section will allow you to choose the threshold based on a
raster plot showing pixel intensity of the index.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>a numeric in the range 0-1. when <code>k</code> is high, local threshold
values tend to be lower. when <code>k</code> is low, local threshold value tend to be
higher.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>windowsize</code></td>
<td>
<p>windowsize controls the number of local neighborhood in
adaptive thresholding. By default it is set to <code>1/3 * minxy</code>, where
<code>minxy</code> is the minimum dimension of the image (in pixels).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tolerance</code></td>
<td>
<p>The minimum height of the object in the units of image
intensity between its highest point (seed) and the point where it contacts
another object (checked for every contact pixel). If the height is smaller
than the tolerance, the object will be combined with one of its neighbors,
which is the highest.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>extension</code></td>
<td>
<p>Radius of the neighborhood in pixels for the detection of
neighboring objects. Higher value smooths out small objects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower_noise</code></td>
<td>
<p>To prevent noise from affecting the image analysis, objects
with lesser than 10% of the mean area of all objects are removed
(<code>lower_noise = 0.1</code>). Increasing this value will remove larger noises (such
as dust points), but can remove desired objects too. To define an explicit
lower or upper size, use the <code>lower_size</code> and <code>upper_size</code> arguments.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower_size, upper_size</code></td>
<td>
<p>Lower and upper limits for size for the image
analysis. Plant images often contain dirt and dust.  Upper limit is set to
<code>NULL</code>, i.e., no upper limit used. One can set a known area or use
<code>lower_limit = 0</code> to select all objects (not advised). Objects that matches
the size of a given range of sizes can be selected by setting up the two
arguments. For example, if <code>lower_size = 120</code> and <code>upper_size = 140</code>,
objects with size greater than or equal 120 and less than or equal 140 will
be considered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>topn_lower, topn_upper</code></td>
<td>
<p>Select the top <code>n</code> objects based on its area.
<code>topn_lower</code> selects the <code>n</code> elements with the smallest area whereas
<code>topn_upper</code> selects the <code>n</code> objects with the largest area.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower_eccent, upper_eccent, lower_circ, upper_circ</code></td>
<td>
<p>Lower and upper limit
for object eccentricity/circularity for the image analysis. Users may use
these arguments to remove objects such as square papers for scale (low
eccentricity) or cut petioles (high eccentricity) from the images. Defaults
to <code>NULL</code> (i.e., no lower and upper limits).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>randomize</code></td>
<td>
<p>Randomize the lines before training the model?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nrows</code></td>
<td>
<p>The number of lines to be used in training step. Defaults to
2000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>
<p>Show image after processing?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show_original</code></td>
<td>
<p>Show the count objects in the original image?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show_chull</code></td>
<td>
<p>Show the convex hull around the objects? Defaults to
<code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show_contour</code></td>
<td>
<p>Show a contour line around the objects? Defaults to
<code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>contour_col, contour_size</code></td>
<td>
<p>The color and size for the contour line around
objects. Defaults to <code>contour_col = "red"</code> and <code>contour_size = 1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show_lw</code></td>
<td>
<p>If <code>TRUE</code>, plots the length and width lines on each object
calling <code>plot_lw()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show_background</code></td>
<td>
<p>Show the background? Defaults to <code>TRUE</code>. A white
background is shown by default when <code>show_original = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show_segmentation</code></td>
<td>
<p>Shows the object segmentation colored with random
permutations. Defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>col_foreground, col_background</code></td>
<td>
<p>Foreground and background color after
image processing. Defaults to <code>NULL</code>, in which <code>"black"</code>, and <code>"white"</code> are
used, respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>marker, marker_col, marker_size</code></td>
<td>
<p>The type, color and size of the object
marker. Defaults to <code>NULL</code>, which plots the object id. Use <code>marker = "point"</code> to show a point in each object or <code>marker = FALSE</code> to omit object
marker.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save_image</code></td>
<td>
<p>Save the image after processing? The image is saved in the
current working directory named as <code style="white-space: pre;">⁠proc_*⁠</code> where <code>*</code> is the image name
given in <code>img</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prefix</code></td>
<td>
<p>The prefix to be included in the processed images. Defaults to
<code>"proc_"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dir_original, dir_processed</code></td>
<td>
<p>The directory containing the original and
processed images. Defaults to <code>NULL</code>. In this case, the function will search
for the image <code>img</code> in the current working directory. After processing, when
<code>save_image = TRUE</code>, the processed image will be also saved in such a
directory. It can be either a full path, e.g., <code>"C:/Desktop/imgs"</code>, or a
subfolder within the current working directory, e.g., <code>"/imgs"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If <code>TRUE</code> (default) a summary is shown in the console.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An object of class <code>anal_obj</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>which</code></td>
<td>
<p>Which to plot. Either 'measure' (object measures) or 'index'
(object index). Defaults to <code>"measure"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measure</code></td>
<td>
<p>The measure to plot. Defaults to <code>"area"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>The type of plot. Either <code>"hist"</code> or <code>"density"</code>. Partial matches
are recognized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Depends on the function:
</p>

<ul><li>
<p> For <code>analyze_objects_iter()</code>, further arguments passed on to
<code>analyze_objects()</code>.
</p>
</li></ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>known_area</code></td>
<td>
<p>The known area of the template object.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A binary image is first generated to segment the foreground and
background. The argument <code>index</code> is useful to choose a proper index to
segment the image (see <code>image_binary()</code> for more details). It is also
possible to provide color palettes for background and foreground (arguments
<code>background</code> and <code>foreground</code>, respectively). When this is used, a general
linear model (binomial family) fitted to the RGB values to segment fore- and
background.
</p>
<p>Then, the number of objects in the  foreground is counted. By setting up
arguments such as <code>lower_size</code> and <code>upper_size</code>, it is possible to set a
threshold for lower and upper sizes of the objects, respectively. The
argument <code>object_size</code> can be used to set up pre-defined values of
<code>tolerance</code> and <code>extension</code> depending on the image resolution. This will
influence the watershed-based object segmentation. Users can also tune up
<code>tolerance</code> and <code>extension</code> explicitly for a better precision of watershed
segmentation.
</p>
<p>If <code>watershed = FALSE</code> is used, all pixels for each connected set of
foreground pixels in <code>img</code> are set to a unique object. This is faster,
especially for a large number of objects, but it is not able to segment
touching objects.
</p>
<p>There are some ways to correct the measures based on a reference object. If
a reference object with a known area (<code>reference_area</code>) is used in the image
and <code>reference = TRUE</code> is used, the measures of the objects will be
corrected, considering the unit of measure informed in <code>reference_area</code>.
There are two main ways to work with reference objects.
</p>

<ul>
<li>
<p> The first, is to provide a reference object that has a contrasting color with
both the background and object of interest. In this case, the arguments
<code>back_fore_index</code> and <code>fore_ref_index</code> can be used to define an index to
first segment the reference object and objects to be measured from the
background, then the reference object from objects to be measured.
</p>
</li>
<li>
<p> The second one is to use a reference object that has a similar color to the
objects to be measured, but has a contrasting size. For example, if we are
counting small brown grains, we can use a brown reference template that has
an area larger (says 3 times the area of the grains) and then uses
<code>reference_larger = TRUE</code>. With this, the larger object in the image will be
used as the reference object. This is particularly useful when images are
captured with background light, such as the example  2. Some types: (i) It
is suggested that the reference object is not too much larger than the
objects of interest (mainly when the <code>watershed = TRUE</code>). In some cases, the
reference object can be broken into several pieces due to the watershed
algorithm. (ii) Since the reference object will increase the mean area of
the object, the argument <code>lower_noise</code> can be increased. By default
(<code>lower_noise = 0.1</code>) objects with lesser than 10% of the mean area of all
objects are removed. Since the mean area will be increased, increasing
<code>lower_noise</code> will remove dust and noises more reliably. The argument
<code>reference_smaller</code> can be used in the same way
</p>
</li>
</ul>
<p>By using <code>pattern</code>, it is possible to process several images with common
pattern names that are stored in the current working directory or in the
subdirectory informed in <code>dir_original</code>. To speed up the computation time,
one can set <code>parallel = TRUE</code>.
</p>
<p><code>analyze_objects_iter()</code> can be used to process several images using an
object with a known area as a template. In this case, all the images in the
current working directory that match the <code>pattern</code> will be processed. For
each image, the function will compute the features for the objects and show
the identification (id) of each object. The user only needs to inform which
is the id of the known object. Then, given the <code>known_area</code>, all the
measures will be adjusted. In the end, a data.frame with the adjusted
measures will be returned. This is useful when the images are taken at
different heights. In such cases, the image resolution cannot be conserved.
Consequently, the measures cannot be adjusted using the argument <code>dpi</code> from
<code>get_measures()</code>, since each image will have a different resolution. NOTE:
This will only work in an interactive section.
</p>

<ul><li>
<p> Additional measures: By default, some measures are not computed, mainly due to
computational efficiency when the user only needs simple measures such as
area, length, and width.
</p>

<ul>
<li>
<p> If <code>haralick = TRUE</code>, The function computes 13 Haralick texture features for
each object based on a gray-level co-occurrence matrix (Haralick et al.
1979). Haralick features depend on the configuration of the parameters
<code>har_nbins</code> and <code>har_scales</code>. <code>har_nbins</code> controls the number of bins used
to compute the Haralick matrix. A smaller <code>har_nbins</code> can give more accurate
estimates of the correlation because the number of events per bin is higher.
While a higher value will give more sensitivity. <code>har_scales</code> controls the
number of scales used to compute the Haralick features. Since Haralick
features compute the correlation of intensities of neighboring pixels it is
possible to identify textures with different scales, e.g., a texture that is
repeated every two pixels or 10 pixels. By default, the Haralick features
are computed with the R band. To chance this default, use the argument
<code>har_band</code>. For example, <code>har_band = 2</code> will compute the features with the
green band.
</p>
</li>
<li>
<p> If <code>efourier = TRUE</code> is used, an Elliptical Fourier Analysis (Kuhl and
Giardina, 1982) is computed for each object contour using <code>efourier()</code>.
</p>
</li>
<li>
<p> If <code>veins = TRUE</code> (experimental), vein features are computed. This will call
<code>object_edge()</code> and applies the Sobel-Feldman Operator to detect edges. The
result is the proportion of edges in relation to the entire area of the
object(s) in the image. Note that THIS WILL BE AN OPERATION ON AN IMAGE
LEVEL, NOT an OBJECT LEVEL! So, If vein features need to be computed for
leaves, it is strongly suggested to use one leaf per image.
</p>
</li>
<li>
<p> If <code>ab_angles = TRUE</code> the apex and base angles of each object are
computed with <code>poly_apex_base_angle()</code>. By default, the function computes
the angle from the first pixel of the apex of the object to the two pixels
that slice the object at the 25th percentile of the object height (apex
angle). The base angle is computed in the same way but from the first base
pixel.
</p>
</li>
<li>
<p> If <code>width_at = TRUE</code>, the width at the  5th, 25th, 50th, 75th, and 95th
percentiles of the object height are computed by default. These quantiles can
be adjusted with the <code>width_at_percentiles</code> argument.
</p>
</li>
</ul>
</li></ul>
<h3>Value</h3>

<p><code>analyze_objects()</code> returns a list with the following objects:
</p>

<ul>
<li> <p><code>results</code> A data frame with the following variables for each object in the
image:
</p>

<ul>
<li> <p><code>id</code>:  object identification.
</p>
</li>
<li> <p><code>x</code>,<code>y</code>:  x and y coordinates for the center of mass of the object.
</p>
</li>
<li> <p><code>area</code>:  area of the object (in pixels).
</p>
</li>
<li> <p><code>area_ch</code>:  the area of the convex hull around object (in pixels).
</p>
</li>
<li> <p><code>perimeter</code>: perimeter (in pixels).
</p>
</li>
<li> <p><code>radius_min</code>, <code>radius_mean</code>, and <code>radius_max</code>: The minimum, mean, and
maximum radius (in pixels), respectively.
</p>
</li>
<li> <p><code>radius_sd</code>: standard deviation of the mean radius (in pixels).
</p>
</li>
<li> <p><code>diam_min</code>, <code>diam_mean</code>, and <code>diam_max</code>: The minimum, mean, and
maximum diameter (in pixels), respectively.
</p>
</li>
<li> <p><code>major_axis</code>, <code>minor_axis</code>: elliptical fit for major and minor axes (in
pixels).
</p>
</li>
<li> <p><code>caliper</code>: The longest distance between any two points on the margin
of the object. See <code>poly_caliper()</code> for more details
</p>
</li>
<li> <p><code>length</code>, <code>width</code> The length and width of objects (in pixels). These
measures are obtained as the range of x and y coordinates after aligning
each object with <code>poly_align()</code>.
</p>
</li>
<li> <p><code>radius_ratio</code>: radius ratio given by <code>radius_max / radius_min</code>.
</p>
</li>
<li> <p><code>theta</code>: object angle (in radians).
</p>
</li>
<li> <p><code>eccentricity</code>: elliptical eccentricity computed using the
ratio of the eigen values (inertia axes of coordinates).
</p>
</li>
<li> <p><code>form_factor</code> (Wu et al., 2007):  the difference between a leaf and a
circle. It is defined as <code>4*pi*A/P</code>, where A is the area and P is the
perimeter of the object.
</p>
</li>
<li> <p><code>narrow_factor</code> (Wu et al., 2007): Narrow factor (<code>caliper / length</code>).
</p>
</li>
<li> <p><code>asp_ratio</code> (Wu et al., 2007): Aspect ratio (<code>length / width</code>).
</p>
</li>
<li> <p><code>rectangularity</code> (Wu et al., 2007): The similarity between a leaf and
a rectangle (<code>length * width/ area</code>).
</p>
</li>
<li> <p><code>pd_ratio</code> (Wu et al., 2007): Ratio of perimeter to diameter
(<code>perimeter / caliper</code>)
</p>
</li>
<li> <p><code>plw_ratio</code> (Wu et al., 2007): Perimeter ratio of length and width
(<code>perimeter / (length + width)</code>)
</p>
</li>
<li> <p><code>solidity</code>: object solidity given by <code>area / area_ch</code>.
</p>
</li>
<li> <p><code>convexity</code>: The convexity of the object computed using the ratio
between the perimeter of the convex hull and the perimeter of the polygon.
</p>
</li>
<li> <p><code>elongation</code>: The elongation of the object computed as <code>1 - width / length</code>.
</p>
</li>
<li> <p><code>circularity</code>: The object circularity given by <code>perimeter ^ 2 / area</code>.
</p>
</li>
<li> <p><code>circularity_haralick</code>: The Haralick's circularity (CH), computed as
<code>CH =  m/sd</code>, where <code>m</code> and <code>sd</code> are the mean and standard deviations
from each pixels of the perimeter to the centroid of the object.
</p>
</li>
<li> <p><code>circularity_norm</code>: The normalized circularity (Cn), to be unity for a
circle. This measure is computed as <code>Cn = perimeter ^ 2 / 4*pi*area</code> and
is invariant under translation, rotation, scaling transformations, and
dimensionless.
</p>
</li>
<li> <p><code>asm</code>: The angular second-moment feature.
</p>
</li>
<li> <p><code>con</code>: The contrast feature
</p>
</li>
<li> <p><code>cor</code>: Correlation measures the linear dependency of gray levels of
neighboring pixels.
</p>
</li>
<li> <p><code>var</code>: The variance of gray levels pixels.
</p>
</li>
<li> <p><code>idm</code>: The Inverse Difference Moment (IDM), i.e., the local
homogeneity.
</p>
</li>
<li> <p><code>sav</code>: The Sum Average.
</p>
</li>
<li> <p><code>sva</code>: The Sum Variance.
</p>
</li>
<li> <p><code>sen</code>: Sum Entropy.
</p>
</li>
<li> <p><code>dva</code>: Difference Variance.
</p>
</li>
<li> <p><code>den</code>: Difference Entropy
</p>
</li>
<li> <p><code>f12</code>: Difference Variance.
</p>
</li>
<li> <p><code>f13</code>: The angular second-moment feature.
</p>
</li>
</ul>
</li>
<li> <p><code>statistics</code>: A data frame with the summary statistics for the area of the
objects.
</p>
</li>
<li> <p><code>count</code>: If <code>pattern</code> is used, shows the number of objects in each image.
</p>
</li>
<li> <p><code>obj_rgb</code>: If <code>object_index</code> is used, returns the R, G, and B values
for each pixel of each object.
</p>
</li>
<li> <p><code>object_index</code>: If <code>object_index</code> is used, returns the index computed for
each object.
</p>
</li>
<li>
<p> Elliptical Fourier Analysis: If <code>efourier = TRUE</code> is used, the following
objects are returned.
</p>

<ul>
<li> <p><code>efourier</code>: The Fourier coefficients.  For more details see
<code>efourier()</code>.
</p>
</li>
<li> <p><code>efourier_norm</code>: The normalized Fourier coefficients. For more details
see <code>efourier_norm()</code>.
</p>
</li>
<li> <p><code>efourier_error</code>: The error between original data and  reconstructed
outline. For more details see <code>efourier_error()</code>.
</p>
</li>
<li> <p><code>efourier_power</code>: The spectrum of harmonic Fourier power.
For more details see <code>efourier_power()</code>.
</p>
</li>
</ul>
</li>
<li> <p><code>veins</code>: If <code>veins = TRUE</code> is used, returns, for each image, the
proportion of veins (in fact the object edges) related to the total object(s)' area.
</p>
</li>
<li> <p><code>analyze_objects_iter()</code> returns a data.frame containing the features
described in the <code>results</code> object of <code>analyze_objects()</code>.
</p>
</li>
<li> <p><code>plot.anal_obj()</code> returns a <code>trellis</code> object containing the distribution
of the pixels, optionally  for each object when <code>facet = TRUE</code> is used.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Tiago Olivoto <a href="mailto:tiagoolivoto@gmail.com">tiagoolivoto@gmail.com</a>
</p>


<h3>References</h3>

<p>Claude, J. (2008) <em>Morphometrics with R</em>, Use R! series,
Springer 316 pp.
</p>
<p>Gupta, S., Rosenthal, D. M., Stinchcombe, J. R., &amp; Baucom, R. S. (2020). The
remarkable morphological diversity of leaf shape in sweet potato (Ipomoea
batatas): the influence of genetics, environment, and G×E. New Phytologist,
225(5), 2183–2195. <a href="https://doi.org/10.1111/NPH.16286">doi:10.1111/NPH.16286</a>
</p>
<p>Haralick, R.M., K. Shanmugam, and I. Dinstein. 1973. Textural Features for Image
Classification. IEEE Transactions on Systems, Man, and Cybernetics SMC-3(6): 610–621.
<a href="https://doi.org/10.1109/TSMC.1973.4309314">doi:10.1109/TSMC.1973.4309314</a>
</p>
<p>Kuhl, F. P., and Giardina, C. R. (1982). Elliptic Fourier features of a
closed contour. Computer Graphics and Image Processing 18, 236–258. doi:
<a href="https://doi.org/10.1016/0146-664X%2882%2990034-X">doi:10.1016/0146-664X(82)90034-X</a>
</p>
<p>Lee, Y., &amp; Lim, W. (2017). Shoelace Formula: Connecting the Area of a Polygon
and the Vector Cross Product. The Mathematics Teacher, 110(8), 631–636.
<a href="https://doi.org/10.5951/mathteacher.110.8.0631">doi:10.5951/mathteacher.110.8.0631</a>
</p>
<p>Montero, R. S., Bribiesca, E., Santiago, R., &amp; Bribiesca, E. (2009). State
of the Art of Compactness and Circularity Measures. International
Mathematical Forum, 4(27), 1305–1335.
</p>
<p>Chen, C.H., and P.S.P. Wang. 2005. Handbook of Pattern Recognition and
Computer Vision. 3rd ed. World Scientific.
</p>
<p>Wu, S. G., Bao, F. S., Xu, E. Y., Wang, Y.-X., Chang, Y.-F., and Xiang, Q.-L.
(2007). A Leaf Recognition Algorithm for Plant Classification Using
Probabilistic Neural Network. in 2007 IEEE International Symposium on Signal
Processing and Information Technology, 11–16.
<a href="https://doi.org/10.1109/ISSPIT.2007.4458016">doi:10.1109/ISSPIT.2007.4458016</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(pliman)
img &lt;- image_pliman("soybean_touch.jpg")
obj &lt;- analyze_objects(img)
obj$statistics

########################### Example 1 #########################
# Enumerate the objects in the original image
# Return the top-5 grains with the largest area
top &lt;-
 analyze_objects(img,
                 marker = "id",
                 topn_upper = 5)
top$results


#' ########################### Example 1 #########################
# Correct the measures based on the area of the largest object
# note that since the reference object

img &lt;- image_pliman("flax_grains.jpg")
res &lt;-
  analyze_objects(img,
                  index = "GRAY",
                  marker = "point",
                  show_contour = FALSE,
                  reference = TRUE,
                  reference_area = 6,
                  reference_larger = TRUE,
                  lower_noise = 0.3)



library(pliman)

img &lt;- image_pliman("soy_green.jpg")
# Segment the foreground (grains) using the normalized blue index (NB, default)
# Shows the average value of the blue index in each object

rgb &lt;-
   analyze_objects(img,
                   marker = "id",
                   object_index = "B",
                   pixel_level_index = TRUE)
# density of area
plot(rgb)

# histogram of perimeter
plot(rgb, measure = "perimeter", type = "histogram") # or 'hist'

# density of the blue (B) index
plot(rgb, which = "index")

</code></pre>


</div>