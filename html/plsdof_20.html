<div class="container">

<table style="width: 100%;"><tr>
<td>pls.model</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Partial Least Squares</h2>

<h3>Description</h3>

<p>This function computes the Partial Least Squares fit.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pls.model(
  X,
  y,
  m = ncol(X),
  Xtest = NULL,
  ytest = NULL,
  compute.DoF = FALSE,
  compute.jacobian = FALSE,
  use.kernel = FALSE,
  method.cor = "pearson"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m=min(ncol(X),nrow(X)-1)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Xtest</code></td>
<td>
<p>optional matrix of test observations. Default is
<code>Xtest=NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ytest</code></td>
<td>
<p>optional vector of test observations. Default is
<code>ytest=NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compute.DoF</code></td>
<td>
<p>Logical variable. If <code>compute.DoF=TRUE</code>, the Degrees
of Freedom of Partial Least Squares are computed. Default is
<code>compute.DoF=FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compute.jacobian</code></td>
<td>
<p>Should the first derivative of the regression
coefficients be computed as well? Default is <code>FALSE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use.kernel</code></td>
<td>
<p>Should the kernel representation be used to compute the
solution. Default is <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method.cor</code></td>
<td>
<p>How should the correlation to the response be computed?
Default is ”pearson”.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function computes the Partial Least Squares fit and its Degrees of
Freedom. Further, it returns the regression coefficients and various
quantities that are needed for model selection in combination with
<code>information.criteria</code>.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>coefficients</code></td>
<td>
<p>matrix of regression coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>vector of intercepts</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>DoF</code></td>
<td>
<p>vector of Degrees of
Freedom</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RSS</code></td>
<td>
<p>vector of residual sum of error</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigmahat</code></td>
<td>
<p>vector
of estimated model error</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Yhat</code></td>
<td>
<p>matrix of fitted values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhat</code></td>
<td>
<p>vector of squared length of fitted values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>covariance</code></td>
<td>
<p>if
<code>compute.jacobian</code> is <code>TRUE</code>, the function returns the array of
covariance matrices for the PLS regression coefficients.</p>
</td>
</tr>
</table>
<p><code>prediction</code>if <code>Xtest</code> is provided, the predicted y-values for
<code>Xtest</code>. <code>mse</code>if <code>Xtest</code> and <code>ytest</code> are provided, the
mean squared error on the test data.  <code>cor</code>if <code>Xtest</code> and
<code>ytest</code> are provided, the correlation to the response on the test data.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). "The Degrees of Freedom of
Partial Least Squares Regression". Journal of the American Statistical
Association 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Sugiyama, M., Braun, M.L. (2009) "Lanczos Approximations for
the Speedup of Partial Least Squares Regression", Proceedings of the 12th
International Conference on Artificial Intelligence and Stastistics, 272 -
279
</p>


<h3>See Also</h3>

<p><code>pls.ic</code>, <code>pls.cv</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
n&lt;-50 # number of observations
p&lt;-15 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

ntest&lt;-200 #
Xtest&lt;-matrix(rnorm(ntest*p),ncol=p) # test data
ytest&lt;-rnorm(ntest) # test data

# compute PLS + degrees of freedom + prediction on Xtest
first.object&lt;-pls.model(X,y,compute.DoF=TRUE,Xtest=Xtest,ytest=NULL)

# compute PLS + test error
second.object=pls.model(X,y,m=10,Xtest=Xtest,ytest=ytest)

</code></pre>


</div>