<div class="container">

<table style="width: 100%;"><tr>
<td>optPenaltyGLMmultiT.kCVauto</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Automatic search for optimal penalty parameters of the targeted ridge GLM estimator.
</h2>

<h3>Description</h3>

<p>Function finds the optimal penalty parameter of the targeted ridge 
regression estimator of the generalized linear model parameter. The 
optimum is defined as the minimizer of the cross-validated loss 
associated with the estimator. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">optPenaltyGLMmultiT.kCVauto(Y, X, lambdaInit, model="linear", targetMat,
                      folds=makeFoldsGLMcv(min(10, length(X)), Y, model=model),
                      loss="loglik", lambdaMin=10^(-5),
                      minSuccDiff=10^(-5), maxIter=100)     
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>           A <code>numeric</code> being the response vector. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>           The design <code>matrix</code>. The number of rows should match the number of elements of <code>Y</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdaInit</code></td>
<td>
<p>  A <code>numeric</code> giving the starting values for search of the optimal penalty parameter. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>       A <code>character</code>, either <code>"linear"</code> and <code>"logistic"</code> (a reference to the models currently implemented), indicating which generalized linear model model instance is to be fitted. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>targetMat</code></td>
<td>
<p>   A <code>matrix</code> with targets for the regression parameter as columns. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>folds</code></td>
<td>
<p>       A <code>list</code>. Each list item representing a fold. It is an <code>integer</code> vector indexing the samples that comprise the fold. This object can be generated with the <code>makeFoldsGLMcv</code> function. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>        A <code>character</code>, either <code>loss="loglik"</code> or <code>"sos"</code>, specifying loss criterion to be used in the cross-validation. Used only if <code>model="linear"</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdaMin </code></td>
<td>
<p> A positive <code>numeric</code>, the lower bound of search interval of the regular ridge penalty parameter. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>, the minimum distance between the loglikelihoods of two successive iterations to be achieved. Used only if <code>model="logistic"</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxIter</code></td>
<td>
<p>     A <code>numeric</code> specifying the maximum number of iterations. Used only if <code>model="logistic"</code>. </p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The function returns an all-positive <code>numeric</code>, the cross-validated optimal penalty parameters. The average loglikelihood over the left-out samples is used as the cross-validation criterion. If <code>model="linear"</code>, also the average sum-of-squares over the left-out samples is offered as cross-validation criterion.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. Binder, H. (2022), "Sequential learning of regression models by penalized estimation", <em>accepted</em>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># set the sample size
n &lt;- 50

# set the true parameter
betas &lt;- (c(0:100) - 50) / 20

# generate covariate data
X &lt;- matrix(rnorm(length(betas)*n), nrow=n)

# sample the response
probs &lt;- exp(tcrossprod(betas, X)[1,]) / (1 + exp(tcrossprod(betas, X)[1,]))
Y     &lt;- numeric()
for (i in 1:n){
    Y &lt;- c(Y, sample(c(0,1), 1, prob=c(1-probs[i], probs[i])))
}

# create targets
targets &lt;- cbind(betas/2, rep(0, length(betas)))

# tune the penalty parameter
### optLambdas &lt;- optPenaltyGLMmultiT.kCVauto(Y, X, c(50,0.1), fold=5,            
###                                           targetMat=targets, model="logistic", 
###                                           minSuccDiff=10^(-3))                  

# estimate the logistic regression parameter
### bHat &lt;- ridgeGLMmultiT(Y, X, lambdas=optLambdas, targetMat=targets, model="logistic") 
</code></pre>


</div>