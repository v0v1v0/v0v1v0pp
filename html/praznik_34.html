<div class="container">

<table style="width: 100%;"><tr>
<td>praznik-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Tools for information-based feature selection and scoring</h2>

<h3>Description</h3>

<p>Praznik is a collection of tools for information theory-based feature selection and scoring.
</p>


<h3>Details</h3>

<p>The first part of the package consists of efficient implementations of several popular information filters, feature selection approaches based on greedy optimisation of certain feature inclusion criterion.
In a nutshell, an algorithm of this class requires an information system <code class="reqn">(X,Y)</code> and a predefined number of features to selected <code class="reqn">k</code>, and works like this.
To start, it estimates mutual information between each feature and the decision, find a feature with maximal such score and stores it as a first on a list of selected features, <code class="reqn">S</code>.
Then, it estimates a value of a certain criterion function <code class="reqn">J(X,Y,S)</code> for each feature <code class="reqn">X</code>; this function also depends on <code class="reqn">Y</code> and the set of already selected features <code class="reqn">S</code>.
As in the first step, the previously unselected feature with a greatest value of the criterion function is selected next.
This is repeated until the method would gather <code class="reqn">k</code> features, or, in case of some methods, when no more informative features can be found.
The methods implemented in praznik consider the following criteria.
</p>
<p>The mutual information maximisation filter, <code>MIM</code>, simply selects top-<code class="reqn">k</code> features of best mutual information, that is
</p>
<p style="text-align: center;"><code class="reqn">J_{MIM}=I(X;Y).</code>
</p>

<p>The minimal conditional mutual information maximisation proposed by F. Fleauret, <code>CMIM</code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{CMIM}(X)=\min(I(X;Y),\min_{W\in S} I(X;Y|W));</code>
</p>

<p>this method is also effectively identical to the information fragments method.
</p>
<p>The minimum redundancy maximal relevancy proposed by H. Peng et al., <code>MRMR</code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{MRMR}=I(X;Y)-\frac{1}{|S|}\sum_{W\in S} I(X;W).</code>
</p>

<p>The joint mutual information filter by H. Yang and J. Moody, <code>JMI</code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{JMI}=\sum_{W\in S} I(X,W;Y).</code>
</p>

<p>The double input symmetrical relevance filter by P. Meyer and G. Bontempi, <code>DISR</code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{DISR}(X)=\sum_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)}.</code>
</p>

<p>The minimal joint mutual information maximisation filter by M. Bennasar, Y. Hicks and R. Setchi, <code>JMIM</code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{JMIM}=\min_{W\in S} I(X,W;Y).</code>
</p>

<p>The minimal normalised joint mutual information maximisation filter by the same authors, <code>NJMIM</code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{NJMIM}=\min_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)}.</code>
</p>

<p>The third-order joint mutual information filter by Sechidis et al., <code>JMI3</code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\frac{1}{2}\sum_{(U,W)\in S^2; U\neq W} I(X,U,W;Y).</code>
</p>

<p>While <code>CMIM</code>, <code>JMIM</code> and <code>NJMIM</code> consider minimal value over already selected features, they may use a somewhat more sophisticated and faster algorithm.
</p>
<p>The second part of the package provides methods for scoring features, useful on its own as well as building blocks for more sophisticated algorithms.
In particular, the package exposes the following functions:
</p>
<p><code>hScores</code> returns 
</p>
<p style="text-align: center;"><code class="reqn">H(X).</code>
</p>

<p><code>jhScores</code> returns 
</p>
<p style="text-align: center;"><code class="reqn">H(X,Y).</code>
</p>

<p><code>miScores</code> returns 
</p>
<p style="text-align: center;"><code class="reqn">I(X;Y).</code>
</p>

<p><code>cmiScores</code> returns, for a given condition vector <code class="reqn">Z</code>,
</p>
<p style="text-align: center;"><code class="reqn">I(X;Y|Z).</code>
</p>

<p><code>jmiScores</code> returns
</p>
<p style="text-align: center;"><code class="reqn">I(X,Z;Y).</code>
</p>

<p><code>njmiScores</code> returns
</p>
<p style="text-align: center;"><code class="reqn">\frac{I(X,Z;Y)}{H(X,Y,Z)}.</code>
</p>

<p><code>minCmiScores</code>, <code>maxCmiScores</code> and <code>minMaxCmiScores</code> return
</p>
<p style="text-align: center;"><code class="reqn">\min_j I(X_i;Y|X_j)</code>
</p>
<p> and/or </p>
<p style="text-align: center;"><code class="reqn">\max_j I(X_i;Y|X_j).</code>
</p>

<p><code>maxJmiScores</code> returns
</p>
<p style="text-align: center;"><code class="reqn">\max_{j\neq i} I(X_i,X_j;Y).</code>
</p>

<p><code>triScores</code> returns, for every triple of features,
</p>
<p style="text-align: center;"><code class="reqn">I(X_i;X_j;X_k).</code>
</p>

<p>These function generally also have their <code>*Matrix</code> counterparts, which efficiently build a matrix of scores between all pairs of features.
This is especially useful for network inference tasks.
</p>
<p>Estimation of mutual information and its generalisations is a hard task; still, praznik aims at speed and simplicity and hence only offers basic, maximum likelihood estimator applicable on discrete data.
For convenience, praznik automatically and silently coerces non-factor inputs into about ten equally-spaced bins, following the heuristic often used in literature.
</p>
<p>Furthermore, praznik provides <code>kTransform</code> function for converting continuous features into discrete ones with Kendall transformation, a novel approach based on Kendall correlation coefficient which allows for multivariate reasoning based on monotonicity agreement.
</p>
<p>Additionally, praznik has a limited, experimental support for replacing entropic statistics with Gini impurity-based; in such framework, entropy is replaced by Gini impurity
</p>
<p style="text-align: center;"><code class="reqn">g(X):=1-\sum_x p_x^2,</code>
</p>

<p>which leads to an impurity gain
</p>
<p style="text-align: center;"><code class="reqn">G(X;Y):=g(Y)-E(g(Y)|X)=\sum_{xy}\frac{p_{xy}^2}{p_x}-\sum_{y} p_y^2,</code>
</p>

<p>a counterpart of mutual information or information gain.
It does not possess most of elegant properties of mutual information, yet values of both are usually highly correlated; moreover, Gini gain is computationally easier to calculate, hence it often replaces MI in performance-sensitive applications, like optimising splits in decision trees.
</p>
<p>In a present version, praznik includes <code>impScores</code> for generating values of <code class="reqn">G</code> for all features (an analog of <code>miScores</code>, as well as <code>JIM</code>, a Gini gain-based feature selection method otherwise identical to <code>JMI</code>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Miron B. Kursa <a href="mailto:m@mbq.me">m@mbq.me</a> (<a href="https://orcid.org/0000-0001-7672-648X">ORCID</a>)
</p>


<h3>References</h3>

<p>"Praznik: High performance information-based feature selection" M.B. Kursa. SoftwareX 16, 100819 (2021).
</p>
<p>"Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection" G. Brown et al. JMLR (2012).
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://gitlab.com/mbq/praznik">https://gitlab.com/mbq/praznik</a>
</p>
</li>
<li>
<p> Report bugs at <a href="https://gitlab.com/mbq/praznik/-/issues">https://gitlab.com/mbq/praznik/-/issues</a>
</p>
</li>
</ul>
</div>