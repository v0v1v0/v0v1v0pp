<div class="container">

<table style="width: 100%;"><tr>
<td>reward</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate the Reward for a POMDP Solution</h2>

<h3>Description</h3>

<p>This function calculates the expected total reward for a POMDP solution
given a starting belief state. The value is calculated using the value function stored
in the POMDP solution. In addition, the policy graph node that represents the belief state
and the optimal action can also be returned using <code>reward_node_action()</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">reward(x, belief = NULL, epoch = 1, ...)

reward_node_action(x, belief = NULL, epoch = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a solved POMDP object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>belief</code></td>
<td>
<p>specification of the current belief state (see argument start
in POMDP for details). By default the belief state defined in
the model as start is used. Multiple belief states can be specified as rows in a matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epoch</code></td>
<td>
<p>return reward for this epoch. Use 1 for converged policies.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments are passed on.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The reward is typically calculated using the value function (alpha vectors)
of the solution. If these are not available, then <code>simulate_POMDP()</code> is
used instead with a warning.
</p>


<h3>Value</h3>

<p><code>reward()</code> returns a vector of reward values, one for each belief if a matrix is specified.
</p>
<p><code>reward_node_action()</code> returns a list with the components
</p>
<table>
<tr style="vertical-align: top;">
<td><code>belief_state</code></td>
<td>
<p>the belief state specified in <code>belief</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reward</code></td>
<td>
<p>the total expected reward given a belief and epoch. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pg_node</code></td>
<td>
<p>the policy node that represents the belief state.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>action</code></td>
<td>
<p>the optimal action.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code>estimate_belief_for_nodes()</code>,
<code>optimal_action()</code>,
<code>plot_belief_space()</code>,
<code>plot_policy_graph()</code>,
<code>policy()</code>,
<code>policy_graph()</code>,
<code>projection()</code>,
<code>solve_POMDP()</code>,
<code>solve_SARSOP()</code>,
<code>value_function()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("Tiger")
sol &lt;- solve_POMDP(model = Tiger)

# if no start is specified, a uniform belief is used.
reward(sol)

# we have additional information that makes us believe that the tiger
# is more likely to the left.
reward(sol, belief = c(0.85, 0.15))

# we start with strong evidence that the tiger is to the left.
reward(sol, belief = "tiger-left")

# Note that in this case, the total discounted expected reward is greater
# than 10 since the tiger problem resets and another game staring with
# a uniform belief is played which produces additional reward.

# return reward, the initial node in the policy graph and the optimal action for
# two beliefs.
reward_node_action(sol, belief = rbind(c(.5, .5), c(.9, .1)))

# manually combining reward with belief space sampling to show the value function
# (color signifies the optimal action)
samp &lt;- sample_belief_space(sol, n = 200)
rew &lt;- reward_node_action(sol, belief = samp)
plot(rew$belief[,"tiger-right"], rew$reward, col = rew$action, ylim = c(0, 15))
legend(x = "top", legend = levels(rew$action), title = "action", col = 1:3, pch = 1)

# this is the piecewise linear value function from the solution
plot_value_function(sol, ylim = c(0, 10))
</code></pre>


</div>