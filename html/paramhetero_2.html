<div class="container">

<table style="width: 100%;"><tr>
<td>compare_coefs</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Compare shared coefficients across models</h2>

<h3>Description</h3>

<p>Compares predictor coefficients across models.
</p>


<h3>Usage</h3>

<pre><code class="language-R">compare_coefs(model_list, padj = "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model_list</code></td>
<td>
<p>A list of regression models.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>padj</code></td>
<td>
<p>A method from <code>p.adjust.methods</code> for adjusting coefficient
p-values for multiple testing.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function currently supports comparing coefficients from two models. For
each model predictor, coefficients are compared across models. P-values come
from a two-sided alternative hypothesis. They can, and should, be adjusted for
multiple testing to reduce the probability of chance significant findings.
</p>


<h3>Value</h3>

<p>Data frame of shared coefficients, the difference between them, the
standard error of the difference, the test statistic comparing them, and the
p-value adjusted using the method provided in <code>padj</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"> ##Simulate data

 N = 500

 m = rep(1:2, each=N)

 x1 = rnorm(n=N*2)
 x2 = rnorm(n=N*2)
 x3 = rnorm(n=N*2)

 y = x1 + x2 + x3 + rnorm(n=N*2)

 dat = data.frame(m, x1, x2, x3, y)

 m1 = lm(y ~ x1 + x2 + x3, data=dat, subset=m==1)
 m2 = lm(y ~ x1 + x2 + x3, data=dat, subset=m==2)

 mList = list(m1, m2)

 compare_coefs(model_list = mList, padj='fdr')

</code></pre>


</div>