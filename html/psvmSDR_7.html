<div class="container">

<table style="width: 100%;"><tr>
<td>rtpsdr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Real time sufficient dimension reduction through principal least squares SVM</h2>

<h3>Description</h3>

<p>In stream data, where we need to constantly update the estimation as new data are collected,
the use of all available data can create computational challenges even for computationally efficient algorithms.
Therefore it is important to develop real time SDR algorithms that work efficiently in the case that there are data streams.
After getting an initial estimator with the currently available data,
the basic idea of real-time method is to update the estimator efficiently as new data are collected.
This function realizes real time least squares SVM SDR method for a both regression and classification problem
It is efficient algorithms for either adding new data or removing old data are provided.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rtpsdr(x, y, obj = NULL, h = 10, lambda = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>x in new data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>y in new data, y is continuous</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj</code></td>
<td>
<p>the latest output object from the <code>rtpsdr</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>h</code></td>
<td>
<p>a number of slices. default is set to 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>hyperparameter for the loss function. default is set to 1.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object with S3 class "rtpsdr". Details are listed below.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>input data matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>iniput response vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Mn</code></td>
<td>
<p>The estimated working matrix, which is obtained by the cumulative
outer product of the estimated parameters over H</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>evalues</code></td>
<td>
<p>Eigenvalues of the Mn</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>evectors</code></td>
<td>
<p>Eigenvectors of the Mn, the first d leading eigenvectors consists
the basis of the central subspace</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N</code></td>
<td>
<p>total number of observation <code class="reqn">n_1 + n_2</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Xbar</code></td>
<td>
<p>mean of total <code class="reqn">\mathbf{x}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r</code></td>
<td>
<p>updated estimated coefficients matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>A</code></td>
<td>
<p>new A part for update. See Artemiou et. al., (2021)</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>References</h3>

<p>Artemiou, A. and Dong, Y. (2016)
<em>Sufficient dimension reduction via principal lq support vector machine,
Electronic Journal of Statistics 10: 783–805</em>.<br>
Artemiou, A., Dong, Y. and Shin, S. J. (2021)
<em>Real-time sufficient dimension reduction through principal least
squares support vector machines, Pattern Recognition 112: 107768</em>.<br>
Kim, B. and Shin, S. J. (2019)
<em>Principal weighted logistic regression for sufficient dimension
reduction in binary classification, Journal of the Korean Statistical Society 48(2): 194–206</em>.<br>
Li, B., Artemiou, A. and Li, L. (2011)
<em>Principal support vector machines for linear and
nonlinear sufficient dimension reduction, Annals of Statistics 39(6): 3182–3210</em>.<br>
Soale, A.-N. and Dong, Y. (2022)
<em>On sufficient dimension reduction via principal asymmetric
least squares, Journal of Nonparametric Statistics 34(1): 77–94</em>.<br>
Wang, C., Shin, S. J. and Wu, Y. (2018)
<em>Principal quantile regression for sufficient dimension
reduction with heteroscedasticity, Electronic Journal of Statistics 12(2): 2114–2140</em>.<br>
Shin, S. J., Wu, Y., Zhang, H. H. and Liu, Y. (2017)
<em>Principal weighted support vector machines for sufficient dimension reduction in
binary classification, Biometrika 104(1): 67–81</em>. <br>
Li, L. (2007)
<em>Sparse sufficient dimension reduction, Biometrika 94(3): 603–613</em>.
</p>


<h3>See Also</h3>

<p><code>psdr</code>, <code>npsdr</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
p &lt;- 5
m &lt;- 500 # batch size
N &lt;- 10  # number of batches
obj &lt;- NULL
for (iter in 1:N){
 set.seed(iter)
 x &lt;- matrix(rnorm(m*p), m, p)
 y &lt;-  x[,1]/(0.5 + (x[,2] + 1)^2) + 0.2 * rnorm(m)
 obj &lt;- rtpsdr(x = x, y = y, obj=obj)
}
print(obj)

</code></pre>


</div>