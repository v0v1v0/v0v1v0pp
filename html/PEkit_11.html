<div class="container">

<table style="width: 100%;"><tr>
<td>tSimLab</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Simultaneously predicted labels of the test data given the training data classification.</h2>

<h3>Description</h3>

<p>Classifies the test data <code>x</code> based on the training data object.
All of the test data is used simultaneously to make the classification.
</p>


<h3>Usage</h3>

<pre><code class="language-R">tSimLab(training, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>training</code></td>
<td>
<p>A training data object from the function <code>classifier.fit()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Test data vector or matrix with rows as data points and columns as features.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The test data are first labeled with the marginal classifier. The simultaneous
classifier then iterates over all test data, assigning each a label by finding
the maximum predictive probability given the current classification structure of
the test data as a whole. This is repeated until the classification structure
converges after iterating over all data.
</p>


<h3>Value</h3>

<p>A vector of predicted labels for test data x.
</p>


<h3>References</h3>

<p>Amiryousefi A. Asymptotic supervised predictive classifiers under
partition exchangeability. . 2021. <a href="https://arxiv.org/abs/2101.10950">https://arxiv.org/abs/2101.10950</a>.
</p>
<p>Corander, J., Cui, Y., Koski, T., and Siren, J.: Have I seen you before?
Principles of Bayesian predictive classification revisited. Springer, Stat.
Comput. 23, (2011), 59â€“73, (&lt;doi: <a href="https://doi.org/10.1007/s11222-011-9291-7">10.1007/s11222-011-9291-7</a>&gt;).
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Create random samples x from Poisson-Dirichlet distributions with different
## psis, treating each sample as coming from a class of its own:
set.seed(111)
x1&lt;-rPD(1050,10)
x2&lt;-rPD(1050,1000)
test.ind1&lt;-sample.int(1050,50) # Sample test datasets from the
test.ind2&lt;-sample.int(1050,50) # original samples
x&lt;-c(x1[-test.ind1],x2[-test.ind2])
## create training data labels:
y1&lt;-rep("1", 1000)
y2&lt;-rep("2", 1000)
y&lt;-c(y1,y2)

## Test data t, with first half belonging to class "1", second have in "2":
t1&lt;-x1[test.ind1]
t2&lt;-x2[test.ind2]
t&lt;-c(t1,t2)

fit&lt;-classifier.fit(x,y)

## Run the classifier, which returns
tS&lt;-tSimLab(fit, t)

##With multidimensional x:
set.seed(111)
x1&lt;-cbind(rPD(500,1),rPD(500,5))
x2&lt;-cbind(rPD(500,10),rPD(500,50))
test.ind1&lt;-sample.int(500,50)
test.ind2&lt;-sample.int(500,50)
x&lt;-rbind(x1[-test.ind1,],x2[-test.ind2,])
y1&lt;-rep("1", 450)
y2&lt;-rep("2", 450)
y&lt;-c(y1,y2)
fit&lt;-classifier.fit(x,y)
t1&lt;-x1[test.ind1,]
t2&lt;-x2[test.ind2,]
t&lt;-rbind(t1,t2)

tS&lt;-tSimLab(fit, t)
</code></pre>


</div>