<div class="container">

<table style="width: 100%;"><tr>
<td>cvpvs.knn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Cross-Validated P-Values (k Nearest Neighbors) </h2>

<h3>Description</h3>

<p>Computes cross-validated nonparametric p-values for the potential class memberships of the training data. The p-values are based on 'k nearest neighbors'.
</p>


<h3>Usage</h3>

<pre><code class="language-R">cvpvs.knn(X, Y, k = NULL, distance = c('euclidean', 'ddeuclidean',
          'mahalanobis'), cova = c('standard', 'M', 'sym'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p> matrix containing training observations, where each observation is a row vector. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p> vector indicating the classes which the training observations belong to. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p> number of nearest neighbors. If <code>k</code> is a vector or <code>k = NULL</code>, the program searches for the best <code>k</code>. For more information see section 'Details'. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distance</code></td>
<td>
<p> the distance measure: <br>
"euclidean":    fixed Euclidean distance, <br>
"ddeuclidean":  data driven Euclidean distance (component-wise standardization), <br>
"mahalanobis":  Mahalanobis distance. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cova</code></td>
<td>
<p> estimator for the covariance matrix: <br>
'standard': standard estimator, <br>
'M': M-estimator, <br>
'sym': symmetrized M-estimator. </p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Computes cross-validated nonparametric p-values for the potential class memberships of the training data. Precisely, for each feature vector <code>X[i,]</code> and each class <code>b</code> the number <code>PV[i,b]</code> is a p-value for the null hypothesis that <code class="reqn">Y[i] = b</code>. <br>
This p-value is based on a permutation test applied to an estimated Bayesian likelihood ratio, using 'k nearest neighbors' with estimated prior probabilities <code class="reqn">N(b)/n</code>. Here <code class="reqn">N(b)</code> is the number of observations of class <code class="reqn">b</code> and <code class="reqn">n</code> is the total number of observations. <br>
If <code>k</code> is a vector, the program searches for the best <code>k</code>. To determine the best <code>k</code> for the p-value <code>PV[i,b]</code>, the class label of the training observation <code class="reqn">X[i,]</code> is set temporarily to <code>b</code> and then for all training observations with <code>Y[j] != b</code> the proportion of the <code>k</code> nearest neighbors of <code>X[j,]</code> belonging to class <code>b</code> is computed. Then the <code>k</code> which minimizes the sum of these values is chosen. <br>
If <code>k = NULL</code>, it is set to 2:ceiling(length(Y)/2).
</p>


<h3>Value</h3>

 
<p><code>PV</code> is a matrix containing the cross-validated p-values. Precisely, for each feature vector <code>X[i,]</code> and each class <code>b</code> the number <code>PV[i,b]</code> is a p-value for the null hypothesis that <code class="reqn">Y[i] = b</code>. <br>
If <code>k</code> is a vector or <code>NULL</code>, <code>PV</code> has an attribute <code>"opt.k"</code>, which is a matrix and <code>opt.k[i,b]</code> is the best <code>k</code> for observation <code>X[i,]</code> and class <code>b</code> (see section 'Details'). <code>opt.k[i,b]</code> is used to compute the p-value for observation <code>X[i,]</code> and class <code>b</code>.
</p>


<h3>Author(s)</h3>

<p>Niki Zumbrunnen <a href="mailto:niki.zumbrunnen@gmail.com">niki.zumbrunnen@gmail.com</a> <br>
Lutz Dümbgen <a href="mailto:lutz.duembgen@stat.unibe.ch">lutz.duembgen@stat.unibe.ch</a> <br><a href="www.imsv.unibe.ch/duembgen/index_ger.html">www.imsv.unibe.ch/duembgen/index_ger.html</a>
</p>


<h3>References</h3>

<p>Zumbrunnen N. and Dümbgen L. (2017)
pvclass: An R Package for p Values for Classification.
<em>Journal of Statistical Software <b>78(4)</b></em>, 1–19.
doi:10.18637/jss.v078.i04
</p>
<p>Dümbgen L., Igl B.-W. and Munk A. (2008)
P-Values for Classification.
<em>Electronic Journal of Statistics <b>2</b></em>, 468–493, available at <a href="http://dx.doi.org/10.1214/08-EJS245">http://dx.doi.org/10.1214/08-EJS245</a>.
</p>
<p>Zumbrunnen N. (2014)
P-Values for Classification – Computational Aspects and Asymptotics.
Ph.D. thesis, University of Bern, available at <a href="http://boris.unibe.ch/id/eprint/53585">http://boris.unibe.ch/id/eprint/53585</a>.
</p>


<h3>See Also</h3>

 
<p><code>cvpvs, cvpvs.gaussian, cvpvs.wnn, cvpvs.logreg</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">X &lt;- iris[, 1:4]
Y &lt;- iris[, 5]

cvpvs.knn(X, Y, k = c(5, 10, 15))
</code></pre>


</div>