<div class="container">

<table style="width: 100%;"><tr>
<td>BAG_Model</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bagging Model</h2>

<h3>Description</h3>

<p>Bagging Model
</p>


<h3>Usage</h3>

<pre><code class="language-R">BAG_Model(Data, xvar, yvar)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Data</code></td>
<td>
<p>The name of the Dataset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xvar</code></td>
<td>
<p>X variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yvar</code></td>
<td>
<p>Y variable.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Decision trees suffer from high
variance (If we split the training data-set randomly into two parts and set a decision tree to both parts, the results might be quite different).
Bagging is an ensemble procedure which reduces the variance and increases the prediction accuracy of a statistical learning method
by considering many training sets
(<code class="reqn">\hat{f}^{1}(x),\hat{f}^{2}(x),\ldots,\hat{f}^{B}(x)</code>)
from the population. Since we can not have multiple training-sets, from a single training data-set, we can generate
<code class="reqn">B</code> different bootstrapped training data-sets
(<code class="reqn">\hat{f}^{*1}(x), \hat{f}^{*2}(x), \ldots,\hat{f}^{*B}(x)</code>)
by each <code class="reqn">B</code> trees and take a majority vote. Therefore, bagging for classification problem  defined as
</p>
<p style="text-align: center;"><code class="reqn">\hat{f}(x)=arg\max_{k}\hat{f}^{*b}(x)</code>
</p>



<h3>Value</h3>

<p>The output from  <code>BAG_Model</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
yvar &lt;- c("Loan.Type")
sample_data &lt;- sample_data[c(1:750),]
xvar &lt;- c("sex", "married", "age", "havejob", "educ", "political.afl",
"rural", "region", "fin.intermdiaries", "fin.knowldge", "income")
BchMk.BAG &lt;- BAG_Model(sample_data, c(xvar, "networth"), yvar )
BchMk.BAG$Roc$auc

</code></pre>


</div>