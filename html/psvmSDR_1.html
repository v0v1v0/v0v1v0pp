<div class="container">

<table style="width: 100%;"><tr>
<td>npsdr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>A unified Principal sufficient dimension reduction method via kernel trick</h2>

<h3>Description</h3>

<p>Principal Sufficient Dimension Reduction method
</p>


<h3>Usage</h3>

<pre><code class="language-R">npsdr(
  x,
  y,
  loss = "svm",
  h = 10,
  lambda = 1,
  b = floor(length(y)/3),
  eps = 1e-05,
  max.iter = 100,
  eta = 0.1,
  mtype,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>data matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>either continuous or (+1,-1) typed binary response vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>pre-specified loss functions belongs to <code>"svm", "logit", "l2svm", "wsvm", "qr", "asls", "wlogit", "wl2svm", "lssvm", "wlssvm"</code>, and user-defined loss function object also can be used formed by inside double (or single) quotation mark. Default is 'svm'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>h</code></td>
<td>
<p>the number of slices. default value is 10</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>hyperparameter for the loss function. default value is 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b</code></td>
<td>
<p>number of basis functions for a kernel trick, floor(length(y)/3) is default</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>threshold for stopping iteration with respect to the magnitude of derivative, default value is 1.0e-4</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>maximum iteration number for the optimization process. default value is 30</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>learning rate for gradient descent method. default value is 0.1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtype</code></td>
<td>
<p>type of margin, either "m" or "r" refer margin and residual, respectively (See, Table 1 in the pacakge manuscript). When one use user-defined loss function this argument should be specified. Default is "m".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>
<p>If <code>TRUE</code> then it produces scatter plots of <code class="reqn">Y</code> versus the first sufficient predictor. The default is FALSE.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object with S3 class "npsdr". Details are listed below.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>evalues</code></td>
<td>
<p>Eigenvalues of the estimated working matrix M.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>evectors</code></td>
<td>
<p>Eigenvectors of the estimated working matrix M, the first d leading eigenvectors consists
the basis of the central subspace.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>References</h3>

<p>Artemiou, A. and Dong, Y. (2016)
<em>Sufficient dimension reduction via principal lq support vector machine,
Electronic Journal of Statistics 10: 783–805</em>.<br>
Artemiou, A., Dong, Y. and Shin, S. J. (2021)
<em>Real-time sufficient dimension reduction through principal least
squares support vector machines, Pattern Recognition 112: 107768</em>.<br>
Kim, B. and Shin, S. J. (2019)
<em>Principal weighted logistic regression for sufficient dimension
reduction in binary classification, Journal of the Korean Statistical Society 48(2): 194–206</em>.<br>
Li, B., Artemiou, A. and Li, L. (2011)
<em>Principal support vector machines for linear and
nonlinear sufficient dimension reduction, Annals of Statistics 39(6): 3182–3210</em>.<br>
Soale, A.-N. and Dong, Y. (2022)
<em>On sufficient dimension reduction via principal asymmetric
least squares, Journal of Nonparametric Statistics 34(1): 77–94</em>.<br>
Wang, C., Shin, S. J. and Wu, Y. (2018)
<em>Principal quantile regression for sufficient dimension
reduction with heteroscedasticity, Electronic Journal of Statistics 12(2): 2114–2140</em>.<br>
Shin, S. J., Wu, Y., Zhang, H. H. and Liu, Y. (2017)
<em>Principal weighted support vector machines for sufficient dimension reduction in
binary classification, Biometrika 104(1): 67–81</em>. <br>
Li, L. (2007)
<em>Sparse sufficient dimension reduction, Biometrika 94(3): 603–613</em>.
</p>


<h3>See Also</h3>

<p><code>npsdr_x</code>, <code>psdr</code>, <code>rtpsdr</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
set.seed(1)
n &lt;- 200;
p &lt;- 5;
x &lt;- matrix(rnorm(n*p, 0, 2), n, p)
y &lt;- 0.5*sqrt((x[,1]^2+x[,2]^2))*(log(x[,1]^2+x[,2]^2))+ 0.2*rnorm(n)
obj_kernel &lt;- npsdr(x, y, plot=FALSE)
print(obj_kernel)
plot(obj_kernel)

</code></pre>


</div>