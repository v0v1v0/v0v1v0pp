<div class="container">

<table style="width: 100%;"><tr>
<td>ridgePrep</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Ridge penalized estimation of the precision matrix from data with replicates.
</h2>

<h3>Description</h3>

<p>Estimation of the precision matrix from data with replicates through a ridge penalized EM (Expectation-Maximization) algorithm. It assumes a simple 'signal+noise' model, both random variables are assumed to be drawn from a multivariate normal distribution with their own unstructured precision matrix. These precision matrices are estimated.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ridgePrep(Y, ids, lambdaZ, lambdaE, 		
          targetZ=matrix(0, ncol(Y), ncol(Y)),
          targetE=matrix(0, ncol(Y), ncol(Y)),
          nInit=100, minSuccDiff=10^(-10))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p> Data <code>matrix</code> with samples (including the repetitions) as rows and variates as columns. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ids</code></td>
<td>
<p> A <code>numeric</code> indicating which rows of <code>Y</code> belong to the same individal.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdaZ</code></td>
<td>
<p> A positive <code>numeric</code> representing the ridge penalty parameter for the signal precision matrix estimate. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdaE</code></td>
<td>
<p> A positive <code>numeric</code> representing the ridge penalty parameter for the error precision matrix estimate. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>targetZ</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the signal precision matrix estimate is shrunken. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>targetE</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the error precision matrix estimate is shrunken. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of the relative change in the absolute difference of the penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Data are assumed to originate from a design with replicates. Each observation <code class="reqn">\mathbf{Y}_{i,k_i}</code> with <code class="reqn">k_i</code> (<code class="reqn">k_i = 1, \ldots, K_i</code>) the <code class="reqn">k_i</code>-th replicate of the <code class="reqn">i</code>-th sample, is described by a ‘signal+noise’ model: <code class="reqn">\mathbf{Y}_{i,k_i} = \mathbf{Z}_i + \boldsymbol{\varepsilon}_{i,k_i}</code>, where <code class="reqn">\mathbf{Z}_i</code> and <code class="reqn">\boldsymbol{\varepsilon}_{i,k_i}</code> represent the signal and noise, respectively. Each observation <code class="reqn">\mathbf{Y}_{i,k_i}</code> follows a multivariate normal law of the form
<code class="reqn">\mathbf{Y}_{i,k_i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_z^{-1} + \boldsymbol{\Omega}_{\varepsilon}^{-1})</code>, which results from the distributional assumptions of the signal and the noise, <code class="reqn">\mathbf{Z}_{i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_z^{-1})</code> and <code class="reqn">\boldsymbol{\varepsilon}_{i, k_i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_{\varepsilon}^{-1})</code>, and their independence. The model parameters are estimated by means of a penalized EM algorithm that maximizes the loglikelihood augmented with the penalty <code class="reqn">\lambda_z \| \boldsymbol{\Omega}_z - \mathbf{T}_z \|_F^2 + \lambda_{\varepsilon} \| \boldsymbol{\Omega}_{\varepsilon} - \mathbf{T}_{\varepsilon} \|_F^2</code>, in which <code class="reqn">\mathbf{T}_z</code> and <code class="reqn">\mathbf{T}_{\varepsilon}</code> are the shrinkage targets of the signal and noise precision matrices, respectively. For more details see van Wieringen and Chen (2019).
</p>


<h3>Value</h3>

<p>The function returns the regularized inverse covariance <code>list</code>-object with slots:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Pz</code></td>
<td>
<p> The estimated signal precision matrix. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Pz</code></td>
<td>
<p> The estimated error precision matrix. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penLL</code></td>
<td>
<p> The penalized loglikelihood of the estimated model. </p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N., Chen, Y. (2021), "Penalized estimation of the Gaussian graphical model from data with replicates", <em>Statistics in Medicine</em>, 40(19), 4279-4293.
</p>


<h3>See Also</h3>

<p><code>optPenaltyPrep.kCVauto</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># set parameters
p        &lt;- 10
Se       &lt;- diag(runif(p))
Sz       &lt;- matrix(3, p, p)
diag(Sz) &lt;- 4

# draw data
n &lt;- 100
ids &lt;- numeric()
Y   &lt;- numeric()
for (i in 1:n){
     Ki &lt;- sample(2:5, 1)
     Zi &lt;- mvtnorm::rmvnorm(1, sigma=Sz)
     for (k in 1:Ki){
          Y   &lt;- rbind(Y, Zi + mvtnorm::rmvnorm(1, sigma=Se))
          ids &lt;- c(ids, i)
     }
}

# estimate
Ps &lt;- ridgePrep(Y, ids, 1, 1) 
</code></pre>


</div>