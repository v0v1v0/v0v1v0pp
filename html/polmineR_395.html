<div class="container">

<table style="width: 100%;"><tr>
<td>pmi</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate Pointwise Mutual Information (PMI).</h2>

<h3>Description</h3>

<p>Calculate Pointwise Mutual Information as an information-theoretic approach
to find collocations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pmi(.Object, ...)

## S4 method for signature 'context'
pmi(.Object)

## S4 method for signature 'Cooccurrences'
pmi(.Object)

## S4 method for signature 'ngrams'
pmi(.Object, observed, p_attribute = p_attributes(.Object)[1])
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>.Object</code></td>
<td>
<p>An object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Arguments methods may require.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>observed</code></td>
<td>
<p>A <code>count</code>-object with the numbers of the observed
occurrences of the tokens in the input <code>ngrams</code> object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p_attribute</code></td>
<td>
<p>The positional attribute which shall be considered. Relevant only
if ngrams have been calculated for more than one p-attribute.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Pointwise mutual information (PMI) is calculated as follows (see
Manning/Schuetze 1999):
</p>
<p style="text-align: center;"><code class="reqn">I(x,y) = log\frac{p(x,y)}{p(x)p(y)}</code>
</p>

<p>The formula is based on maximum likelihood estimates: When we know the number
of observations for token x, <code class="reqn">o_{x}</code>, the number of observations
for token y, <code class="reqn">o_{y}</code> and the size of the corpus N, the
propabilities for the tokens x and y, and for the co-occcurence of x and y
are as follows:
</p>
<p style="text-align: center;"><code class="reqn">p(x) = \frac{o_{x}}{N}</code>
</p>

<p style="text-align: center;"><code class="reqn">p(y) = \frac{o_{y}}{N}</code>
</p>

<p>The term p(x,y) is the number of observed co-occurrences of x and y.
</p>
<p>Note that the computation uses log base 2, not the natural logarithm you find
in examples (e.g. <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information">https://en.wikipedia.org/wiki/Pointwise_mutual_information</a>).
</p>


<h3>References</h3>

<p>Manning, Christopher D.; Schuetze, Hinrich (1999): <em>Foundations of Statistical Natural Language
Processing</em>. MIT Press: Cambridge, Mass., pp. 178-183.
</p>


<h3>See Also</h3>

<p>Other statistical methods: 
<code>chisquare()</code>,
<code>ll()</code>,
<code>t_test()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">y &lt;- cooccurrences("REUTERS", query = "oil", method = "pmi")
N &lt;- size(y)[["partition"]]
I &lt;- log2((y[["count_coi"]]/N) / ((count(y) / N) * (y[["count_partition"]] / N)))
use("polmineR")
use(pkg = "RcppCWB", corpus = "REUTERS")

dt &lt;- decode(
  "REUTERS",
  p_attribute = "word",
  s_attribute = character(), 
  to = "data.table",
  verbose = FALSE
)
n &lt;- ngrams(dt, n = 2L, p_attribute = "word")
obs &lt;- count("REUTERS", p_attribute = "word")
phrases &lt;- pmi(n, observed = obs)
</code></pre>


</div>