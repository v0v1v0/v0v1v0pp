<div class="container">

<table style="width: 100%;"><tr>
<td>t_test</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Perform t-test.</h2>

<h3>Description</h3>

<p>Compute t-scores to find collocations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">t_test(.Object)

## S4 method for signature 'context'
t_test(.Object)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>.Object</code></td>
<td>
<p>A <code>context</code> or <code>features</code> object</p>
</td>
</tr></table>
<h3>Details</h3>

<p>The calculation of the t-test is based on the formula
</p>
<p style="text-align: center;"><code class="reqn">t = \frac{\overline{x} - \mu}{\sqrt{\frac{s^2}{N}}}</code>
</p>

<p>where <code class="reqn">\mu</code> is the mean of the distribution, x the sample mean,
<code class="reqn">s^2</code> the sample variance, and N the sample size.
</p>
<p>Following Manning and Schuetze (1999), to test whether two tokens (a and b)
are a collocation, the sample mean <code class="reqn">\mu</code> is the number of observed
co-occurrences of a and b divided by corpus size N:
</p>
<p style="text-align: center;"><code class="reqn">\mu = \frac{o_{ab}}{N}</code>
</p>

<p>For the mean of the distribution <code class="reqn">\overline{x}</code>, maximum likelihood estimates
are used. Given that we know the number of observations of token a, <code class="reqn">o_{a}</code>, the
number of observations of b, <code class="reqn">o_{b}</code> and the size of the corpus N, the
propabilities for the tokens a and b, and for the co-occcurence of a and be
are as follows, if independence is assumed:
</p>
<p style="text-align: center;"><code class="reqn">P(a) = \frac{o_{a}}{N}</code>
</p>

<p style="text-align: center;"><code class="reqn">P(b) = \frac{o_{b}}{N}</code>
</p>

<p style="text-align: center;"><code class="reqn">P(ab) = P(a)P(b)</code>
</p>

<p>See the examples for a sample calulation of the t-test, and Evert (2005: 83)
for a critical discussion of the "highly questionable" assumptions when using
the t-test for detecting co-occurrences.
</p>


<h3>References</h3>

<p>Manning, Christopher D.; Schuetze, Hinrich (1999):
<em>Foundations of Statistical Natural Language Processing</em>. MIT Press:
Cambridge, Mass., pp. 163-166.
</p>
<p>Church, Kenneth W. et al. (1991): Using Statistics in Lexical
Analysis. In: Uri Zernik (ed.), <em>Lexical Acquisition</em>. Hillsdale,
NJ:Lawrence Erlbaum, pp. 115-164
<a href="https://doi.org/10.4324/9781315785387-8">doi:10.4324/9781315785387-8</a>
</p>
<p>Evert, Stefan (2005): <em>The Statistics of Word Cooccurrences.
Word Pairs and Collocations.</em> URN urn:nbn:de:bsz:93-opus-23714.
<a href="https://elib.uni-stuttgart.de/bitstream/11682/2573/1/Evert2005phd.pdf">https://elib.uni-stuttgart.de/bitstream/11682/2573/1/Evert2005phd.pdf</a>
</p>


<h3>See Also</h3>

<p>Other statistical methods: 
<code>chisquare()</code>,
<code>ll()</code>,
<code>pmi()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">use("polmineR")
y &lt;- cooccurrences("REUTERS", query = "oil", left = 1L, right = 0L, method = "t_test")
# The critical value (for a = 0.005) is 2.579, so "crude" is a collocation
# of "oil" according to t-test.

# A sample calculation
count_oil &lt;- count("REUTERS", query = "oil")
count_crude &lt;- count("REUTERS", query = "crude")
count_crude_oil &lt;- count("REUTERS", query = '"crude" "oil"', cqp = TRUE)

p_crude &lt;- count_crude$count / size("REUTERS")
p_oil &lt;- count_oil$count / size("REUTERS")
p_crude_oil &lt;- p_crude * p_oil

x &lt;- count_crude_oil$count / size("REUTERS")

t_value &lt;- (x - p_crude_oil) / sqrt(x / size("REUTERS"))
# should be identical with previous result:
as.data.frame(subset(y, word == "crude"))$t_test
</code></pre>


</div>