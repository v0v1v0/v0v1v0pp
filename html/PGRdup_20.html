<div class="container">

<table style="width: 100%;"><tr>
<td>ReviewProbDup</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Retrieve probable duplicate set information from PGR passport database for
review</h2>

<h3>Description</h3>

<p><code>ReviewProbDup</code> retrieves information associated with the probable
duplicate sets from the original PGR passport database(s) from which they
were identified in order to facilitate manual clerical review.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ReviewProbDup(
  pdup,
  db1,
  db2 = NULL,
  extra.db1 = NULL,
  extra.db2 = NULL,
  max.count = 30,
  insert.blanks = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pdup</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>db1</code></td>
<td>
<p>A data frame of the PGR passport database.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>db2</code></td>
<td>
<p>A data frame of the PGR passport database. Required when
<code>pdup</code> was created using more than one KWIC Index.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>extra.db1</code></td>
<td>
<p>A character vector of extra <code>db1</code> column names to be
retrieved.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>extra.db2</code></td>
<td>
<p>A character vector of extra <code>db2</code> column names to be
retrieved.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.count</code></td>
<td>
<p>The maximum count of probable duplicate sets whose
information is to be retrieved.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>insert.blanks</code></td>
<td>
<p>logical. If <code>TRUE</code>, inserts a row of /codeNAs
after each set.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function helps to retrieve PGR passport information associated with
fuzzy, phonetic or semantic probable duplicate sets in an object of class
<code>ProbDup</code> from the original databases(s) from which they were
identified. The original information of accessions comprising a set, which
have not been subjected to data standardization can be compared under manual
clerical review for the validation of the set.
</p>
<p>By default only the fields(columns) which were used initially for creation of
the KWIC indexes using the <code>KWIC</code> function are retrieved.
Additional fields(columns) if necessary can be specified using the
<code>extra.db1</code> and <code>extra.db2</code> arguments.
</p>
<p>The output data frame can be subjected to clerical review either after
exporting into an external spreadsheet using
<code>write.csv</code> function or by using the
<code>edit</code> function.
</p>
<p>The column <code>DEL</code> can be used to indicate whether a record has to be
deleted from a set or not. <code>Y</code> indicates "Yes", and the default <code>N</code>
indicates "No".
</p>
<p>The column <code>SPLIT</code> similarly can be used to indicate whether a record in
a set has to be branched into a new set. A set of identical integers in this
column other than the default <code>0</code> can be used to indicate that they are
to be removed and assembled into a new set.
</p>


<h3>Value</h3>

<p>A data frame of the long/narrow form of the probable duplicate sets
data along with associated fields from the original database(s). The core
columns in the resulting data frame are as follows: </p>

<table>
<tr>
<td style="text-align: left;">
  <code>SET_NO</code> </td>
<td style="text-align: left;"> The set number. </td>
</tr>
<tr>
<td style="text-align: left;"> <code>TYPE</code> </td>
<td style="text-align: left;"> The type of
  probable duplicate set. 'F' for fuzzy, 'P' for phonetic and 'S' for
  semantic matching sets. </td>
</tr>
<tr>
<td style="text-align: left;"> <code>K[*]</code> </td>
<td style="text-align: left;"> The KWIC index or database of
  origin of the record. The <code>method</code> is specified within the square
  brackets in the column name.  </td>
</tr>
<tr>
<td style="text-align: left;"> <code>PRIM_ID</code> </td>
<td style="text-align: left;"> The primary ID of the
  accession record from which the set could be identified. </td>
</tr>
<tr>
<td style="text-align: left;"> <code>IDKW</code>
  </td>
<td style="text-align: left;"> The 'matching' keywords along with the IDs. </td>
</tr>
<tr>
<td style="text-align: left;"> <code>DEL</code> </td>
<td style="text-align: left;"> Column
  to indicate whether record has to be deleted or not. </td>
</tr>
<tr>
<td style="text-align: left;"> <code>SPLIT</code> </td>
<td style="text-align: left;">
  Column to indicate whether record has to be branched and assembled into new
  set. </td>
</tr>
<tr>
<td style="text-align: left;"> <code>COUNT</code> </td>
<td style="text-align: left;"> The number of elements in a set. </td>
</tr>
<tr>
<td style="text-align: left;"> </td>
</tr>
</table>
<p> For the
retrieved columns(fields) the prefix <code>K*</code> indicates the KWIC index of
origin.
</p>


<h3>Note</h3>

<p>When any primary ID/key records in the fuzzy, phonetic or semantic
duplicate sets are found to be missing from the original databases
<code>db1</code> and <code>db2</code>, then they are ignored and only the matching
records are considered for retrieving the information with a warning.
</p>
<p>This may be due to data standardization of the primary ID/key field using
the function <code>DataClean</code> before creation of the KWIC
index and subsequent identification of probable duplicate sets. In such a
case, it is recommended to use an identical data standardization operation
on the databases <code>db1</code> and <code>db2</code> before running this function.
</p>
<p>With <code>R</code> &lt;= v3.0.2, due to copying of named objects by <code>list()</code>,
<code>Invalid .internal.selfref detected and fixed...</code> warning can appear,
which may be safely ignored.
</p>


<h3>See Also</h3>

<p><code>DataClean</code>, <code>KWIC</code>,
<code>ProbDup</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">


## Not run: 

# Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE",
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT",
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE",
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R",
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE",
         "U", "VALENCIA", "VIRGINIA", "WHITE")

# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary",
                 semantic = TRUE, syn = syn)

# Get disjoint probable duplicate sets of each kind
disGNdup &lt;- DisProbDup(GNdup, combine = NULL)

# Get the data frame for reviewing the duplicate sets identified
RevGNdup &lt;- ReviewProbDup(pdup = disGNdup, db1 = GN1000,
                          extra.db1 = c("SourceCountry", "TransferYear"),
                          max.count = 30, insert.blanks = TRUE)
# Examine and review the duplicate sets using edit function
RevGNdup &lt;- edit(RevGNdup)

# OR examine and review the duplicate sets after exporting them as a csv file
write.csv(file="Duplicate sets for review.csv", x=RevGNdup)


## End(Not run)



</code></pre>


</div>