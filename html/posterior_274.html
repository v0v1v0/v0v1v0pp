<div class="container">

<table style="width: 100%;"><tr>
<td>rstar</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate R* convergence diagnostic</h2>

<h3>Description</h3>

<p>The <code>rstar()</code> function generates a measure of convergence for MCMC draws
based on whether it is possible to determine the Markov chain that generated
a draw with probability greater than chance. To do so, it fits a machine
learning classifier to a training set of MCMC draws and evaluates its
predictive accuracy on a testing set: giving the ratio of accuracy to
predicting a chain uniformly at random.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rstar(
  x,
  split = TRUE,
  uncertainty = FALSE,
  method = "rf",
  hyperparameters = NULL,
  training_proportion = 0.7,
  nsimulations = 1000,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>(draws) A <code>draws_df</code> object or one coercible to a <code>draws_df</code> object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>split</code></td>
<td>
<p>(logical) Should the estimate be computed on split chains? The
default is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>uncertainty</code></td>
<td>
<p>(logical). Indicates whether to provide a vector of R*
values representing uncertainty in the calculated value (if <code>TRUE</code>) or a
single value (if <code>FALSE</code>). The default is <code>TRUE.</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>(string) The machine learning classifier to use (must be
available in the <span class="pkg">caret</span> package). The default is <code>"rf"</code>, which calls
the random forest classifier.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hyperparameters</code></td>
<td>
<p>(named list) Hyperparameter settings passed to the classifier.
The default for the random forest classifier (<code>method = "rf"</code>) is
<code>list(mtry = floor(sqt(nvariables(x))))</code>.
The default for the gradient-based model (<code>method = "gbm"</code>) is
<code>list(interaction.depth = 3, n.trees = 50, shrinkage = 0.1, n.minobsinnode = 10)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>training_proportion</code></td>
<td>
<p>(positive real) The proportion (in <code style="white-space: pre;">⁠(0,1)⁠</code>) of
iterations in used to train the classifier. The default is <code>0.7</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nsimulations</code></td>
<td>
<p>(positive integer) The number of R* values in the
returned vector if <code>uncertainty</code> is <code>TRUE</code>. The default is <code>1000.</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other arguments passed to <code>caret::train()</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>rstar()</code> function provides a measure of MCMC convergence based
on whether it is possible to determine the chain that generated a
particular draw with a probability greater than chance. To do so, it fits a
machine learning classifier to a subset of the original MCMC draws (the
training set) and evaluates its predictive accuracy on the remaining draws
(the testing set). If predictive accuracy exceeds chance (i.e. predicting
the chain that generated a draw uniformly at random), the diagnostic
measure R* will be above 1, indicating that convergence has yet to occur.
This statistic is recently developed, and it is currently unclear what is a
reasonable threshold for diagnosing convergence.
</p>
<p>The statistic, R*, is stochastic, meaning that each time the test is run,
unless the random seed is fixed, it will generally produce a different
result. To minimize the implications of this stochasticity, it is
recommended to repeatedly run this function to calculate a distribution of
R*; alternatively, an approximation to this distribution can be obtained by
setting <code>uncertainty = TRUE</code>, although this approximation of uncertainty
will generally have a lower mean.
</p>
<p>By default, a random forest classifier is used (<code>method = "rf"</code>), which tends
to perform best for target distributions of around 4 dimensions and above.
For lower dimensional targets, gradient boosted models (called via
<code>method = "gbm"</code>) tend to have a higher classification accuracy. On a given
MCMC sample, it is recommended to try both of these classifiers.
</p>


<h3>Value</h3>

<p>A numeric vector of length 1 (by default) or length <code>nsimulations</code>
(if <code>uncertainty = TRUE</code>).
</p>


<h3>References</h3>

<p>Ben Lambert, Aki Vehtari (2020) R*: A robust MCMC convergence
diagnostic with uncertainty using gradient-boosted machines.
<em>arXiv preprint</em> <code>arXiv:2003.07900</code>.
</p>


<h3>See Also</h3>

<p>Other diagnostics: 
<code>ess_basic()</code>,
<code>ess_bulk()</code>,
<code>ess_quantile()</code>,
<code>ess_sd()</code>,
<code>ess_tail()</code>,
<code>mcse_mean()</code>,
<code>mcse_quantile()</code>,
<code>mcse_sd()</code>,
<code>pareto_diags()</code>,
<code>pareto_khat()</code>,
<code>rhat()</code>,
<code>rhat_basic()</code>,
<code>rhat_nested()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
if (require("caret", quietly = TRUE) &amp;&amp; require("randomForest", quietly = TRUE)) {
  x &lt;- example_draws("eight_schools")
  print(rstar(x))
  print(rstar(x, split = FALSE))
  print(rstar(x, method = "gbm"))
  # can pass additional arguments to methods
  print(rstar(x, method = "gbm", verbose = FALSE))

  # with uncertainty, returns a vector of R* values
  hist(rstar(x, uncertainty = TRUE))
  hist(rstar(x, uncertainty = TRUE, nsimulations = 100))

  # can use other classification methods in caret library
  print(rstar(x, method = "knn"))
}

</code></pre>


</div>