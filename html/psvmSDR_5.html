<div class="container">

<table style="width: 100%;"><tr>
<td>psdr</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Unified linear principal sufficient dimension reduction methods</h2>

<h3>Description</h3>

<p>A function for a linear principal sufficient dimension reduction.
</p>


<h3>Usage</h3>

<pre><code class="language-R">psdr(
  x,
  y,
  loss = "svm",
  h = 10,
  lambda = 1,
  eps = 1e-05,
  max.iter = 100,
  eta = 0.1,
  mtype = "m",
  plot = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>input matrix, of dimension <code>nobs</code> x <code>nvars</code>; each row is an observation vector. Requirement: <code>nvars</code>&gt;1; in other words, <code>x</code> should have 2 or more columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>response variable, either can be continuous variable or (+1,-1) coded binary response vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>pre-specified loss functions belongs to <code>"svm", "logit", "l2svm", "wsvm", "qr", "asls", "wlogit", "wl2svm", "lssvm", "wlssvm"</code>, and user-defined loss function object also can be used formed by inside double (or single) quotation mark. Default is 'svm'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>h</code></td>
<td>
<p>the number of slices and probabilities equally spaced in <code class="reqn">(0,1)</code>. Default value is 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>the cost parameter for the svm loss function. The default value is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>the threshold for stopping iteration with respect to the magnitude of the change of the derivative. The default value is 1.0e-5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>maximum iteration number for the optimization process. default value is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>learning rate for the gradient descent algorithm. The default value is 0.1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtype</code></td>
<td>
<p>a margin type, which is either margin ("m") or residual ("r") (See, Table 1 in the manuscript). Only need when user-defined loss is used. Default is "m".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>
<p>If <code>TRUE</code> then it produces scatter plots of <code class="reqn">Y</code> versus <code class="reqn">\hat{B^{\top}}_{j}\mathbf{X}</code>. <code class="reqn">j</code> can be specified by the user with <code class="reqn">j=1</code> as a default. The default is FALSE.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Two examples of the usage of user-defined losses are presented below (<code>u</code> represents a margin):
</p>
<p><code>mylogit &lt;- function(u, ...) log(1+exp(-u))</code>,
</p>
<p><code>myls &lt;- function(u ...) u^2</code>.
</p>
<p>Argument <code>u</code> is a function variable  (any character is possible) and the argument <code>mtype</code> for <code>psdr()</code> determines a type of a margin, either (<code>type="m"</code>) or (<code>type="r"</code>) method. <code>type="m"</code> is a default.
Users have to change <code>type="r"</code>, when applying residual type loss.
Any additional parameters of the loss can be specified via <code>...</code> argument.
</p>


<h3>Value</h3>

<p>An object with S3 class "psdr". Details are listed below.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Mn</code></td>
<td>
<p>The estimated working matrix, which is obtained by the cumulative
outer product of the estimated parameters over the slices. It will not print out, unless it is called manually.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>evalues</code></td>
<td>
<p>Eigenvalues of the working matrix <code class="reqn">Mn</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>evectors</code></td>
<td>
<p>Eigenvectors of the <code class="reqn">Mn</code>, the first leading <code class="reqn">d</code> eigenvectors consists
the basis of the central subspace</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>References</h3>

<p>Artemiou, A. and Dong, Y. (2016)
<em>Sufficient dimension reduction via principal lq support vector machine,
Electronic Journal of Statistics 10: 783–805</em>.<br>
Artemiou, A., Dong, Y. and Shin, S. J. (2021)
<em>Real-time sufficient dimension reduction through principal least
squares support vector machines, Pattern Recognition 112: 107768</em>.<br>
Kim, B. and Shin, S. J. (2019)
<em>Principal weighted logistic regression for sufficient dimension
reduction in binary classification, Journal of the Korean Statistical Society 48(2): 194–206</em>.<br>
Li, B., Artemiou, A. and Li, L. (2011)
<em>Principal support vector machines for linear and
nonlinear sufficient dimension reduction, Annals of Statistics 39(6): 3182–3210</em>.<br>
Soale, A.-N. and Dong, Y. (2022)
<em>On sufficient dimension reduction via principal asymmetric
least squares, Journal of Nonparametric Statistics 34(1): 77–94</em>.<br>
Wang, C., Shin, S. J. and Wu, Y. (2018)
<em>Principal quantile regression for sufficient dimension
reduction with heteroscedasticity, Electronic Journal of Statistics 12(2): 2114–2140</em>.<br>
Shin, S. J., Wu, Y., Zhang, H. H. and Liu, Y. (2017)
<em>Principal weighted support vector machines for sufficient dimension reduction in
binary classification, Biometrika 104(1): 67–81</em>. <br>
Li, L. (2007)
<em>Sparse sufficient dimension reduction, Biometrika 94(3): 603–613</em>.
</p>


<h3>See Also</h3>

<p><code>psdr_bic</code>, <code>rtpsdr</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## ----------------------------
## Linear PM
## ----------------------------
set.seed(1)
n &lt;- 200; p &lt;- 5;
x &lt;- matrix(rnorm(n*p, 0, 2), n, p)
y &lt;-  x[,1]/(0.5 + (x[,2] + 1)^2) + 0.2*rnorm(n)
y.tilde &lt;- sign(y)
obj &lt;- psdr(x, y)
print(obj)
plot(obj, d=2)

## ----------------------------
## Kernel PM
## ----------------------------
obj_wsvm &lt;- psdr(x, y.tilde, loss="wsvm")
plot(obj_wsvm)

## ----------------------------
## User-defined loss function
## ----------------------------
mylogistic &lt;- function(u) log(1+exp(-u))
psdr(x, y, loss="mylogistic")

</code></pre>


</div>