<div class="container">

<table style="width: 100%;"><tr>
<td>gkpfcm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Gustafson-Kessel Clustering Using PFCM
</h2>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Gustafson-Kessel (GK) algorithm within the PFCM (Possibilistic Fuzzy C-Means) clustering algorithm (Ojeda-Magaina et al, 2006).
</p>


<h3>Usage</h3>

<pre><code class="language-R">gkpfcm(x, centers, memberships, m=2, eta=2, K=1, omega, a, b, 
    dmetric="sqeuclidean", pw = 2, alginitv="kmpp", 
    alginitu="imembrand", nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>
<p>a number for the relative importance of the fuzzy part of the objective function. The default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b</code></td>
<td>
<p>a number for the relative importance of the possibilistic part of the objective function. The default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>a number greater than 0 to be used as the weight of penalty term. The default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>omega</code></td>
<td>
<p>a numeric vector of reference distances. If missing, it is internally generated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code>get.dmetrics</code> for the alternative options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code>get.algorithms</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Gustafson-Kessel clustering within Possibilistic Fuzzy C-Means (GKPFCM) algorithm is an improvement for PFCM algorithm that consists of the modification of the distance metric for <code class="reqn">d_{ij_A}</code>. The original PFCM uses the Euclidean distance whereas GKPFCM uses the Mahalanobis distance with GK algorithm. Babuska et al (2002) have proposed an improvement for calculating the covariance matrix <code class="reqn">\mathbf{F}_j</code> as follows:
</p>
<p><code class="reqn">\mathbf{F}_j := (1 - \gamma) \mathbf{F}_j + \gamma (\mathbf{F}_0)^{1/n} \mathbf{I}</code>
</p>
<p>In the above equation, <code class="reqn">\mathbf{F}_j</code> involves a weighted identity matrix. The eigenvalues <code class="reqn">\lambda_{ij}</code> and the eigenvectors <code class="reqn">\Phi_{ij}</code> of the resulting matrix are calculated, and the maximum eigenvalue <code class="reqn">\lambda_{i,max} = max_{j}/ \lambda_{ij}</code> is determined. With the obtained results, <code class="reqn">\lambda_{i,max} = \lambda_{ij}/\beta, \forall j</code>, which satisfies <code class="reqn">\lambda_{i,max} / \lambda_{ij} \geq \beta</code> is calculated. Finally, <code class="reqn">\mathbf{F}_j</code> is recomputed with the following equation:
</p>
<p><code class="reqn">\mathbf{F}_j = [\Phi_{j,1},\dots, \Phi_{j,n}] diag(\lambda_{j,1}, \dots, \lambda_{j,n}) [\Phi_{j,1},\dots, \Phi_{j,n}]^{-1} \;\; \forall j</code>
</p>
<p>The objective function of GKPFCM is:
</p>
<p><code class="reqn">J_{GKPFCM}(\mathbf{X}; \mathbf{V}, \mathbf{A}, \mathbf{U}) = \sum\limits_{i=1}^n \sum\limits_{j=1}^k  u_{ij}^m d_{A_j}(\vec{x}_i, \vec{v}_j)</code>
</p>
<p><code class="reqn">m</code> is the fuzzifier to specify the amount of fuzziness for the clustering; <code class="reqn">1\leq m\leq \infty</code>. It is usually chosen as 2. 
</p>
<p><code class="reqn">\eta</code> is the typicality exponent to specify the amount of typicality for the clustering; <code class="reqn">1\leq \eta\leq \infty</code>. It is usually chosen as 2. 
</p>
<p>The objective function <code class="reqn">J_{GKPFCM}</code> is minimized by using the following update equations:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d_{A_j}(\vec{x}_i, \vec{v}_j)}{d_{A_j}(\vec{x}_i, \vec{v}_l)}\Big)^{2/(m-1)} \Bigg]^{-1} \;\;; 1\leq i \leq n \;,\; 1 \leq l \leq k</code>
</p>
<p><code class="reqn">t_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d_{A_j}(\vec{x}_i, \vec{v}_j))}{d_{A_j}(\vec{x}_i, \vec{v}_l))}\Big)^{2/(\eta-1)} \Bigg]^{-1} \;;\; 1 \leq i \leq n \;;\, 1 \leq l \leq k</code>
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n (u_{ij}^m + t_{ij}^\eta) \vec{x}_i}{\sum\limits_{i=1}^n (u_{ij}^m + t_{ij}^\eta)} \;\;; {1\leq j\leq k}</code>
</p>


<h3>Value</h3>

<p>an object of class ‘ppclust’, which is a list consists of the following items:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>
<p>a number for the relative importance of the fuzzy part of the objective function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b</code></td>
<td>
<p>a number for the relative importance of the possibilistic part of the objective function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>omega</code></td>
<td>
<p>a numeric vector of reference distances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is ‘FCM’ with this function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>a string for the matched function call generating this ‘ppclust’ object.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Zeynel Cebeci &amp; Cagatay Cebeci
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Gustafson, D. E. &amp; Kessel, W. C. (1979). Fuzzy clustering with a fuzzy covariance matrix. In <em>Proc. of IEEE Conf. on Decision and Control including the 17th Symposium on Adaptive Processes</em>, San Diego. pp. 761-766. &lt;doi:10.1109/CDC.1978.268028&gt;
</p>
<p>Babuska, R., van der Veen, P. J. &amp;  Kaymak, U. (2002). Improved covariance estimation for Gustafson-Kessel clustering. In <em>Proc. of Int. Conf. on Fuzzy Systems</em>, Hawaii, 2002, pp. 1081-1085. &lt;<a href="https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control">https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control</a>&gt;.
</p>
<p>Ojeda-Magaina, B., Ruelas, R., Corona-Nakamura, M. A. &amp; Andina, D. (2006). An improvement to the possibilistic fuzzy c-means clustering algorithm. In <em>Proc. of IEEE World Automation Congress, 2006 (WAC'06)</em>. pp. 1-8. &lt;doi:10.1109/WAC.2006.376056&gt;
</p>


<h3>See Also</h3>

<p><code>ekm</code>,
<code>fcm</code>,
<code>fcm2</code>,
<code>fpcm</code>,
<code>fpppcm</code>,
<code>gg</code>,
<code>gk</code>,
<code>hcm</code>,
<code>pca</code>,
<code>pcm</code>,
<code>pcmr</code>,
<code>pfcm</code>,
<code>upfc</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=3)$v

# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=3)$u

# Run FCM with the initial prototypes and memberships
gkpfcm.res &lt;- gkpfcm(x, centers=v, memberships=u, m=2)

# Show the fuzzy membership degrees for the top 5 objects
head(gkpfcm.res$u, 5)

## End(Not run)
</code></pre>


</div>