<div class="container">

<table style="width: 100%;"><tr>
<td>details_boost_tree_xgboost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Boosted trees via xgboost</h2>

<h3>Description</h3>

<p><code>xgboost::xgb.train()</code> creates a series of decision trees forming an
ensemble. Each tree depends on the results of previous trees. All trees in
the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 8 tuning parameters:
</p>

<ul>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 6L)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 15L)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.3)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1L)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0.0)
</p>
</li>
<li> <p><code>sample_size</code>: Proportion Observations Sampled (type: double, default:
1.0)
</p>
</li>
<li> <p><code>stop_iter</code>: # Iterations Before Stopping (type: integer, default:
Inf)
</p>
</li>
</ul>
<p>For <code>mtry</code>, the default value of <code>NULL</code> translates to using all
available columns.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric(),
  stop_iter = integer()
) %&gt;%
  set_engine("xgboost") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
##   sample_size = numeric()
##   stop_iter = integer()
## 
## Computational engine: xgboost 
## 
## Model fit template:
## parsnip::xgb_train(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     colsample_bynode = integer(), nrounds = integer(), min_child_weight = integer(), 
##     max_depth = integer(), eta = numeric(), gamma = numeric(), 
##     subsample = numeric(), early_stop = integer(), nthread = 1, 
##     verbose = 0)
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric(),
  stop_iter = integer()
) %&gt;% 
  set_engine("xgboost") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
##   sample_size = numeric()
##   stop_iter = integer()
## 
## Computational engine: xgboost 
## 
## Model fit template:
## parsnip::xgb_train(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     colsample_bynode = integer(), nrounds = integer(), min_child_weight = integer(), 
##     max_depth = integer(), eta = numeric(), gamma = numeric(), 
##     subsample = numeric(), early_stop = integer(), nthread = 1, 
##     verbose = 0)
</pre></div>
<p><code>xgb_train()</code> is a wrapper around
<code>xgboost::xgb.train()</code> (and other functions)
that makes it easier to run this model.
</p>



<h4>Preprocessing requirements</h4>

<p>xgboost does not have a means to translate factor predictors to grouped
splits. Factor/categorical predictors need to be converted to numeric
values (e.g., dummy or indicator variables) for this engine. When using
the formula method via <code>fit.model_spec()</code>, parsnip
will convert factor columns to indicators using a one-hot encoding.
</p>
<p>For classification, non-numeric outcomes (i.e., factors) are internally
converted to numeric. For binary classification, the <code>event_level</code>
argument of <code>set_engine()</code> can be set to either <code>"first"</code> or <code>"second"</code>
to specify which level should be used as the event. This can be helpful
when a watchlist is used to monitor performance from with the xgboost
training process.
</p>



<h4>Other details</h4>



<h5>Interfacing with the <code>params</code> argument</h5>

<p>The xgboost function that parsnip indirectly wraps,
<code>xgboost::xgb.train()</code>, takes most arguments via
the <code>params</code> list argument. To supply engine-specific arguments that are
documented in <code>xgboost::xgb.train()</code> as
arguments to be passed via <code>params</code>, supply the list elements directly
as named arguments to <code>set_engine()</code> rather than as
elements in <code>params</code>. For example, pass a non-default evaluation metric
like this:
</p>
<div class="sourceCode r"><pre># good
boost_tree() %&gt;%
  set_engine("xgboost", eval_metric = "mae")
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (unknown mode)
## 
## Engine-Specific Arguments:
##   eval_metric = mae
## 
## Computational engine: xgboost
</pre></div>
<p>…rather than this:
</p>
<div class="sourceCode r"><pre># bad
boost_tree() %&gt;%
  set_engine("xgboost", params = list(eval_metric = "mae"))
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (unknown mode)
## 
## Engine-Specific Arguments:
##   params = list(eval_metric = "mae")
## 
## Computational engine: xgboost
</pre></div>
<p>parsnip will then route arguments as needed. In the case that arguments
are passed to <code>params</code> via <code>set_engine()</code>, parsnip will
warn and re-route the arguments as needed. Note, though, that arguments
passed to <code>params</code> cannot be tuned.
</p>



<h5>Sparse matrices</h5>

<p>xgboost requires the data to be in a sparse format. If your predictor
data are already in this format, then use
<code>fit_xy.model_spec()</code> to pass it to the model
function. Otherwise, parsnip converts the data to this format.
</p>



<h5>Parallel processing</h5>

<p>By default, the model is trained without parallel processing. This can
be change by passing the <code>nthread</code> parameter to
<code>set_engine()</code>. However, it is unwise to combine this
with external parallel processing when using the package.
</p>



<h5>Interpreting <code>mtry</code>
</h5>

<p>The <code>mtry</code> argument denotes the number of predictors that will be
randomly sampled at each split when creating tree models.
</p>
<p>Some engines, such as <code>"xgboost"</code>, <code>"xrf"</code>, and <code>"lightgbm"</code>, interpret
their analogue to the <code>mtry</code> argument as the <em>proportion</em> of predictors
that will be randomly sampled at each split rather than the <em>count</em>. In
some settings, such as when tuning over preprocessors that influence the
number of predictors, this parameterization is quite
helpful—interpreting <code>mtry</code> as a proportion means that <code style="white-space: pre;">⁠[0, 1]⁠</code> is
always a valid range for that parameter, regardless of input data.
</p>
<p>parsnip and its extensions accommodate this parameterization using the
<code>counts</code> argument: a logical indicating whether <code>mtry</code> should be
interpreted as the number of predictors that will be randomly sampled at
each split. <code>TRUE</code> indicates that <code>mtry</code> will be interpreted in its
sense as a count, <code>FALSE</code> indicates that the argument will be
interpreted in its sense as a proportion.
</p>
<p><code>mtry</code> is a main model argument for
<code>boost_tree()</code> and
<code>rand_forest()</code>, and thus should not have an
engine-specific interface. So, regardless of engine, <code>counts</code> defaults
to <code>TRUE</code>. For engines that support the proportion interpretation
(currently <code>"xgboost"</code> and <code>"xrf"</code>, via the rules package, and
<code>"lightgbm"</code> via the bonsai package) the user can pass the
<code>counts = FALSE</code> argument to <code>set_engine()</code> to supply <code>mtry</code> values
within <code style="white-space: pre;">⁠[0, 1]⁠</code>.
</p>



<h5>Early stopping</h5>

<p>The <code>stop_iter()</code> argument allows the model to prematurely stop training
if the objective function does not improve within <code>early_stop</code>
iterations.
</p>
<p>The best way to use this feature is in conjunction with an <em>internal
validation set</em>. To do this, pass the <code>validation</code> parameter of
<code>xgb_train()</code> via the parsnip
<code>set_engine()</code> function. This is the
proportion of the training set that should be reserved for measuring
performance (and stopping early).
</p>
<p>If the model specification has <code>early_stop &gt;= trees</code>, <code>early_stop</code> is
converted to <code>trees - 1</code> and a warning is issued.
</p>
<p>Note that, since the <code>validation</code> argument provides an alternative
interface to <code>watchlist</code>, the <code>watchlist</code> argument is guarded by parsnip
and will be ignored (with a warning) if passed.
</p>



<h5>Objective function</h5>

<p>parsnip chooses the objective function based on the characteristics of
the outcome. To use a different loss, pass the <code>objective</code> argument to
<code>set_engine()</code> directly.
</p>




<h4>Saving fitted model objects</h4>

<p>This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the <a href="https://butcher.tidymodels.org">butcher</a> package.
</p>
<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>



<h4>Examples</h4>

<p>The “Fitting and Predicting with parsnip” article contains
<a href="https://parsnip.tidymodels.org/articles/articles/Examples.html#boost-tree-xgboost">examples</a>
for <code>boost_tree()</code> with the <code>"xgboost"</code> engine.
</p>



<h4>References</h4>


<ul>
<li> <p><a href="https://arxiv.org/abs/1603.02754">XGBoost: A Scalable Tree Boosting System</a>
</p>
</li>
<li>
<p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li>
</ul>
</div>