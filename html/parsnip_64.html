<div class="container">

<table style="width: 100%;"><tr>
<td>details_boost_tree_lightgbm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Boosted trees via lightgbm</h2>

<h3>Description</h3>

<p><code>lightgbm::lgb.train()</code> creates a series of decision trees
forming an ensemble. Each tree depends on the results of previous trees.
All trees in the ensemble are combined to produce a final prediction.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: regression and classification
</p>


<h4>Tuning Parameters</h4>

<p>This model has 6 tuning parameters:
</p>

<ul>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: -1)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 100)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.1)
</p>
</li>
<li> <p><code>mtry</code>: # Randomly Selected Predictors (type: integer, default: see
below)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 20)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0)
</p>
</li>
</ul>
<p>The <code>mtry</code> parameter gives the <em>number</em> of predictors that will be
randomly sampled at each split. The default is to use all predictors.
</p>
<p>Rather than as a number,
<code>lightgbm::lgb.train()</code>’s <code>feature_fraction</code>
argument encodes <code>mtry</code> as the <em>proportion</em> of predictors that will be
randomly sampled at each split. parsnip translates <code>mtry</code>, supplied as
the <em>number</em> of predictors, to a proportion under the hood. That is, the
user should still supply the argument as <code>mtry</code> to <code>boost_tree()</code>, and
do so in its sense as a number rather than a proportion; before passing
<code>mtry</code> to <code>lightgbm::lgb.train()</code>, parsnip will
convert the <code>mtry</code> value to a proportion.
</p>
<p>Note that parsnip’s translation can be overridden via the <code>counts</code>
argument, supplied to <code>set_engine()</code>. By default, <code>counts</code> is set to
<code>TRUE</code>, but supplying the argument <code>counts = FALSE</code> allows the user to
supply <code>mtry</code> as a proportion rather than a number.
</p>



<h4>Translation from parsnip to the original package (regression)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), tree_depth = integer(), 
  learn_rate = numeric(), min_n = integer(), loss_reduction = numeric()
) %&gt;%
  set_engine("lightgbm") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
## 
## Computational engine: lightgbm 
## 
## Model fit template:
## bonsai::train_lightgbm(x = missing_arg(), y = missing_arg(), 
##     feature_fraction_bynode = integer(), num_iterations = integer(), 
##     min_data_in_leaf = integer(), max_depth = integer(), learning_rate = numeric(), 
##     min_gain_to_split = numeric(), verbose = -1, num_threads = 0, 
##     seed = sample.int(10^5, 1), deterministic = TRUE)
</pre></div>



<h4>Translation from parsnip to the original package (classification)</h4>

<p>The <strong>bonsai</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>boost_tree(
  mtry = integer(), trees = integer(), tree_depth = integer(), 
  learn_rate = numeric(), min_n = integer(), loss_reduction = numeric()
) %&gt;% 
  set_engine("lightgbm") %&gt;% 
  set_mode("classification") %&gt;% 
  translate()
</pre></div>
<div class="sourceCode"><pre>## Boosted Tree Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   loss_reduction = numeric()
## 
## Computational engine: lightgbm 
## 
## Model fit template:
## bonsai::train_lightgbm(x = missing_arg(), y = missing_arg(), 
##     feature_fraction_bynode = integer(), num_iterations = integer(), 
##     min_data_in_leaf = integer(), max_depth = integer(), learning_rate = numeric(), 
##     min_gain_to_split = numeric(), verbose = -1, num_threads = 0, 
##     seed = sample.int(10^5, 1), deterministic = TRUE)
</pre></div>
<p><code>bonsai::train_lightgbm()</code> is a wrapper
around <code>lightgbm::lgb.train()</code> (and other
functions) that make it easier to run this model.
</p>



<h4>Other details</h4>



<h5>Preprocessing</h5>

<p>This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. <code style="white-space: pre;">⁠{a, c}⁠</code> vs <code style="white-space: pre;">⁠{b, d}⁠</code>) when splitting at a node. Dummy variables
are not required for this model.
</p>
<p>Non-numeric predictors (i.e., factors) are internally converted to
numeric. In the classification context, non-numeric outcomes (i.e.,
factors) are also internally converted to numeric.
</p>



<h5>Interpreting <code>mtry</code>
</h5>

<p>The <code>mtry</code> argument denotes the number of predictors that will be
randomly sampled at each split when creating tree models.
</p>
<p>Some engines, such as <code>"xgboost"</code>, <code>"xrf"</code>, and <code>"lightgbm"</code>, interpret
their analogue to the <code>mtry</code> argument as the <em>proportion</em> of predictors
that will be randomly sampled at each split rather than the <em>count</em>. In
some settings, such as when tuning over preprocessors that influence the
number of predictors, this parameterization is quite
helpful—interpreting <code>mtry</code> as a proportion means that <code style="white-space: pre;">⁠[0, 1]⁠</code> is
always a valid range for that parameter, regardless of input data.
</p>
<p>parsnip and its extensions accommodate this parameterization using the
<code>counts</code> argument: a logical indicating whether <code>mtry</code> should be
interpreted as the number of predictors that will be randomly sampled at
each split. <code>TRUE</code> indicates that <code>mtry</code> will be interpreted in its
sense as a count, <code>FALSE</code> indicates that the argument will be
interpreted in its sense as a proportion.
</p>
<p><code>mtry</code> is a main model argument for
<code>boost_tree()</code> and
<code>rand_forest()</code>, and thus should not have an
engine-specific interface. So, regardless of engine, <code>counts</code> defaults
to <code>TRUE</code>. For engines that support the proportion interpretation
(currently <code>"xgboost"</code> and <code>"xrf"</code>, via the rules package, and
<code>"lightgbm"</code> via the bonsai package) the user can pass the
<code>counts = FALSE</code> argument to <code>set_engine()</code> to supply <code>mtry</code> values
within <code style="white-space: pre;">⁠[0, 1]⁠</code>.
</p>



<h5>Saving fitted model objects</h5>

<p>Models fitted with this engine may require native serialization methods
to be properly saved and/or passed between R sessions. To learn more
about preparing fitted models for serialization, see the bundle package.
</p>



<h5>Bagging</h5>

<p>The <code>sample_size</code> argument is translated to the <code>bagging_fraction</code>
parameter in the <code>param</code> argument of <code>lgb.train</code>. The argument is
interpreted by lightgbm as a <em>proportion</em> rather than a count, so bonsai
internally reparameterizes the <code>sample_size</code> argument with
<code>dials::sample_prop()</code> during tuning.
</p>
<p>To effectively enable bagging, the user would also need to set the
<code>bagging_freq</code> argument to lightgbm. <code>bagging_freq</code> defaults to 0, which
means bagging is disabled, and a <code>bagging_freq</code> argument of <code>k</code> means
that the booster will perform bagging at every <code>k</code>th boosting iteration.
Thus, by default, the <code>sample_size</code> argument would be ignored without
setting this argument manually. Other boosting libraries, like xgboost,
do not have an analogous argument to <code>bagging_freq</code> and use <code>k = 1</code> when
the analogue to <code>bagging_fraction</code> is in $(0, 1)$. <em>bonsai will thus
automatically set</em> <code>bagging_freq = 1</code> <em>in</em> <code>set_engine("lightgbm", ...)</code>
if <code>sample_size</code> (i.e. <code>bagging_fraction</code>) is not equal to 1 and no
<code>bagging_freq</code> value is supplied. This default can be overridden by
setting the <code>bagging_freq</code> argument to <code>set_engine()</code> manually.
</p>



<h5>Verbosity</h5>

<p>bonsai quiets much of the logging output from
<code>lightgbm::lgb.train()</code> by default. With
default settings, logged warnings and errors will still be passed on to
the user. To print out all logs during training, set <code>quiet = TRUE</code>.
</p>




<h4>Examples</h4>

<p>The “Introduction to bonsai” article contains
<a href="https://bonsai.tidymodels.org/articles/bonsai.html">examples</a> of
<code>boost_tree()</code> with the <code>"lightgbm"</code> engine.
</p>



<h4>References</h4>


<ul>
<li> <p><a href="https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a>
</p>
</li>
<li>
<p> Kuhn, M, and K Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer.
</p>
</li>
</ul>
</div>