<div class="container">

<table style="width: 100%;"><tr>
<td>MDP_policy_functions</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Functions for MDP Policies</h2>

<h3>Description</h3>

<p>Implementation several functions useful to deal with MDP policies.
</p>


<h3>Usage</h3>

<pre><code class="language-R">q_values_MDP(model, U = NULL)

MDP_policy_evaluation(
  pi,
  model,
  U = NULL,
  k_backups = 1000,
  theta = 0.001,
  verbose = FALSE
)

greedy_MDP_action(s, Q, epsilon = 0, prob = FALSE)

random_MDP_policy(model, prob = NULL)

manual_MDP_policy(model, actions)

greedy_MDP_policy(Q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>an MDP problem specification.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>a vector with value function representing the state utilities
(expected sum of discounted rewards from that point on).
If <code>model</code> is a solved model, then the state
utilities are taken from the solution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pi</code></td>
<td>
<p>a policy as a data.frame with at least columns for states and action.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k_backups</code></td>
<td>
<p>number of look ahead steps used for approximate policy evaluation
used by the policy iteration method. Set k_backups to <code>Inf</code> to only use
<code class="reqn">\theta</code> as the stopping criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>stop when the largest change in a state value is less
than <code class="reqn">\theta</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical; should progress and approximation errors be printed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p>a state.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Q</code></td>
<td>
<p>an action value function with Q-values as a state by action matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>an <code>epsilon &gt; 0</code> applies an epsilon-greedy policy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prob</code></td>
<td>
<p>probability vector for random actions for <code>random_MDP_policy()</code>.
a logical indicating if action probabilities should be returned for
<code>greedy_MDP_action()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>actions</code></td>
<td>
<p>a vector with the action (either the action label or the
numeric id) for each state.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Implemented functions are:
</p>

<ul>
<li> <p><code>q_values_MDP()</code> calculates (approximates)
Q-values for a given model using the Bellman
optimality equation:
</p>
<p style="text-align: center;"><code class="reqn">q(s,a) = \sum_{s'} T(s'|s,a) [R(s,a) + \gamma U(s')]</code>
</p>

<p>Q-values can be used as the input for several other functions.
</p>
</li>
<li> <p><code>MDP_policy_evaluation()</code> evaluates a policy <code class="reqn">\pi</code> for a model and returns
(approximate) state values by applying the Bellman equation as an update
rule for each state and iteration <code class="reqn">k</code>:
</p>
<p style="text-align: center;"><code class="reqn">U_{k+1}(s) =\sum_a \pi{a|s} \sum_{s'} T(s' | s,a) [R(s,a) + \gamma U_k(s')]</code>
</p>

<p>In each iteration, all states are updated. Updating is stopped after
<code>k_backups</code> iterations or after the
largest update <code class="reqn">||U_{k+1} - U_k||_\infty &lt; \theta</code>.
</p>
</li>
<li> <p><code>greedy_MDP_action()</code> returns the action with the largest Q-value given a
state.
</p>
</li>
<li> <p><code>random_MDP_policy()</code>, <code>manual_MDP_policy()</code>, and <code>greedy_MDP_policy()</code>
generates different policies. These policies can be added to a problem
using <code>add_policy()</code>.
</p>
</li>
</ul>
<h3>Value</h3>

<p><code>q_values_MDP()</code> returns a state by action matrix specifying the Q-function,
i.e., the action value for executing each action in each state. The Q-values
are calculated from the value function (U) and the transition model.
</p>
<p><code>MDP_policy_evaluation()</code> returns a vector with (approximate)
state values (U).
</p>
<p><code>greedy_MDP_action()</code> returns the action with the highest q-value
for state <code>s</code>. If <code>prob = TRUE</code>, then a vector with
the probability for each action is returned.
</p>
<p><code>random_MDP_policy()</code> returns a data.frame with the columns state and action to define a policy.
</p>
<p><code>manual_MDP_policy()</code> returns a data.frame with the columns state and action to define a policy.
</p>
<p><code>greedy_MDP_policy()</code> returns the greedy policy given <code>Q</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
</p>


<h3>See Also</h3>

<p>Other MDP: 
<code>MDP()</code>,
<code>MDP2POMDP</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>add_policy()</code>,
<code>gridworld</code>,
<code>reachable_and_absorbing</code>,
<code>regret()</code>,
<code>simulate_MDP()</code>,
<code>solve_MDP()</code>,
<code>transition_graph()</code>,
<code>value_function()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(Maze)
Maze

# create several policies:
# 1. optimal policy using value iteration
maze_solved &lt;- solve_MDP(Maze, method = "value_iteration")
maze_solved
pi_opt &lt;- policy(maze_solved)
pi_opt
gridworld_plot_policy(add_policy(Maze, pi_opt), main = "Optimal Policy")

# 2. a manual policy (go up and in some squares to the right)
acts &lt;- rep("up", times = length(Maze$states))
names(acts) &lt;- Maze$states
acts[c("s(1,1)", "s(1,2)", "s(1,3)")] &lt;- "right"
pi_manual &lt;- manual_MDP_policy(Maze, acts)
pi_manual
gridworld_plot_policy(add_policy(Maze, pi_manual), main = "Manual Policy")

# 3. a random policy
set.seed(1234)
pi_random &lt;- random_MDP_policy(Maze)
pi_random
gridworld_plot_policy(add_policy(Maze, pi_random), main = "Random Policy")

# 4. an improved policy based on one policy evaluation and
#   policy improvement step.
u &lt;- MDP_policy_evaluation(pi_random, Maze)
q &lt;- q_values_MDP(Maze, U = u)
pi_greedy &lt;- greedy_MDP_policy(q)
pi_greedy
gridworld_plot_policy(add_policy(Maze, pi_greedy), main = "Greedy Policy")

#' compare the approx. value functions for the policies (we restrict
#'    the number of backups for the random policy since it may not converge)
rbind(
  random = MDP_policy_evaluation(pi_random, Maze, k_backups = 100),
  manual = MDP_policy_evaluation(pi_manual, Maze),
  greedy = MDP_policy_evaluation(pi_greedy, Maze),
  optimal = MDP_policy_evaluation(pi_opt, Maze)
)

# For many functions, we first add the policy to the problem description
#   to create a "solved" MDP
maze_random &lt;- add_policy(Maze, pi_random)
maze_random

# plotting
plot_value_function(maze_random)
gridworld_plot_policy(maze_random)

# compare to a benchmark
regret(maze_random, benchmark = maze_solved)

# calculate greedy actions for state 1
q &lt;- q_values_MDP(maze_random)
q
greedy_MDP_action(1, q, epsilon = 0, prob = FALSE)
greedy_MDP_action(1, q, epsilon = 0, prob = TRUE)
greedy_MDP_action(1, q, epsilon = .1, prob = TRUE)
</code></pre>


</div>