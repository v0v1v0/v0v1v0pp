<div class="container">

<table style="width: 100%;"><tr>
<td>bayesOpt</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bayesian Optimization with Gaussian Processes</h2>

<h3>Description</h3>

<p>Maximizes a user defined function within a set of bounds. After the function
is sampled a pre-determined number of times, a Gaussian process is fit to
the results. An acquisition function is then maximized to determine the most
likely location of the global maximum of the user defined function.  This
process is repeated for a set number of iterations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bayesOpt(
  FUN,
  bounds,
  saveFile = NULL,
  initGrid,
  initPoints = 4,
  iters.n = 3,
  iters.k = 1,
  otherHalting = list(timeLimit = Inf, minUtility = 0),
  acq = "ucb",
  kappa = 2.576,
  eps = 0,
  parallel = FALSE,
  gsPoints = pmax(100, length(bounds)^3),
  convThresh = 1e+08,
  acqThresh = 1,
  errorHandling = "stop",
  plotProgress = FALSE,
  verbose = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>FUN</code></td>
<td>
<p>the function to be maximized. This function should return a
named list with at least 1 component. The first component must be named
<code>Score</code> and should contain the metric to be maximized. You may
return other named scalar elements that you wish to include in the final
summary table.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bounds</code></td>
<td>
<p>named list of lower and upper bounds for each <code>FUN</code> input.
The names of the list should be arguments passed to <code>FUN</code>.
Use "L" suffix to indicate integers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>saveFile</code></td>
<td>
<p>character filepath (including file name and extension, .RDS) that
specifies the location to save results as they are obtained. A <code>bayesOpt</code>
object is saved to the file after each epoch.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initGrid</code></td>
<td>
<p>user specified points to sample the scoring function, should
be a <code>data.frame</code> or <code>data.table</code> with identical column names as bounds.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initPoints</code></td>
<td>
<p>Number of points to initialize the process with. Points are
chosen with latin hypercube sampling within the bounds supplied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iters.n</code></td>
<td>
<p>The total number of times FUN will be run after initialization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iters.k</code></td>
<td>
<p>integer that specifies the number of times to sample FUN
at each Epoch (optimization step). If running in parallel, good practice
is to set <code>iters.k</code> to some multiple of the number of cores you have designated
for this process. Must be lower than, and preferrably some multiple of <code>iters.n</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>otherHalting</code></td>
<td>
<p>A list of other halting specifications. The process will stop if any of
the following is true. These checks are only performed in between optimization steps:
</p>

<ul>
<li>
<p> The elapsed seconds is greater than the list element <code>timeLimit</code>.
</p>
</li>
<li>
<p> The utility expected from the Gaussian process is less than the list element
<code>minUtility</code>.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>acq</code></td>
<td>
<p>acquisition function type to be used. Can be "ucb", "ei", "eips" or "poi".
</p>

<ul>
<li> <p><code>ucb</code>   Upper Confidence Bound
</p>
</li>
<li> <p><code>ei</code>    Expected Improvement
</p>
</li>
<li> <p><code>eips</code>  Expected Improvement Per Second
</p>
</li>
<li> <p><code>poi</code>   Probability of Improvement
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kappa</code></td>
<td>
<p>tunable parameter kappa of the upper confidence bound.
Adjusts exploitation/exploration. Increasing kappa will increase the
importance that uncertainty (unexplored space) has, therefore incentivising
exploration. This number represents the standard deviations above 0 of your upper
confidence bound. Default is 2.56, which corresponds to the ~99th percentile.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>tunable parameter epsilon of ei, eips and poi. Adjusts exploitation/exploration.
This value is added to y_max after the scaling, so should between -0.1 and 0.1.
Increasing eps will make the "improvement" threshold for new points higher, therefore
incentivising exploitation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>should the process run in parallel? If TRUE, several criteria must be met:
</p>

<ul>
<li>
<p> A parallel backend must be registered
</p>
</li>
<li>
<p> Objects required by <code>FUN</code> must be loaded into each cluster.
</p>
</li>
<li>
<p> Packages required by <code>FUN</code> must be loaded into each cluster. See vignettes.
</p>
</li>
<li> <p><code>FUN</code> must be thread safe.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gsPoints</code></td>
<td>
<p>integer that specifies how many initial points to try when
searching for the optimum of the acquisition function. Increase this for a higher
chance to find global optimum, at the expense of more time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>convThresh</code></td>
<td>
<p>convergence threshold passed to <code>factr</code> when the
<code>optim</code> function (L-BFGS-B) is called. Lower values will take longer
to converge, but may be more accurate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>acqThresh</code></td>
<td>
<p>number 0-1. Represents the minimum percentage
of the global optimal utility required for a local optimum to
be included as a candidate parameter set in the next scoring function.
If 1.0, only the global optimum will be used as a candidate
parameter set. If 0.5, only local optimums with 50 percent of the utility
of the global optimum will be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>errorHandling</code></td>
<td>
<p>If FUN returns an error, how to proceed. All errors are
stored in <code>scoreSummary</code>. Can be one of 3 options: "stop" stops the
function running and returns results. "continue" keeps the process running.
Passing an integer will allow the process to continue until that many errors
have occured, after which the results will be returned.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plotProgress</code></td>
<td>
<p>Should the progress of the Bayesian optimization be
printed? Top graph shows the score(s) obtained at each iteration.
The bottom graph shows the estimated utility of each point.
This is useful to display how much utility the Gaussian Process is
assuming still exists. If your utility is approaching 0, then you
can be confident you are close to an optimal parameter set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Whether or not to print progress to the console.
If 0, nothing will be printed. If 1, progress will be printed.
If 2, progress and information about new parameter-score pairs will be printed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other parameters passed to <code>DiceKriging::km()</code>. All FUN inputs and scores
are scaled from 0-1 before being passed to km. FUN inputs are scaled within <code>bounds</code>,
and scores are scaled by 0 = min(scores), 1 = max(scores).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object of class <code>bayesOpt</code> containing information about the process.
</p>

<ul>
<li> <p><code>FUN</code>          The scoring function.
</p>
</li>
<li> <p><code>bounds</code>       The bounds originally supplied.
</p>
</li>
<li> <p><code>iters</code>        The total iterations that have been run.
</p>
</li>
<li> <p><code>initPars</code>     The initialization parameters.
</p>
</li>
<li> <p><code>optPars</code>      The optimization parameters.
</p>
</li>
<li> <p><code>GauProList</code>   A list containing information on the Gaussian Processes used in optimization.
</p>
</li>
<li> <p><code>scoreSummary</code> A <code>data.table</code> with results from the execution of <code>FUN</code>
at different inputs. Includes information on the epoch, iteration, function inputs, score, and any other
information returned by <code>FUN</code>.
</p>
</li>
<li> <p><code>stopStatus</code>   Information on what caused the function to stop running. Possible explenations are
time limit, minimum utility not met, errors in <code>FUN</code>, iters.n was reached, or the Gaussian Process encountered
an error.
</p>
</li>
<li> <p><code>elapsedTime</code> The total time in seconds the function was executing.
</p>
</li>
</ul>
<h3>Vignettes</h3>

<p>It is highly recommended to read the <a href="https://github.com/AnotherSamWilson/ParBayesianOptimization">GitHub</a> for examples.
There are also several vignettes available from the official <a href="https://CRAN.R-project.org/package=ParBayesianOptimization">CRAN Listing</a>.
</p>


<h3>References</h3>

<p>Jasper Snoek, Hugo Larochelle, Ryan P. Adams (2012) <em>Practical Bayesian Optimization of Machine Learning Algorithms</em>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Example 1 - Optimization of a continuous single parameter function
scoringFunction &lt;- function(x) {
  a &lt;- exp(-(2-x)^2)*1.5
  b &lt;- exp(-(4-x)^2)*2
  c &lt;- exp(-(6-x)^2)*1
  return(list(Score = a+b+c))
}

bounds &lt;- list(x = c(0,8))

Results &lt;- bayesOpt(
    FUN = scoringFunction
  , bounds = bounds
  , initPoints = 3
  , iters.n = 2
  , gsPoints = 10
)

## Not run: 
# Example 2 - Hyperparameter Tuning in xgboost
if (requireNamespace('xgboost', quietly = TRUE)) {
  library("xgboost")

  data(agaricus.train, package = "xgboost")

  Folds &lt;- list(
      Fold1 = as.integer(seq(1,nrow(agaricus.train$data),by = 3))
    , Fold2 = as.integer(seq(2,nrow(agaricus.train$data),by = 3))
    , Fold3 = as.integer(seq(3,nrow(agaricus.train$data),by = 3))
  )

  scoringFunction &lt;- function(max_depth, min_child_weight, subsample) {

    dtrain &lt;- xgb.DMatrix(agaricus.train$data,label = agaricus.train$label)

    Pars &lt;- list(
        booster = "gbtree"
      , eta = 0.01
      , max_depth = max_depth
      , min_child_weight = min_child_weight
      , subsample = subsample
      , objective = "binary:logistic"
      , eval_metric = "auc"
    )

    xgbcv &lt;- xgb.cv(
         params = Pars
       , data = dtrain
       , nround = 100
       , folds = Folds
       , prediction = TRUE
       , showsd = TRUE
       , early_stopping_rounds = 5
       , maximize = TRUE
       , verbose = 0
    )

    return(
      list(
          Score = max(xgbcv$evaluation_log$test_auc_mean)
        , nrounds = xgbcv$best_iteration
      )
    )
  }

  bounds &lt;- list(
      max_depth = c(2L, 10L)
    , min_child_weight = c(1, 100)
    , subsample = c(0.25, 1)
  )

  ScoreResult &lt;- bayesOpt(
      FUN = scoringFunction
    , bounds = bounds
    , initPoints = 3
    , iters.n = 2
    , iters.k = 1
    , acq = "ei"
    , gsPoints = 10
    , parallel = FALSE
    , verbose = 1
  )
}

## End(Not run)
</code></pre>


</div>