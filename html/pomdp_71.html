<div class="container">

<table style="width: 100%;"><tr>
<td>solve_MDP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Solve an MDP Problem</h2>

<h3>Description</h3>

<p>Implementation of value iteration, modified policy iteration and other
methods based on reinforcement learning techniques to solve finite
state space MDPs.
</p>


<h3>Usage</h3>

<pre><code class="language-R">solve_MDP(model, method = "value", ...)

solve_MDP_DP(
  model,
  method = "value_iteration",
  horizon = NULL,
  discount = NULL,
  N_max = 1000,
  error = 0.01,
  k_backups = 10,
  U = NULL,
  verbose = FALSE
)

solve_MDP_TD(
  model,
  method = "q_learning",
  horizon = NULL,
  discount = NULL,
  alpha = 0.5,
  epsilon = 0.1,
  N = 100,
  U = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>an MDP problem specification.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>string; one of the following solution methods: <code>'value_iteration'</code>,
<code>'policy_iteration'</code>, <code>'q_learning'</code>, <code>'sarsa'</code>, or <code>'expected_sarsa'</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further parameters are passed on to the solver function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>horizon</code></td>
<td>
<p>an integer with the number of epochs for problems with a
finite planning horizon. If set to <code>Inf</code>, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
<code>NULL</code>, then the horizon specified in <code>model</code> will be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>discount</code></td>
<td>
<p>discount factor in range <code class="reqn">(0, 1]</code>. If <code>NULL</code>, then the
discount factor specified in <code>model</code> will be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N_max</code></td>
<td>
<p>maximum number of iterations allowed to converge. If the
maximum is reached then the non-converged solution is returned with a
warning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>error</code></td>
<td>
<p>value iteration: maximum error allowed in the utility of any state
(i.e., the maximum policy loss) used as the termination criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k_backups</code></td>
<td>
<p>policy iteration: number of look ahead steps used for approximate policy evaluation
used by the policy iteration method.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>a vector with initial utilities used for each state. If
<code>NULL</code>, then the default of a vector of all 0s is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, the function provides the
output of the solver in the R console.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>step size in <code style="white-space: pre;">⁠(0, 1]⁠</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>used for <code class="reqn">\epsilon</code>-greedy policies.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N</code></td>
<td>
<p>number of episodes used for learning.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Implemented are the following dynamic programming methods (following
Russell and Norvig, 2010):
</p>

<ul>
<li> <p><strong>Modified Policy Iteration</strong>
starts with a random policy and iteratively performs
a sequence of
</p>

<ol>
<li>
<p> approximate policy evaluation (estimate the value function for the
current policy using <code>k_backups</code> and function <code>MDP_policy_evaluation()</code>), and
</p>
</li>
<li>
<p> policy improvement (calculate a greedy policy given the value function).
The algorithm stops when it converges to a stable policy (i.e., no changes
between two iterations).
</p>
</li>
</ol>
</li>
<li> <p><strong>Value Iteration</strong> starts with
an arbitrary value function (by default all 0s) and iteratively
updates the value function for each state using the Bellman equation.
The iterations
are terminated either after <code>N_max</code> iterations or when the solution converges.
Approximate convergence is achieved
for discounted problems (with <code class="reqn">\gamma &lt; 1</code>)
when the maximal value function change for any state <code class="reqn">\delta</code> is
<code class="reqn">\delta \le error (1-\gamma) / \gamma</code>. It can be shown that this means
that no state value is more than
<code class="reqn">error</code> from the value in the optimal value function. For undiscounted
problems, we use <code class="reqn">\delta \le error</code>.
</p>
<p>The greedy policy
is calculated from the final value function. Value iteration can be seen as
policy iteration with truncated policy evaluation.
</p>
</li>
</ul>
<p>Note that the policy converges earlier than the value function.
</p>
<p>Implemented are the following temporal difference control methods
described in Sutton and Barto (2020).
Note that the MDP transition and reward models are only used to simulate
the environment for these reinforcement learning methods.
The algorithms use a step size parameter <code class="reqn">\alpha</code> (learning rate) for the
updates and the exploration parameter <code class="reqn">\epsilon</code> for
the <code class="reqn">\epsilon</code>-greedy policy.
</p>
<p>If the model has absorbing states to terminate episodes, then no maximal episode length
(<code>horizon</code>) needs to
be specified. To make sure that the algorithm does finish in a reasonable amount of time,
episodes are stopped after 10,000 actions with a warning. For models without absorbing states,
a episode length has to be specified via <code>horizon</code>.
</p>

<ul>
<li> <p><strong>Q-Learning</strong> is an off-policy temporal difference method that uses
an <code class="reqn">\epsilon</code>-greedy behavior policy and learns a greedy target
policy.
</p>
</li>
<li> <p><strong>Sarsa</strong> is an on-policy method that follows and learns
an <code class="reqn">\epsilon</code>-greedy policy. The final <code class="reqn">\epsilon</code>-greedy policy
is converted into a greedy policy.
</p>
</li>
<li> <p><strong>Expected Sarsa</strong>: We implement an on-policy version that uses
the expected value under the current policy for the update.
It moves deterministically in the same direction as Sarsa
moves in expectation. Because it uses the expectation, we can
set the step size <code class="reqn">\alpha</code> to large values and even 1.
</p>
</li>
</ul>
<h3>Value</h3>

<p><code>solve_MDP()</code> returns an object of class POMDP which is a list with the
model specifications (<code>model</code>), the solution (<code>solution</code>).
The solution is a list with the elements:
</p>

<ul>
<li> <p><code>policy</code> a list representing the policy graph. The list only has one element for converged solutions.
</p>
</li>
<li> <p><code>converged</code> did the algorithm converge (<code>NA</code>) for finite-horizon problems.
</p>
</li>
<li> <p><code>delta</code> final <code class="reqn">\delta</code> (value iteration and infinite-horizon only)
</p>
</li>
<li> <p><code>iterations</code> number of iterations to convergence (infinite-horizon only)
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Russell, S., Norvig, P. (2021). Artificial Intelligence: A Modern Approach.
Fourth edition. Prentice Hall.
</p>
<p>Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
</p>


<h3>See Also</h3>

<p>Other solver: 
<code>solve_POMDP()</code>,
<code>solve_SARSOP()</code>
</p>
<p>Other MDP: 
<code>MDP()</code>,
<code>MDP2POMDP</code>,
<code>MDP_policy_functions</code>,
<code>accessors</code>,
<code>actions()</code>,
<code>add_policy()</code>,
<code>gridworld</code>,
<code>reachable_and_absorbing</code>,
<code>regret()</code>,
<code>simulate_MDP()</code>,
<code>transition_graph()</code>,
<code>value_function()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(Maze)
Maze

# use value iteration
maze_solved &lt;- solve_MDP(Maze, method = "value_iteration")
maze_solved
policy(maze_solved)

# plot the value function U
plot_value_function(maze_solved)

# Maze solutions can be visualized
gridworld_plot_policy(maze_solved)

# use modified policy iteration
maze_solved &lt;- solve_MDP(Maze, method = "policy_iteration")
policy(maze_solved)

# finite horizon
maze_solved &lt;- solve_MDP(Maze, method = "value_iteration", horizon = 3)
policy(maze_solved)
gridworld_plot_policy(maze_solved, epoch = 1)
gridworld_plot_policy(maze_solved, epoch = 2)
gridworld_plot_policy(maze_solved, epoch = 3)

# create a random policy where action n is very likely and approximate
#  the value function. We change the discount factor to .9 for this.
Maze_discounted &lt;- Maze
Maze_discounted$discount &lt;- .9
pi &lt;- random_MDP_policy(Maze_discounted, 
        prob = c(n = .7, e = .1, s = .1, w = 0.1))
pi

# compare the utility function for the random policy with the function for the optimal
#  policy found by the solver.
maze_solved &lt;- solve_MDP(Maze)

MDP_policy_evaluation(pi, Maze, k_backup = 100)
MDP_policy_evaluation(policy(maze_solved), Maze, k_backup = 100)

# Note that the solver already calculates the utility function and returns it with the policy
policy(maze_solved)

# Learn a Policy using Q-Learning
maze_learned &lt;- solve_MDP(Maze, method = "q_learning", N = 100)
maze_learned

maze_learned$solution
policy(maze_learned)
plot_value_function(maze_learned)
gridworld_plot_policy(maze_learned)
</code></pre>


</div>