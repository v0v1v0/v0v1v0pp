<div class="container">

<table style="width: 100%;"><tr>
<td>MI</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Shannon's Mutual Information <code class="reqn">I(X,Y)</code>
</h2>

<h3>Description</h3>

<p>Compute Shannon's Mutual Information based on the identity <code class="reqn">I(X,Y) =
H(X) + H(Y) - H(X,Y)</code> based on a given joint-probability vector <code class="reqn">P(X,Y)</code>
and probability vectors <code class="reqn">P(X)</code> and <code class="reqn">P(Y)</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">MI(x, y, xy, unit = "log2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric probability vector <code class="reqn">P(X)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a numeric probability vector <code class="reqn">P(Y)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xy</code></td>
<td>
<p>a numeric joint-probability vector <code class="reqn">P(X,Y)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function might be useful to fastly compute Shannon's Mutual Information
for any given joint-probability vector and probability vectors.
</p>


<h3>Value</h3>

<p>Shannon's Mutual Information in bit.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Shannon, Claude E. 1948. "A Mathematical Theory of
Communication". <em>Bell System Technical Journal</em> <b>27</b> (3): 379-423.
</p>


<h3>See Also</h3>

<p><code>H</code>, <code>JE</code>, <code>CE</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
MI( x = 1:10/sum(1:10), y = 20:29/sum(20:29), xy = 1:10/sum(1:10) )

</code></pre>


</div>