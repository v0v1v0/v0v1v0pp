<div class="container">

<table style="width: 100%;"><tr>
<td>ProDenICA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Product Density Independent Component Analysis</h2>

<h3>Description</h3>

<p>Fits an ICA model by directly estimating the densities of the
independent components using Poisson GAMs. The densities have the form
of tilted Gaussians, and hense directly estimate the contrast functions
that lead to negentropy measures. This function supports Section 14.7.4
of 'Elements of Statistical Learning (Hastie, Tibshirani and Friedman,
2009, 2nd Edition)'. Models include 'FastICA'. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">ProDenICA(x, k = p, W0 = NULL, whiten = FALSE, maxit = 20, thresh = 1e-07,
restarts = 0, trace = FALSE, Gfunc = GPois, eps.rank = 1e-07, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>input matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>Number of components required, less than or equal to the
number of columns of x</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W0</code></td>
<td>
<p>Optional initial matrix (for comparing algorithms)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>whiten</code></td>
<td>
<p>Logical variable - should x be whitened. If TRUE, the SVD of X=UDV' is computed, and U is used (up to
rank(X) columns). Also k is reduced to min(k,rank(X)). If FALSE (default), it is
assumed that the user has pre-whitened x (and if not, the function may
not perform properly)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>Maximum number of iterations; default is 20</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thresh</code></td>
<td>
<p>Convergence threshold, in terms of relative change in
Amari metric; dfault is 1e-7</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restarts</code></td>
<td>
<p>Number of random restarts; default is 0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>Trace iterations; default is FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Gfunc</code></td>
<td>
<p>Contrast functional which is basis for negentropy
measure. Default is <code>'GPois'</code> which fits a tilted Gaussian density using
a Poisson GAM. Other options are <code>'G1'</code> (cosh negentropy) and
<code>'G0'</code> (kurtosis negentropy)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps.rank</code></td>
<td>
<p>Threshold for deciding rank of x if option
<code>whiten=TRUE</code>. Any singular value less than <code>eps.thresh</code>
smaller than the largest is treated as zero</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments for <code>Gfunc</code> areguments</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>See Section 14.7.4
of <em>Elements of Statistical Learning</em> (Hastie, Tibshirani and Friedman,
2009, 2nd Edition)</p>


<h3>Value</h3>

<p>An object of S3 class <code>"ProDenICA"</code> is returned, with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>W</code></td>
<td>
<p>Orthonormal matrix that takes the whitened version of x to the
independent components</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negentropy</code></td>
<td>
<p>The total negentropy measure of this solution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p>the matrix of k independent components</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>whitner</code></td>
<td>
<p>if <code>whiten=TRUE</code>, the matrix that whitens <code>x</code>,
else <code>NULL</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call that produced this object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>density</code></td>
<td>
<p>If <code>Gfunc=GPois</code>, an list of length <code>k</code> with the density
estimates for each component</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Trevor Hastie and Rob Tibshirani
</p>


<h3>References</h3>

<p>Hastie, T. and Tibshirani, R. (2003) <em>Independent Component Analysis
through Product Density Estimation</em> in <em>Advances in Neural Information
Processing Systems 15</em> (Becker, S. and Obermayer, K., eds), MIT Press,
Cambridge, MA. pp 649-656<br>
Hastie, T., Tibshirani, R. and Friedman, J. (2009) Elements of
Statistical Learning (2nd edition), Springer.<br><a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</p>


<h3>See Also</h3>

<p><code>GPois</code>, <code>G1</code> and <code>plot</code> method.
</p>


<h3>Examples</h3>

<pre><code class="language-R">p=2
### Can use letters a-r below for dist
dist="n" 
N=1024
A0&lt;-mixmat(p)
s&lt;-scale(cbind(rjordan(dist,N),rjordan(dist,N)))
x &lt;- s %*% A0
###Whiten the data
x &lt;- scale(x, TRUE, FALSE)
sx &lt;- svd(x)	### orthogonalization function
x &lt;- sqrt(N) * sx$u
target &lt;- solve(A0)
target &lt;- diag(sx$d) %*% t(sx$v) %*% target/sqrt(N)
W0 &lt;- matrix(rnorm(2*2), 2, 2)
W0 &lt;- ICAorthW(W0)
W1 &lt;- ProDenICA(x, W0=W0,trace=TRUE,Gfunc=G1)$W
fit=ProDenICA(x, W0=W0,Gfunc=GPois,trace=TRUE, density=TRUE)
W2 &lt;- fit$W
#distance of FastICA from target
amari(W1,target)
#distance of ProDenICA from target
amari(W2,target)
par(mfrow=c(2,1))
plot(fit)
</code></pre>


</div>