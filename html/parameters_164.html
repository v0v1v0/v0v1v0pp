<div class="container">

<table style="width: 100%;"><tr>
<td>p_significance.lm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Practical Significance (ps)</h2>

<h3>Description</h3>

<p>Compute the probability of <strong>Practical Significance</strong> (<em>ps</em>),
which can be conceptualized as a unidirectional equivalence test. It returns
the probability that an effect is above a given threshold corresponding to a
negligible effect in the median's direction, considering a parameter's <em>full</em>
confidence interval. In comparison the the <code>equivalence_test()</code> function,
where the <em>SGPV</em> (second generation p-value) describes the proportion of the
<em>full</em> confidence interval that is <em>inside</em> the ROPE, the value returned by
<code>p_significance()</code> describes the <em>larger</em> proportion of the <em>full</em> confidence
interval that is <em>outside</em> the ROPE. This makes <code>p_significance()</code> comparable
to <code>bayestestR::p_direction()</code>, however, while <code>p_direction()</code> compares to
a point-null by default, <code>p_significance()</code> compares to a range-null.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'lm'
p_significance(x, threshold = "default", ci = 0.95, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A statistical model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>The threshold value that separates significant from
negligible effect, which can have following possible values:
</p>

<ul>
<li> <p><code>"default"</code>, in which case the range is set to <code>0.1</code> if input is a vector,
and based on <code>rope_range()</code> if a (Bayesian) model is provided.
</p>
</li>
<li>
<p> a single numeric value (e.g., 0.1), which is used as range around zero
(i.e. the threshold range is set to -0.1 and 0.1, i.e. reflects a symmetric
interval)
</p>
</li>
<li>
<p> a numeric vector of length two (e.g., <code>c(-0.2, 0.1)</code>), useful for
asymmetric intervals.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ci</code></td>
<td>
<p>Confidence Interval (CI) level. Default to <code>0.95</code> (<code style="white-space: pre;">⁠95%⁠</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Toggle warnings and messages.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Arguments passed to other methods, e.g. <code>ci()</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>p_significance()</code> returns the proportion of the <em>full</em> confidence
interval range (assuming a normally distributed, equal-tailed interval) that
is outside a certain range (the negligible effect, or ROPE, see argument
<code>threshold</code>). If there are values of the distribution both below and above
the ROPE, <code>p_significance()</code> returns the higher probability of a value being
outside the ROPE. Typically, this value should be larger than 0.5 to indicate
practical significance. However, if the range of the negligible effect is
rather large compared to the range of the confidence interval,
<code>p_significance()</code> will be less than 0.5, which indicates no clear practical
significance.
</p>
<p>Note that the assumed interval, which is used to calculate the practical
significance, is an <em>approximation</em> of the <em>full interval</em> based on the
chosen confidence level. For example, if the 95% confidence interval of a
coefficient ranges from -1 to 1, the underlying <em>full (normally distributed)
interval</em> approximately ranges from -1.9 to 1.9, see also following code:
</p>
<div class="sourceCode"><pre># simulate full normal distribution
out &lt;- bayestestR::distribution_normal(10000, 0, 0.5)
# range of "full" distribution
range(out)
# range of 95% CI
round(quantile(out, probs = c(0.025, 0.975)), 2)
</pre></div>
<p>This ensures that the practical significance always refers to the general
compatible parameter space of coefficients. Therefore, the <em>full interval</em> is
similar to a Bayesian posterior distribution of an equivalent Bayesian model,
see following code:
</p>
<div class="sourceCode"><pre>library(bayestestR)
library(brms)
m &lt;- lm(mpg ~ gear + wt + cyl + hp, data = mtcars)
m2 &lt;- brm(mpg ~ gear + wt + cyl + hp, data = mtcars)
# probability of significance (ps) for frequentist model
p_significance(m)
# similar to ps of Bayesian models
p_significance(m2)
# similar to ps of simulated draws / bootstrap samples
p_significance(simulate_model(m))
</pre></div>


<h3>Value</h3>

<p>A data frame.
</p>


<h3>Statistical inference - how to quantify evidence</h3>

<p>There is no standardized approach to drawing conclusions based on the
available data and statistical models. A frequently chosen but also much
criticized approach is to evaluate results based on their statistical
significance (<em>Amrhein et al. 2017</em>).
</p>
<p>A more sophisticated way would be to test whether estimated effects exceed
the "smallest effect size of interest", to avoid even the smallest effects
being considered relevant simply because they are statistically significant,
but clinically or practically irrelevant (<em>Lakens et al. 2018, Lakens 2024</em>).
</p>
<p>A rather unconventional approach, which is nevertheless advocated by various
authors, is to interpret results from classical regression models in terms of
probabilities, similar to the usual approach in Bayesian statistics
(<em>Greenland et al. 2022; Rafi and Greenland 2020; Schweder 2018; Schweder and
Hjort 2003; Vos 2022</em>).
</p>
<p>The <strong>parameters</strong> package provides several options or functions to aid
statistical inference. These are, for example:
</p>

<ul>
<li> <p><code>equivalence_test()</code>, to compute the (conditional) equivalence test for
frequentist models
</p>
</li>
<li> <p><code>p_significance()</code>, to compute the probability of <em>practical significance</em>,
which can be conceptualized as a unidirectional equivalence test
</p>
</li>
<li> <p><code>p_function()</code>, or <em>consonance function</em>, to compute p-values and
compatibility (confidence) intervals for statistical models
</p>
</li>
<li>
<p> the <code>pd</code> argument (setting <code>pd = TRUE</code>) in <code>model_parameters()</code> includes
a column with the <em>probability of direction</em>, i.e. the probability that a
parameter is strictly positive or negative. See <code>bayestestR::p_direction()</code>
for details.
</p>
</li>
<li>
<p> the <code>s_value</code> argument (setting <code>s_value = TRUE</code>) in <code>model_parameters()</code>
replaces the p-values with their related <em>S</em>-values (<em>Rafi and Greenland 2020</em>)
</p>
</li>
<li>
<p> finally, it is possible to generate distributions of model coefficients by
generating bootstrap-samples (setting <code>bootstrap = TRUE</code>) or simulating
draws from model coefficients using <code>simulate_model()</code>. These samples
can then be treated as "posterior samples" and used in many functions from
the <strong>bayestestR</strong> package.
</p>
</li>
</ul>
<p>Most of the above shown options or functions derive from methods originally
implemented for Bayesian models (<em>Makowski et al. 2019</em>). However, assuming
that model assumptions are met (which means, the model fits well to the data,
the correct model is chosen that reflects the data generating process
(distributional model family) etc.), it seems appropriate to interpret
results from classical frequentist models in a "Bayesian way" (more details:
documentation in <code>p_function()</code>).
</p>


<h3>Note</h3>

<p>There is also a <a href="https://easystats.github.io/see/articles/bayestestR.html"><code>plot()</code>-method</a>
implemented in the <a href="https://easystats.github.io/see/"><strong>see</strong>-package</a>.
</p>


<h3>See Also</h3>

<p>For more details, see <code>bayestestR::p_significance()</code>. See also
<code>equivalence_test()</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data(qol_cancer)
model &lt;- lm(QoL ~ time + age + education, data = qol_cancer)

p_significance(model)
p_significance(model, threshold = c(-0.5, 1.5))

# plot method
if (require("see", quietly = TRUE)) {
  result &lt;- p_significance(model)
  plot(result)
}

</code></pre>


</div>