<div class="container">

<table style="width: 100%;"><tr>
<td>details_rule_fit_xrf</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>RuleFit models via xrf</h2>

<h3>Description</h3>

<p><code>xrf::xrf()</code> fits a model that derives simple feature rules from a tree
ensemble and uses the rules as features to a regularized model. <code>rules::xrf_fit()</code>
is a wrapper around this function.
</p>


<h3>Details</h3>

<p>For this engine, there are multiple modes: classification and regression
</p>


<h4>Tuning Parameters</h4>

<p>This model has 8 tuning parameters:
</p>

<ul>
<li> <p><code>mtry</code>: Proportion Randomly Selected Predictors (type: double,
default: see below)
</p>
</li>
<li> <p><code>trees</code>: # Trees (type: integer, default: 15L)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 1L)
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 6L)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.3)
</p>
</li>
<li> <p><code>loss_reduction</code>: Minimum Loss Reduction (type: double, default: 0.0)
</p>
</li>
<li> <p><code>sample_size</code>: Proportion Observations Sampled (type: double, default:
1.0)
</p>
</li>
<li> <p><code>penalty</code>: Amount of Regularization (type: double, default: 0.1)
</p>
</li>
</ul>
<h4>Translation from parsnip to the underlying model call (regression)</h4>

<p>The <strong>rules</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(rules)

rule_fit(
  mtry = numeric(1),
  trees = integer(1),
  min_n = integer(1),
  tree_depth = integer(1),
  learn_rate = numeric(1),
  loss_reduction = numeric(1),
  sample_size = numeric(1),
  penalty = numeric(1)
) %&gt;%
  set_engine("xrf") %&gt;%
  set_mode("regression") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## RuleFit Model Specification (regression)
## 
## Main Arguments:
##   mtry = numeric(1)
##   trees = integer(1)
##   min_n = integer(1)
##   tree_depth = integer(1)
##   learn_rate = numeric(1)
##   loss_reduction = numeric(1)
##   sample_size = numeric(1)
##   penalty = numeric(1)
## 
## Computational engine: xrf 
## 
## Model fit template:
## rules::xrf_fit(formula = missing_arg(), data = missing_arg(), 
##     xgb_control = missing_arg(), colsample_bynode = numeric(1), 
##     nrounds = integer(1), min_child_weight = integer(1), max_depth = integer(1), 
##     eta = numeric(1), gamma = numeric(1), subsample = numeric(1), 
##     lambda = numeric(1))
</pre></div>



<h4>Translation from parsnip to the underlying model call (classification)</h4>

<p>The <strong>rules</strong> extension package is required to fit this model.
</p>
<div class="sourceCode r"><pre>library(rules)

rule_fit(
  mtry = numeric(1),
  trees = integer(1),
  min_n = integer(1),
  tree_depth = integer(1),
  learn_rate = numeric(1),
  loss_reduction = numeric(1),
  sample_size = numeric(1),
  penalty = numeric(1)
) %&gt;%
  set_engine("xrf") %&gt;%
  set_mode("classification") %&gt;%
  translate()
</pre></div>
<div class="sourceCode"><pre>## RuleFit Model Specification (classification)
## 
## Main Arguments:
##   mtry = numeric(1)
##   trees = integer(1)
##   min_n = integer(1)
##   tree_depth = integer(1)
##   learn_rate = numeric(1)
##   loss_reduction = numeric(1)
##   sample_size = numeric(1)
##   penalty = numeric(1)
## 
## Computational engine: xrf 
## 
## Model fit template:
## rules::xrf_fit(formula = missing_arg(), data = missing_arg(), 
##     xgb_control = missing_arg(), colsample_bynode = numeric(1), 
##     nrounds = integer(1), min_child_weight = integer(1), max_depth = integer(1), 
##     eta = numeric(1), gamma = numeric(1), subsample = numeric(1), 
##     lambda = numeric(1))
</pre></div>



<h4>Differences from the xrf package</h4>

<p>Note that, per the documentation in <code>?xrf</code>, transformations of the
response variable are not supported. To use these with <code>rule_fit()</code>, we
recommend using a recipe instead of the formula method.
</p>
<p>Also, there are several configuration differences in how <code>xrf()</code> is fit
between that package and the wrapper used in <strong>rules</strong>. Some differences
in default values are:</p>

<table>
<tr>
<td style="text-align: left;">
   parameter </td>
<td style="text-align: left;"> <strong>xrf</strong> </td>
<td style="text-align: left;"> <strong>rules</strong> </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>trees</code> </td>
<td style="text-align: left;"> 100 </td>
<td style="text-align: left;"> 15 </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>max_depth</code> </td>
<td style="text-align: left;"> 3 </td>
<td style="text-align: left;"> 6 </td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p>These differences will create a disparity in the values of the <code>penalty</code>
argument that <strong>glmnet</strong> uses. Also, <strong>rules</strong> can also set <code>penalty</code>
whereas <strong>xrf</strong> uses an internal 5-fold cross-validation to determine it
(by default).
</p>



<h4>Preprocessing requirements</h4>

<p>Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via <code>fit()</code>, parsnip will
convert factor columns to indicators.
</p>



<h4>Other details</h4>



<h5>Interpreting <code>mtry</code>
</h5>

<p>The <code>mtry</code> argument denotes the number of predictors that will be
randomly sampled at each split when creating tree models.
</p>
<p>Some engines, such as <code>"xgboost"</code>, <code>"xrf"</code>, and <code>"lightgbm"</code>, interpret
their analogue to the <code>mtry</code> argument as the <em>proportion</em> of predictors
that will be randomly sampled at each split rather than the <em>count</em>. In
some settings, such as when tuning over preprocessors that influence the
number of predictors, this parameterization is quite
helpful—interpreting <code>mtry</code> as a proportion means that <code style="white-space: pre;">⁠[0, 1]⁠</code> is
always a valid range for that parameter, regardless of input data.
</p>
<p>parsnip and its extensions accommodate this parameterization using the
<code>counts</code> argument: a logical indicating whether <code>mtry</code> should be
interpreted as the number of predictors that will be randomly sampled at
each split. <code>TRUE</code> indicates that <code>mtry</code> will be interpreted in its
sense as a count, <code>FALSE</code> indicates that the argument will be
interpreted in its sense as a proportion.
</p>
<p><code>mtry</code> is a main model argument for
<code>boost_tree()</code> and
<code>rand_forest()</code>, and thus should not have an
engine-specific interface. So, regardless of engine, <code>counts</code> defaults
to <code>TRUE</code>. For engines that support the proportion interpretation
(currently <code>"xgboost"</code> and <code>"xrf"</code>, via the rules package, and
<code>"lightgbm"</code> via the bonsai package) the user can pass the
<code>counts = FALSE</code> argument to <code>set_engine()</code> to supply <code>mtry</code> values
within <code style="white-space: pre;">⁠[0, 1]⁠</code>.
</p>



<h5>Early stopping</h5>

<p>The <code>stop_iter()</code> argument allows the model to prematurely stop training
if the objective function does not improve within <code>early_stop</code>
iterations.
</p>
<p>The best way to use this feature is in conjunction with an <em>internal
validation set</em>. To do this, pass the <code>validation</code> parameter of
<code>xgb_train()</code> via the parsnip
<code>set_engine()</code> function. This is the
proportion of the training set that should be reserved for measuring
performance (and stopping early).
</p>
<p>If the model specification has <code>early_stop &gt;= trees</code>, <code>early_stop</code> is
converted to <code>trees - 1</code> and a warning is issued.
</p>




<h4>Case weights</h4>

<p>The underlying model implementation does not allow for case weights.
</p>



<h4>References</h4>


<ul><li>
<p> Friedman and Popescu. “Predictive learning via rule ensembles.” Ann.
Appl. Stat. 2 (3) 916- 954, September 2008
</p>
</li></ul>
</div>