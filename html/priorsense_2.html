<div class="container">

<table style="width: 100%;"><tr>
<td>cjs_dist</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cumulative Jensen-Shannon divergence</h2>

<h3>Description</h3>

<p>Computes the cumulative Jensen-Shannon distance between two
samples.
</p>


<h3>Usage</h3>

<pre><code class="language-R">cjs_dist(
  x,
  y,
  x_weights = NULL,
  y_weights = NULL,
  metric = TRUE,
  unsigned = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>numeric vector of samples from first distribution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>numeric vector of samples from second distribution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x_weights</code></td>
<td>
<p>numeric vector of weights of first distribution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y_weights</code></td>
<td>
<p>numeric vector of weights of second distribution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>Logical; if TRUE, return square-root of CJS</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unsigned</code></td>
<td>
<p>Logical; if TRUE then return max of CJS(P(x) ||
Q(x)) and CJS(P(-x) || Q(-x)). This ensures invariance to
transformations such as PCA.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>unused</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Cumulative Jensen-Shannon distance is a symmetric metric based
on the cumulative Jensen-Shannon divergence. The divergence CJS(P || Q)
between two cumulative distribution functions P and Q is defined as:
</p>
<p style="text-align: center;"><code class="reqn">CJS(P || Q) = \sum P(x) \log \frac{P(x)}{0.5 (P(x) + Q(x))} +
\frac{1}{2 \ln 2} \sum (Q(x) - P(x))</code>
</p>

<p>The symmetric metric is defined as:
</p>
<p style="text-align: center;"><code class="reqn">CJS_{dist}(P || Q) = \sqrt{CJS(P || Q) + CJS(Q || P)}</code>
</p>

<p>This has an upper bound of <code class="reqn">\sqrt{ \sum (P(x) + Q(x))}</code>
</p>


<h3>Value</h3>

<p>distance value based on CJS computation.
</p>


<h3>References</h3>

<p>Nguyen H-V., Vreeken J. (2015).  Non-parametric
Jensen-Shannon Divergence.  In: Appice A., Rodrigues P., Santos
Costa V., Gama J., Jorge A., Soares C. (eds) Machine Learning
and Knowledge Discovery in Databases.  ECML PKDD 2015. Lecture
Notes in Computer Science, vol 9285.  Springer, Cham.
<code>doi:10.1007/978-3-319-23525-7_11</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- rnorm(100)
y &lt;- rnorm(100, 2, 2)
cjs_dist(x, y, x_weights = NULL, y_weights = NULL)
</code></pre>


</div>