<div class="container">

<table style="width: 100%;"><tr>
<td>gJSD</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generalized Jensen-Shannon Divergence</h2>

<h3>Description</h3>

<p>This function computes the Generalized Jensen-Shannon Divergence of a probability matrix.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gJSD(x, unit = "log2", weights = NULL, est.prob = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a probability matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>a numeric vector specifying the weights for each distribution in <code>x</code>.
Default: <code>weights</code> = <code>NULL</code>; in this case all distributions are weighted equally (= uniform distribution of weights).
In case users wish to specify non-uniform weights for e.g. 3 distributions, they
can specify the argument <code>weights = c(0.5, 0.25, 0.25)</code>. This notation
denotes that <code>vec1</code> is weighted by <code>0.5</code>, <code>vec2</code> is weighted by <code>0.25</code>, and <code>vec3</code> is weighted by <code>0.25</code> as well.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>est.prob</code></td>
<td>
<p>method to estimate probabilities from input count vectors such as non-probability vectors. Default: <code>est.prob = NULL</code>. Options are:
</p>

<ul><li> <p><code>est.prob = "empirical"</code>: The relative frequencies of each vector are computed internally. For example an input matrix <code>rbind(1:10, 11:20)</code> will be transformed to a probability vector <code>rbind(1:10 / sum(1:10), 11:20 / sum(11:20))</code>
</p>
</li></ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Function to compute the Generalized Jensen-Shannon Divergence
</p>
<p><code class="reqn">JSD_{\pi_1,...,\pi_n}(P_1, ..., P_n) = H(\sum_{i = 1}^n \pi_i * P_i) - \sum_{i = 1}^n \pi_i*H(P_i)</code>
</p>
<p>where <code class="reqn">\pi_1,...,\pi_n</code> denote the weights selected for the probability vectors <code>P_1,...,P_n</code> and <code>H(P_i)</code> denotes the Shannon Entropy of probability vector <code>P_i</code>.
</p>


<h3>Value</h3>

<p>The Jensen-Shannon divergence between all possible combinations of comparisons.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>See Also</h3>

<p><code>KL</code>, <code>H</code>, <code>JSD</code>,
<code>CE</code>, <code>JE</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># define input probability matrix
Prob &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29), 30:39/sum(30:39))

# compute the Generalized JSD comparing the PS probability matrix
gJSD(Prob)

# Generalized Jensen-Shannon Divergence between three vectors using different log bases
gJSD(Prob, unit = "log2") # Default
gJSD(Prob, unit = "log")
gJSD(Prob, unit = "log10")

# Jensen-Shannon Divergence Divergence between count vectors P.count and Q.count
P.count &lt;- 1:10
Q.count &lt;- 20:29
R.count &lt;- 30:39
x.count &lt;- rbind(P.count, Q.count, R.count)
gJSD(x.count, est.prob = "empirical")

</code></pre>


</div>