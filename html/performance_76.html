<div class="container">

<table style="width: 100%;"><tr>
<td>performance_cv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cross-validated model performance</h2>

<h3>Description</h3>

<p>This function cross-validates regression models in a
user-supplied new sample or by using holdout (train-test), k-fold, or
leave-one-out cross-validation.
</p>


<h3>Usage</h3>

<pre><code class="language-R">performance_cv(
  model,
  data = NULL,
  method = c("holdout", "k_fold", "loo"),
  metrics = "all",
  prop = 0.3,
  k = 5,
  stack = TRUE,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A regression model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Optional. A data frame containing the same variables as <code>model</code>
that will be used as the cross-validation sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Character string, indicating the cross-validation method to use:
whether holdout (<code>"holdout"</code>, aka train-test), k-fold (<code>"k_fold"</code>), or
leave-one-out (<code>"loo"</code>). If <code>data</code> is supplied, this argument is ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>
<p>Can be <code>"all"</code>, <code>"common"</code> or a character vector of metrics to be
computed (some of <code>c("ELPD", "Deviance", "MSE", "RMSE", "R2")</code>). "common" will
compute R2 and RMSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prop</code></td>
<td>
<p>If <code>method = "holdout"</code>, what proportion of the sample to hold
out as the test sample?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>If <code>method = "k_fold"</code>, the number of folds to use.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stack</code></td>
<td>
<p>Logical. If <code>method = "k_fold"</code>, should performance be computed
by stacking residuals from each holdout fold and calculating each metric on
the stacked data (<code>TRUE</code>, default) or should performance be computed by
calculating metrics within each holdout fold and averaging performance
across each fold (<code>FALSE</code>)?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Toggle warnings.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Not used.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A data frame with columns for each metric requested, as well as <code>k</code>
if <code>method = "holdout"</code> and the <code>Method</code> used for cross-validation. If
<code>method = "holdout"</code> and <code>stack = TRUE</code>, the standard error (standard
deviation across holdout folds) for each metric is also included.
</p>


<h3>Examples</h3>

<pre><code class="language-R">model &lt;- lm(mpg ~ wt + cyl, data = mtcars)
performance_cv(model)

</code></pre>


</div>