<div class="container">

<table style="width: 100%;"><tr>
<td>cforest</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Random Forest </h2>

<h3>Description</h3>

<p>An implementation of the random forest and bagging ensemble algorithms
utilizing conditional inference trees as base learners.
</p>


<h3>Usage</h3>

<pre><code class="language-R">cforest(formula, data = list(), subset = NULL, weights = NULL, 
        controls = cforest_unbiased(),
        xtrafo = ptrafo, ytrafo = ptrafo, scores = NULL)
proximity(object, newdata = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p> a symbolic description of the model to be fit. Note 
that symbols like <code>:</code> and <code>-</code> will not work
and the tree will make use of all variables listed on the
rhs of <code>formula</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p> an data frame containing the variables in the model. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p> an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p> an optional vector of weights to be used in the fitting
process. Non-negative integer valued weights are
allowed as well as non-negative real weights.
Observations are sampled (with or without replacement)
according to probabilities <code>weights / sum(weights)</code>.
The fraction of observations to be sampled (without replacement)
is computed based on the sum of the weights if all weights
are integer-valued and based on the number of weights greater zero
else. Alternatively, <code>weights</code> can be a double matrix defining
case weights for all <code>ncol(weights)</code> trees in the forest directly.
This requires more storage but gives the user more control.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>controls</code></td>
<td>
<p>an object of class <code>ForestControl-class</code>, which can be
obtained using <code>cforest_control</code> (and its
convenience interfaces <code>cforest_unbiased</code> and <code>cforest_classical</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtrafo</code></td>
<td>
<p> a function to be applied to all input variables.
By default, the <code>ptrafo</code> function is applied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ytrafo</code></td>
<td>
<p> a function to be applied to all response variables.
By default, the <code>ptrafo</code> function is applied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scores</code></td>
<td>
<p> an optional named list of scores to be attached to ordered
factors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p> an object as returned by <code>cforest</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p> an optional data frame containing test data.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This implementation of the random forest (and bagging) algorithm differs
from the reference implementation in <code>randomForest</code>
with respect to the base learners used and the aggregation scheme applied.
</p>
<p>Conditional inference trees, see <code>ctree</code>, are fitted to each
of the <code>ntree</code> (defined via <code>cforest_control</code>) 
bootstrap samples of the learning sample. Most of the hyper parameters in 
<code>cforest_control</code> regulate the construction of the conditional inference trees.
Therefore you MUST NOT change anything you don't understand completely.
</p>
<p>Hyper parameters you might want to change in <code>cforest_control</code> are: 
</p>
<p>1. The number of randomly preselected variables <code>mtry</code>, which is fixed 
to the value 5 by default here for technical reasons, while in 
<code>randomForest</code> the default values for classification and regression
vary with the number of input variables. 
</p>
<p>2. The number of trees <code>ntree</code>. Use more trees if you have more variables.
</p>
<p>3. The depth of the trees, regulated by <code>mincriterion</code>. Usually unstopped and unpruned
trees are used in random forests. To grow large trees, set <code>mincriterion</code> to a small value.
</p>
<p>The aggregation scheme works by averaging observation weights extracted
from each of the <code>ntree</code> trees and NOT by averaging predictions directly
as in <code>randomForest</code>.
See Hothorn et al. (2004) for a description. 
</p>
<p>Predictions can be computed using <code>predict</code>. For observations
with zero weights, predictions are computed from the fitted tree 
when <code>newdata = NULL</code>. While <code>predict</code> returns predictions
of the same type as the response in the data set by default (i.e., predicted class labels for factors), 
<code>treeresponse</code> returns the statistics of the conditional distribution of the response
(i.e., predicted class probabilities for factors). The same is done by <code>predict(..., type = "prob")</code>.
Note that for multivariate responses <code>predict</code> does not convert predictions to the type 
of the response, i.e., <code>type = "prob"</code> is used.
</p>
<p>Ensembles of conditional inference trees have not yet been extensively
tested, so this routine is meant for the expert user only and its current
state is rather experimental. However, there are some things available 
in <code>cforest</code> that can't be done with <code>randomForest</code>, 
for example fitting forests to censored response variables (see Hothorn et al., 2006a) or to
multivariate and ordered responses.
</p>
<p>Moreover, when predictors vary in their scale of measurement of number 
of categories, variable selection and computation of variable importance is biased 
in favor of variables with many potential cutpoints in <code>randomForest</code>, 
while in <code>cforest</code> unbiased trees and an adequate resampling scheme 
are used by default. See Hothorn et al. (2006b) and Strobl et al. (2007) 
as well as Strobl et al. (2009). 
</p>
<p>The <code>proximity</code> matrix is an <code class="reqn">n \times n</code> matrix <code class="reqn">P</code> with <code class="reqn">P_{ij}</code>
equal to the fraction of trees where observations <code class="reqn">i</code> and <code class="reqn">j</code> 
are element of the same terminal node (when both <code class="reqn">i</code> and <code class="reqn">j</code>
had non-zero weights in the same bootstrap sample).
</p>


<h3>Value</h3>

<p>An object of class <code>RandomForest-class</code>.
</p>


<h3>References</h3>

 
<p>Leo Breiman (2001). Random Forests. <em>Machine Learning</em>, 45(1), 5–32.
</p>
<p>Torsten Hothorn, Berthold Lausen, Axel Benner and Martin Radespiel-Troeger
(2004). Bagging Survival Trees. <em>Statistics in Medicine</em>, <b>23</b>(1), 77–91.
</p>
<p>Torsten Hothorn, Peter Buhlmann, Sandrine Dudoit, Annette Molinaro 
and Mark J. van der Laan (2006a). Survival Ensembles. <em>Biostatistics</em>, 
<b>7</b>(3), 355–373.
</p>
<p>Torsten Hothorn, Kurt Hornik and Achim Zeileis (2006b). Unbiased
Recursive Partitioning: A Conditional Inference Framework.
<em>Journal of Computational and Graphical Statistics</em>, <b>15</b>(3),
651–674.  Preprint available from 
<a href="https://www.zeileis.org/papers/Hothorn+Hornik+Zeileis-2006.pdf">https://www.zeileis.org/papers/Hothorn+Hornik+Zeileis-2006.pdf</a>
</p>
<p>Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis and Torsten Hothorn (2007).
Bias in Random Forest Variable Importance Measures: Illustrations, Sources and 
a Solution. <em>BMC Bioinformatics</em>, <b>8</b>, 25. 
<a href="https://doi.org/10.1186/1471-2105-8-25">doi:10.1186/1471-2105-8-25</a>
</p>
<p>Carolin Strobl, James Malley and Gerhard Tutz (2009).
An Introduction to Recursive Partitioning: Rationale, Application, and Characteristics of
Classification and Regression Trees, Bagging, and Random forests.
<em>Psychological Methods</em>, <b>14</b>(4), 323–348.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
    set.seed(290875)

    ### honest (i.e., out-of-bag) cross-classification of
    ### true vs. predicted classes
    data("mammoexp", package = "TH.data")
    table(mammoexp$ME, predict(cforest(ME ~ ., data = mammoexp, 
                               control = cforest_unbiased(ntree = 50)),
                               OOB = TRUE))

    ### fit forest to censored response
    if (require("TH.data") &amp;&amp; require("survival")) {

        data("GBSG2", package = "TH.data")
        bst &lt;- cforest(Surv(time, cens) ~ ., data = GBSG2, 
                   control = cforest_unbiased(ntree = 50))

        ### estimate conditional Kaplan-Meier curves
        treeresponse(bst, newdata = GBSG2[1:2,], OOB = TRUE)

        ### if you can't resist to look at individual trees ...
        party:::prettytree(bst@ensemble[[1]], names(bst@data@get("input")))
    }

    ### proximity, see ?randomForest
    iris.cf &lt;- cforest(Species ~ ., data = iris, 
                       control = cforest_unbiased(mtry = 2))
    iris.mds &lt;- cmdscale(1 - proximity(iris.cf), eig = TRUE)
    op &lt;- par(pty="s")
    pairs(cbind(iris[,1:4], iris.mds$points), cex = 0.6, gap = 0, 
          col = c("red", "green", "blue")[as.numeric(iris$Species)],
          main = "Iris Data: Predictors and MDS of Proximity Based on cforest")
    par(op)

</code></pre>


</div>