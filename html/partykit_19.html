<div class="container">

<table style="width: 100%;"><tr>
<td>ctree</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Conditional Inference Trees</h2>

<h3>Description</h3>

<p>Recursive partitioning for continuous, censored, ordered, nominal and
multivariate response variables in a conditional inference framework. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">ctree(formula, data, subset, weights, na.action = na.pass, offset, cluster, 
    control = ctree_control(...), ytrafo = NULL, 
    converged = NULL, scores = NULL, doFit = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p> a symbolic description of the model to be fit. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p> a data frame containing the variables in the model. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p> an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p> an optional vector of weights to be used in the fitting
process. Only non-negative integer valued weights are
allowed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p> an optional vector of offset values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cluster</code></td>
<td>
<p> an optional factor indicating independent clusters. 
Highly experimental, use at your own risk.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain missing value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>a list with control parameters, see
<code>ctree_control</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ytrafo</code></td>
<td>
<p>an optional named list of functions to be applied to the response 
variable(s) before testing their association with the explanatory 
variables. Note that this transformation is only
performed once for the root node and does not take weights into account.
Alternatively, <code>ytrafo</code> can be a function of <code>data</code> and
<code>weights</code>. In this case, the transformation is computed for
every node with corresponding weights. This feature is experimental
and the user interface likely to change.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>converged</code></td>
<td>
<p>an optional function for checking user-defined criteria
before splits are implemented. This is not to be used and
very likely to change.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scores</code></td>
<td>
<p>an optional named list of scores to be attached to ordered
factors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>doFit</code></td>
<td>
<p>a logical, if <code>FALSE</code>, the tree is not fitted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>arguments passed to <code>ctree_control</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Function <code>partykit::ctree</code> is a reimplementation of (most of)
<code>party::ctree</code> employing the new <code>party</code> infrastructure
of the <span class="pkg">partykit</span> infrastructure. The vignette <code>vignette("ctree", package = "partykit")</code>
explains internals of the different implementations.
</p>
<p>Conditional inference trees estimate a regression relationship by binary recursive
partitioning in a conditional inference framework. Roughly, the algorithm
works as follows: 1) Test the global null hypothesis of independence between
any of the input variables and the response (which may be multivariate as well). 
Stop if this hypothesis cannot be rejected. Otherwise select the input
variable with strongest association to the response. This
association is measured by a p-value corresponding to a test for the
partial null hypothesis of a single input variable and the response.
2) Implement a binary split in the selected input variable. 
3) Recursively repeate steps 1) and 2). 
</p>
<p>The implementation utilizes a unified framework for conditional inference,
or permutation tests, developed by Strasser and Weber (1999). The stop
criterion in step 1) is either based on multiplicity adjusted p-values 
(<code>testtype = "Bonferroni"</code> in <code>ctree_control</code>)
or on the univariate p-values (<code>testtype = "Univariate"</code>). In both cases, the
criterion is maximized, i.e., 1 - p-value is used. A split is implemented 
when the criterion exceeds the value given by <code>mincriterion</code> as
specified in <code>ctree_control</code>. For example, when 
<code>mincriterion = 0.95</code>, the p-value must be smaller than
$0.05$ in order to split this node. This statistical approach ensures that
the right-sized tree is grown without additional (post-)pruning or cross-validation.
The level of <code>mincriterion</code> can either be specified to be appropriate
for the size of the data set (and <code>0.95</code> is typically appropriate for
small to moderately-sized data sets) or could potentially be treated like a
hyperparameter (see Section~3.4 in Hothorn, Hornik and Zeileis, 2006).
The selection of the input variable to split in
is based on the univariate p-values avoiding a variable selection bias
towards input variables with many possible cutpoints. The test statistics
in each of the nodes can be extracted with the <code>sctest</code> method.
(Note that the generic is in the <span class="pkg">strucchange</span> package so this either
needs to be loaded or <code>sctest.constparty</code> has to be called directly.)
In cases where splitting stops due to the sample size (e.g., <code>minsplit</code>
or <code>minbucket</code> etc.), the test results may be empty.
</p>
<p>Predictions can be computed using <code>predict</code>, which returns predicted means,
predicted classes or median predicted survival times and 
more information about the conditional
distribution of the response, i.e., class probabilities
or predicted Kaplan-Meier curves. For observations
with zero weights, predictions are computed from the fitted tree 
when <code>newdata = NULL</code>.
</p>
<p>By default, the scores for each ordinal factor <code>x</code> are
<code>1:length(x)</code>, this may be changed for variables in the formula 
using <code>scores = list(x = c(1, 5, 6))</code>, for example.
</p>
<p>For a general description of the methodology see Hothorn, Hornik and
Zeileis (2006) and Hothorn, Hornik, van de Wiel and Zeileis (2006).
</p>


<h3>Value</h3>

<p>An object of class <code>party</code>.
</p>


<h3>References</h3>

 
<p>Hothorn T, Hornik K, Van de Wiel MA, Zeileis A (2006).
A Lego System for Conditional Inference.
<em>The American Statistician</em>, <b>60</b>(3), 257–263.
</p>
<p>Hothorn T, Hornik K, Zeileis A (2006).
Unbiased Recursive Partitioning: A Conditional Inference Framework.
<em>Journal of Computational and Graphical Statistics</em>, <b>15</b>(3), 651–674. 
</p>
<p>Hothorn T, Zeileis A (2015).
partykit: A Modular Toolkit for Recursive Partytioning in R.
<em>Journal of Machine Learning Research</em>, <b>16</b>, 3905–3909.
</p>
<p>Strasser H, Weber C (1999).
On the Asymptotic Theory of Permutation Statistics.
<em>Mathematical Methods of Statistics</em>, <b>8</b>, 220–250.
</p>


<h3>Examples</h3>

<pre><code class="language-R">### regression
airq &lt;- subset(airquality, !is.na(Ozone))
airct &lt;- ctree(Ozone ~ ., data = airq)
airct
plot(airct)
mean((airq$Ozone - predict(airct))^2)

### classification
irisct &lt;- ctree(Species ~ .,data = iris)
irisct
plot(irisct)
table(predict(irisct), iris$Species)

### estimated class probabilities, a list
tr &lt;- predict(irisct, newdata = iris[1:10,], type = "prob")

### survival analysis
if (require("TH.data") &amp;&amp; require("survival") &amp;&amp; 
    require("coin") &amp;&amp; require("Formula")) {

  data("GBSG2", package = "TH.data")
  (GBSG2ct &lt;- ctree(Surv(time, cens) ~ ., data = GBSG2))
  predict(GBSG2ct, newdata = GBSG2[1:2,], type = "response")	  
  plot(GBSG2ct)

  ### with weight-dependent log-rank scores
  ### log-rank trafo for observations in this node only (= weights &gt; 0)
  h &lt;- function(y, x, start = NULL, weights, offset, estfun = TRUE, object = FALSE, ...) {
      if (is.null(weights)) weights &lt;- rep(1, NROW(y))
      s &lt;- logrank_trafo(y[weights &gt; 0,,drop = FALSE])
      r &lt;- rep(0, length(weights))
      r[weights &gt; 0] &lt;- s
      list(estfun = matrix(as.double(r), ncol = 1), converged = TRUE)
  }

  ### very much the same tree
  (ctree(Surv(time, cens) ~ ., data = GBSG2, ytrafo = h))
}

### multivariate responses
airct2 &lt;- ctree(Ozone + Temp ~ ., data = airq)
airct2
plot(airct2)
</code></pre>


</div>