<div class="container">

<table style="width: 100%;"><tr>
<td>policy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Extract the Policy from a POMDP/MDP</h2>

<h3>Description</h3>

<p>Extracts the policy from a solved POMDP/MDP.
</p>


<h3>Usage</h3>

<pre><code class="language-R">policy(x, drop = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A solved POMDP or MDP object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>drop</code></td>
<td>
<p>logical; drop the list for converged, epoch-independent policies.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A list (one entry per epoch) with the optimal policy.
For converged, infinite-horizon problems solutions, a list with only the
converged solution is produced.
For a POMDP, the policy is a data.frame consisting of:
</p>

<ul>
<li>
<p> Part 1: The alpha vectors for the belief states (defines also the
utility of the belief). The columns have
the names of states.
</p>
</li>
<li>
<p> Part 2: The last column named <code>action</code> contains the prescribed action.
</p>
</li>
</ul>
<p>For an MDP, the policy is a data.frame with columns for:
</p>

<ul>
<li> <p><code>state</code>: The state.
</p>
</li>
<li> <p><code>U</code>: The state's value (discounted expected utility U) if the policy
is followed
</p>
</li>
<li> <p><code>action</code>: The prescribed action.
</p>
</li>
</ul>
<h3>Value</h3>

<p>A list with the policy for each epoch. Converged policies
have only one element. If <code>drop = TRUE</code> then the policy is returned
without a list.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code>estimate_belief_for_nodes()</code>,
<code>optimal_action()</code>,
<code>plot_belief_space()</code>,
<code>plot_policy_graph()</code>,
<code>policy_graph()</code>,
<code>projection()</code>,
<code>reward()</code>,
<code>solve_POMDP()</code>,
<code>solve_SARSOP()</code>,
<code>value_function()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("Tiger")

# Infinite horizon
sol &lt;- solve_POMDP(model = Tiger)
sol

# policy with value function, optimal action and transitions for observations.
policy(sol)
plot_value_function(sol)

# Finite horizon (we use incremental pruning because grid does not converge)
sol &lt;- solve_POMDP(model = Tiger, method = "incprune", 
  horizon = 3, discount = 1)
sol

policy(sol)
# Note: We see that it is initially better to listen till we make 
#       a decision in the final epoch.

# MDP policy
data(Maze)

sol &lt;- solve_MDP(Maze)

policy(sol)
</code></pre>


</div>