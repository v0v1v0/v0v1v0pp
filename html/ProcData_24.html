<div class="container">

<table style="width: 100%;"><tr>
<td>seq2feature_seq2seq</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Feature Extraction by autoencoder</h2>

<h3>Description</h3>

<p><code>seq2feature_seq2seq</code> extract features from response processes by autoencoder.
</p>


<h3>Usage</h3>

<pre><code class="language-R">seq2feature_seq2seq(seqs, ae_type = "action", K, rnn_type = "lstm",
  n_epoch = 50, method = "last", step_size = 1e-04,
  optimizer_name = "adam", cumulative = FALSE, log = TRUE,
  weights = c(1, 0.5), samples_train, samples_valid,
  samples_test = NULL, pca = TRUE, verbose = TRUE,
  return_theta = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>seqs</code></td>
<td>
<p>an object of class <code>"proc"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ae_type</code></td>
<td>
<p>a string specifies the type of autoencoder. The autoencoder can be an
action sequence autoencoder ("action"), a time sequence autoencoder ("time"), or an 
action-time sequence autoencoder ("both").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>the number of features to be extracted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rnn_type</code></td>
<td>
<p>the type of recurrent unit to be used for modeling
response processes. <code>"lstm"</code> for the long-short term memory unit. 
<code>"gru"</code> for the gated recurrent unit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_epoch</code></td>
<td>
<p>the number of training epochs for the autoencoder.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>the method for computing features from the output of an
recurrent neural network in the encoder. Available options are 
<code>"last"</code> and <code>"avg"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step_size</code></td>
<td>
<p>the learning rate of optimizer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimizer_name</code></td>
<td>
<p>a character string specifying the optimizer to be used
for training. Availabel options are <code>"sgd"</code>, <code>"rmsprop"</code>, 
<code>"adadelta"</code>, and <code>"adam"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cumulative</code></td>
<td>
<p>logical. If TRUE, the sequence of cumulative time up to each event is
used as input to the neural network. If FALSE, the sequence of inter-arrival time (gap 
time between an event and the previous event) will be used as input to the neural network.
Default is FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>log</code></td>
<td>
<p>logical. If TRUE, for the timestamp sequences, input of the neural net is
the base-10 log of the original sequence of times plus 1 (i.e., log10(t+1)). If FALSE,
the original sequence of times is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>a vector of 2 elements for the weight of the loss of action sequences
(categorical_crossentropy) and time sequences (mean squared error), respectively. 
The total loss is calculated as the weighted sum of the two losses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>samples_train, samples_valid, samples_test</code></td>
<td>
<p>vectors of indices specifying the
training, validation and test sets for training autoencoder.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pca</code></td>
<td>
<p>logical. If TRUE, the principal components of features are
returned. Default is TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical. If TRUE, training progress is printed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_theta</code></td>
<td>
<p>logical. If TRUE, extracted features are returned.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function wraps <code>aseq2feature_seq2seq</code>, 
<code>tseq2feature_seq2seq</code>, and <code>atseq2feature_seq2seq</code>.
</p>


<h3>Value</h3>

<p><code>seq2feature_seq2seq</code> returns a list containing
</p>
<table>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>a matrix containing <code>K</code> features or principal features. Each column is a feature.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train_loss</code></td>
<td>
<p>a vector of length <code>n_epoch</code> recording the trace of training losses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>valid_loss</code></td>
<td>
<p>a vector of length <code>n_epoch</code> recording the trace of validation losses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test_loss</code></td>
<td>
<p>a vector of length <code>n_epoch</code> recording the trace of test losses. Exists only if <code>samples_test</code> is not <code>NULL</code>.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Tang, X., Wang, Z., Liu, J., and Ying, Z. (2020) An exploratory analysis of the latent 
structure of process data via action sequence autoencoders. <em>British Journal of 
Mathematical and Statistical Psychology</em>. 74(1), 1-33.
</p>


<h3>See Also</h3>

<p><code>chooseK_seq2seq</code> for choosing <code>K</code> through cross-validation.
</p>
<p>Other feature extraction methods: <code>aseq2feature_seq2seq</code>,
<code>atseq2feature_seq2seq</code>,
<code>seq2feature_mds_large</code>,
<code>seq2feature_mds</code>,
<code>seq2feature_ngram</code>,
<code>tseq2feature_seq2seq</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"> 
if (!system("python -c 'import tensorflow as tf'", ignore.stdout = TRUE, ignore.stderr= TRUE)) {
  n &lt;- 50
  data(cc_data)
  samples &lt;- sample(1:length(cc_data$seqs$time_seqs), n)
  seqs &lt;- sub_seqs(cc_data$seqs, samples)

  # action sequence autoencoder
  K_res &lt;- chooseK_seq2seq(seqs=seqs, ae_type="action", K_cand=c(5, 10), 
                           n_epoch=5, n_fold=2, valid_prop=0.2)
  seq2seq_res &lt;- seq2feature_seq2seq(seqs=seqs, ae_type="action", K=K_res$K, 
                         n_epoch=5, samples_train=1:40, samples_valid=41:50)
  theta &lt;- seq2seq_res$theta

  # time sequence autoencoder
  K_res &lt;- chooseK_seq2seq(seqs=seqs, ae_type="time", K_cand=c(5, 10), 
                           n_epoch=5, n_fold=2, valid_prop=0.2)
  seq2seq_res &lt;- seq2feature_seq2seq(seqs=seqs, ae_type="time", K=K_res$K, 
                         n_epoch=5, samples_train=1:40, samples_valid=41:50)
  theta &lt;- seq2seq_res$theta

  # action and time sequence autoencoder
  K_res &lt;- chooseK_seq2seq(seqs=seqs, ae_type="both", K_cand=c(5, 10), 
                           n_epoch=5, n_fold=2, valid_prop=0.2)
  seq2seq_res &lt;- seq2feature_seq2seq(seqs=seqs, ae_type="both", K=K_res$K, 
                         n_epoch=5, samples_train=1:40, samples_valid=41:50)
  theta &lt;- seq2seq_res$theta
  plot(seq2seq_res$train_loss, col="blue", type="l")
  lines(seq2seq_res$valid_loss, col="red")
}

</code></pre>


</div>