<div class="container">

<table style="width: 100%;"><tr>
<td>psqn_generic</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generic Partially Separable Function Optimization</h2>

<h3>Description</h3>

<p>Optimization method for generic partially separable functions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">psqn_generic(
  par,
  fn,
  n_ele_func,
  rel_eps = 1e-08,
  max_it = 100L,
  n_threads = 1L,
  c1 = 1e-04,
  c2 = 0.9,
  use_bfgs = TRUE,
  trace = 0L,
  cg_tol = 0.5,
  strong_wolfe = TRUE,
  env = NULL,
  max_cg = 0L,
  pre_method = 1L,
  mask = as.integer(c()),
  gr_tol = -1
)

psqn_aug_Lagrang_generic(
  par,
  fn,
  n_ele_func,
  consts,
  n_constraints,
  multipliers = as.numeric(c()),
  penalty_start = 1L,
  rel_eps = 1e-08,
  max_it = 100L,
  max_it_outer = 100L,
  violations_norm_thresh = 1e-06,
  n_threads = 1L,
  c1 = 1e-04,
  c2 = 0.9,
  tau = 1.5,
  use_bfgs = TRUE,
  trace = 0L,
  cg_tol = 0.5,
  strong_wolfe = TRUE,
  env = NULL,
  max_cg = 0L,
  pre_method = 1L,
  mask = as.integer(c()),
  gr_tol = -1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>par</code></td>
<td>
<p>Initial values for the parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fn</code></td>
<td>
<p>Function to compute the element functions and their
derivatives. Each call computes an element function. See the examples
section.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_ele_func</code></td>
<td>
<p>Number of element functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rel_eps</code></td>
<td>
<p>Relative convergence threshold.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_it</code></td>
<td>
<p>Maximum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>c1</code></td>
<td>
<p>Thresholds for the Wolfe condition.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>c2</code></td>
<td>
<p>Thresholds for the Wolfe condition.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_bfgs</code></td>
<td>
<p>Logical for whether to use BFGS updates or SR1 updates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>Integer where larger values gives more information during the
optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cg_tol</code></td>
<td>
<p>Threshold for the conjugate gradient method.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>strong_wolfe</code></td>
<td>
<p><code>TRUE</code> if the strong Wolfe condition should be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>env</code></td>
<td>
<p>Environment to evaluate <code>fn</code> in. <code>NULL</code> yields the
global environment.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_cg</code></td>
<td>
<p>Maximum number of conjugate gradient iterations in each
iteration. Use zero if there should not be a limit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pre_method</code></td>
<td>
<p>Preconditioning method in the conjugate gradient method.
Zero yields no preconditioning, one yields diagonal preconditioning,
two yields the incomplete Cholesky factorization from Eigen, and
three yields a block diagonal preconditioning. One and three are fast
options with three seeming to work well for some poorly conditioned
problems.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mask</code></td>
<td>
<p>zero based indices for parameters to mask (i.e. fix).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gr_tol</code></td>
<td>
<p>convergence tolerance for the Euclidean norm of the gradient. A negative
value yields no check.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>consts</code></td>
<td>
<p>Function to compute the constraints which must be equal to
zero. See the example Section.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_constraints</code></td>
<td>
<p>The number of constraints.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>multipliers</code></td>
<td>
<p>Staring values for the multipliers in the augmented
Lagrangian method. There needs to be the same number of multipliers as the
number of constraints. An empty vector, <code>numeric()</code>, yields zero as
the starting value for all multipliers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty_start</code></td>
<td>
<p>Starting value for the penalty parameterin the
augmented Lagrangian method.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_it_outer</code></td>
<td>
<p>Maximum number of augmented Lagrangian steps.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>violations_norm_thresh</code></td>
<td>
<p>Threshold for the norm of the constraint
violations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>Multiplier used for the penalty parameter between each outer
iterations.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function follows the method described by Nocedal and Wright (2006)
and mainly what is described in Section 7.4. Details are provided
in the psqn vignette. See <code>vignette("psqn", package = "psqn")</code>.
</p>
<p>The partially separable function we consider can be quite general and the
only restriction is that we can write the function to be minimized as a sum
of so-called element functions each of which only depends on a small number
of the parameters. A more restricted version is available through the
<code>psqn</code> function.
</p>
<p>The optimization function is also available in C++ as a header-only
library. Using C++ may reduce the computation time substantially. See
the vignette in the package for examples.
</p>


<h3>Value</h3>

<p>A list like <code>psqn</code> and <code>psqn_aug_Lagrang</code>.
</p>


<h3>References</h3>

<p>Nocedal, J. and Wright, S. J. (2006). <em>Numerical Optimization</em>
(2nd ed.). Springer.
</p>
<p>Lin, C. and Mor√©, J. J. (1999). <em>Incomplete Cholesky factorizations
with limited memory</em>. SIAM Journal on Scientific Computing.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># example with a GLM as in the vignette

# assign the number of parameters and number of observations
set.seed(1)
K &lt;- 20L
n &lt;- 5L * K

# simulate the data
truth_limit &lt;- runif(K, -1, 1)
dat &lt;- replicate(
  n, {
    # sample the indices
    n_samp &lt;- sample.int(5L, 1L) + 1L
    indices &lt;- sort(sample.int(K, n_samp))

    # sample the outcome, y, and return
    list(y = rpois(1, exp(sum(truth_limit[indices]))),
         indices = indices)
  }, simplify = FALSE)

# we need each parameter to be present at least once
stopifnot(length(unique(unlist(
  lapply(dat, `[`, "indices")
))) == K) # otherwise we need to change the code

# assign the function we need to pass to psqn_generic
#
# Args:
#   i cluster/element function index.
#   par the parameters that this element function depends on. It has length zero
#       if we need to pass the one-based indices of the parameters that the i'th
#       element function depends on.
#   comp_grad TRUE of the gradient should be computed.
r_func &lt;- function(i, par, comp_grad){
  z &lt;- dat[[i]]
  if(length(par) == 0L)
    # return the indices
    return(z$indices)

  eta &lt;- sum(par)
  exp_eta &lt;- exp(eta)
  out &lt;- -z$y * eta + exp_eta
  if(comp_grad)
    attr(out, "grad") &lt;- rep(-z$y + exp_eta, length(z$indices))
  out
}

# minimize the function
R_res &lt;- psqn_generic(
  par = numeric(K), fn = r_func, n_ele_func = length(dat), c1 = 1e-4, c2 = .1,
  trace = 0L, rel_eps = 1e-9, max_it = 1000L, env = environment())

# get the same as if we had used optim
R_func &lt;- function(x){
  out &lt;- vapply(dat, function(z){
    eta &lt;- sum(x[z$indices])
    -z$y * eta + exp(eta)
  }, 0.)
  sum(out)
}
R_func_gr &lt;- function(x){
  out &lt;- numeric(length(x))
  for(z in dat){
    idx_i &lt;- z$indices
    eta &lt;- sum(x[idx_i])
    out[idx_i] &lt;- out[idx_i] -z$y + exp(eta)
  }
  out
}

opt &lt;- optim(numeric(K), R_func, R_func_gr, method = "BFGS",
             control = list(maxit = 1000L))

# we got the same
all.equal(opt$value, R_res$value)

# also works if we fix some parameters
to_fix &lt;- c(7L, 1L, 18L)
par_fix &lt;- numeric(K)
par_fix[to_fix] &lt;- c(-1, -.5, 0)

R_res &lt;- psqn_generic(
  par = par_fix, fn = r_func, n_ele_func = length(dat), c1 = 1e-4, c2 = .1,
  trace = 0L, rel_eps = 1e-9, max_it = 1000L, env = environment(),
  mask = to_fix - 1L) # notice the -1L because of the zero based indices

# the equivalent optim version is
opt &lt;- optim(
  numeric(K - length(to_fix)),
  function(par) { par_fix[-to_fix] &lt;- par; R_func   (par_fix) },
  function(par) { par_fix[-to_fix] &lt;- par; R_func_gr(par_fix)[-to_fix] },
  method = "BFGS", control = list(maxit = 1000L))

res_optim &lt;- par_fix
res_optim[-to_fix] &lt;- opt$par

# we got the same
all.equal(res_optim, R_res$par, tolerance = 1e-5)
all.equal(R_res$par[to_fix], par_fix[to_fix]) # the parameters are fixed

# add equality constraints
idx_constrained &lt;- list(c(2L, 19L, 11L, 7L), c(3L, 5L, 8L), 9:7)

# evaluates the c(x) in equalities c(x) = 0.
#
# Args:
#   i constrain index.
#   par the constrained parameters. It has length zero if we need to pass the
#       one-based indices of the parameters that the i'th constrain depends on.
#   what integer which is zero if the function should be returned and one if the
#        gradient should be computed.
consts &lt;- function(i, par, what){
  if(length(par) == 0)
    # need to return the indices
    return(idx_constrained[[i]])

  if(i == 1){
    out &lt;- exp(sum(par[1:2])) + exp(sum(par[3:4])) - 1
    if(what == 1)
      attr(out, "grad") &lt;- c(rep(exp(sum(par[1:2])), 2),
                             rep(exp(sum(par[3:4])), 2))

  } else if(i == 2){
    # the parameters need to be on a circle
    out &lt;- sum(par^2) - 1
    if(what == 1)
      attr(out, "grad") &lt;- 2 * par
  } else if(i == 3){
    out &lt;- sum(par) - .5
    if(what == 1)
      attr(out, "grad") &lt;- rep(1, length(par))
  }

  out
}

# optimize with the constraints and masking
res_consts &lt;- psqn_aug_Lagrang_generic(
  par = par_fix, fn = r_func, n_ele_func = length(dat), c1 = 1e-4, c2 = .1,
  trace = 0L, rel_eps = 1e-8, max_it = 1000L, env = environment(),
  consts = consts, n_constraints = length(idx_constrained),
  mask = to_fix - 1L)

res_consts

# the constraints are satisfied
consts(1, res_consts$par[idx_constrained[[1]]], 0) # ~ 0
consts(2, res_consts$par[idx_constrained[[2]]], 0) # ~ 0
consts(3, res_consts$par[idx_constrained[[3]]], 0) # ~ 0

# compare with the alabama package
if(require(alabama)){
    ala_fit &lt;- auglag(
      par_fix, R_func, R_func_gr,
      heq = function(x){
        c(x[to_fix] - par_fix[to_fix],
          consts(1, x[idx_constrained[[1]]], 0),
          consts(2, x[idx_constrained[[2]]], 0),
          consts(3, x[idx_constrained[[3]]], 0))
      }, control.outer = list(trace = 0L))

    cat(sprintf("Difference in objective value is %.6f. Parametes are\n",
                ala_fit$value - res_consts$value))
    print(rbind(alabama = ala_fit$par,
                psqn = res_consts$par))

    cat("\nOutput from all.equal\n")
    print(all.equal(ala_fit$par, res_consts$par))
}

# the overhead here is though quite large with the R interface from the psqn
# package. A C++ implementation is much faster as shown in
# vignette("psqn", package = "psqn"). The reason it is that it is very fast
# to evaluate the element functions in this case

</code></pre>


</div>