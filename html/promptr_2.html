<div class="container">

<table style="width: 100%;"><tr>
<td>complete_prompt</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Complete an LLM Prompt</h2>

<h3>Description</h3>

<p>Submits a text prompt to OpenAI's "Completion" API endpoint and formats the response into a string or tidy dataframe. (Note that, as of 2024, this endpoint is considered "Legacy" by OpenAI and is likely to be deprecated.)
</p>


<h3>Usage</h3>

<pre><code class="language-R">complete_prompt(
  prompt,
  model = "gpt-3.5-turbo-instruct",
  openai_api_key = Sys.getenv("OPENAI_API_KEY"),
  max_tokens = 1,
  temperature = 0,
  seed = NULL,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>prompt</code></td>
<td>
<p>The prompt</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>Which OpenAI model to use. Defaults to 'gpt-3.5-turbo-instruct'</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>openai_api_key</code></td>
<td>
<p>Your API key. By default, looks for a system environment variable called "OPENAI_API_KEY" (recommended option). Otherwise, it will prompt you to enter the API key as an argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_tokens</code></td>
<td>
<p>How many tokens (roughly 4 characters of text) should the model return? Defaults to a single token (next word prediction).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>temperature</code></td>
<td>
<p>A numeric between 0 and 2 When set to zero, the model will always return the most probable next token. For values greater than zero, the model selects the next word probabilistically.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>An integer. If specified, the OpenAI API will "make a best effort to sample deterministically".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>TRUE to submit API requests in parallel. Setting to FALSE can reduce rate limit errors at the expense of longer runtime.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>If max_tokens = 1, returns a dataframe with the 5 most likely next words and their probabilities. If max_tokens &gt; 1, returns a single string of text generated by the model.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
complete_prompt('I feel like a')
complete_prompt('Here is my haiku about frogs:',
                max_tokens = 100)

## End(Not run)
</code></pre>


</div>