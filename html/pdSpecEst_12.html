<div class="container">

<table style="width: 100%;"><tr>
<td>pdMean</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Weighted Karcher mean of HPD matrices</h2>

<h3>Description</h3>

<p><code>pdMean</code> calculates an (approximate) weighted Karcher or Frechet mean of a sample of
<code class="reqn">(d,d)</code>-dimensional HPD matrices intrinsic to a user-specified metric. In the case of the
affine-invariant Riemannian metric as detailed in e.g., (Bhatia 2009)[Chapter 6] or
(Pennec et al. 2006), the weighted Karcher mean is either approximated via
the fast recursive algorithm in (Ho et al. 2013) or computed via the slower, but more accurate,
gradient descent algorithm in (Pennec 2006). By default, the unweighted Karcher mean is computed.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pdMean(M, w, metric = "Riemannian", grad_desc = FALSE, maxit = 1000,
  reltol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>M</code></td>
<td>
<p>a <code class="reqn">(d,d,S)</code>-dimensional array corresponding to a sample of <code class="reqn">(d,d)</code>-dimensional HPD matrices of
size <code class="reqn">S</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>an <code class="reqn">S</code>-dimensional nonnegative weight vector, such that <code>sum(w) = 1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>the distance measure, one of <code>'Riemannian'</code>, <code>'logEuclidean'</code>,
<code>'Cholesky'</code>, <code>'Euclidean'</code> or <code>'rootEuclidean'</code>. Defaults to <code>'Riemannian'</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>grad_desc</code></td>
<td>
<p>if <code>metric = "Riemannian"</code>, a logical value indicating if the
gradient descent algorithm in (Pennec 2006) should be used, defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>maximum number of iterations in gradient descent algorithm, only used if
<code>grad_desc = TRUE</code> and <code>metric = "Riemannian"</code>. Defaults to <code>1000</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reltol</code></td>
<td>
<p>optional tolerance parameter in gradient descent algorithm, only used if
<code>grad_desc = TRUE</code> and <code>metric = "Riemannian"</code>. Defaults to <code>1E-10</code>.</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>The function does not check for positive definiteness of the input matrices, and (depending on the
specified metric) may fail if matrices are close to being singular.
</p>


<h3>References</h3>

<p>Bhatia R (2009).
<em>Positive Definite Matrices</em>.
Princeton University Press, New Jersey.<br><br> Ho J, Cheng G, Salehian H, Vemuri B (2013).
“Recursive Karcher expectation estimators and recursive law of large numbers.”
<em>Artificial Intelligence and Statistics</em>, 325–332.<br><br> Pennec X (2006).
“Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements.”
<em>Journal of Mathematical Imaging and Vision</em>, <b>25</b>(1), 127–154.<br><br> Pennec X, Fillard P, Ayache N (2006).
“A Riemannian framework for tensor computing.”
<em>International Journal of Computer Vision</em>, <b>66</b>(1), 41–66.
</p>


<h3>See Also</h3>

<p><code>Mid</code>, <code>pdMedian</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Generate random sample of HPD matrices
m &lt;- function(){
 X &lt;- matrix(complex(real=rnorm(9), imaginary=rnorm(9)), nrow=3)
 t(Conj(X)) %*% X
}
M &lt;- replicate(100, m())
z &lt;- rnorm(100)
## Generate random weight vector
w &lt;- abs(z)/sum(abs(z))
## Compute weighted (Riemannian) Karcher mean
pdMean(M, w)

</code></pre>


</div>