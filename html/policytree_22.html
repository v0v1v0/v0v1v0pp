<div class="container">

<table style="width: 100%;"><tr>
<td>predict.policy_tree</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Predict method for policy_tree</h2>

<h3>Description</h3>

<p>Predict values based on fitted policy_tree object.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'policy_tree'
predict(object, newdata, type = c("action.id", "node.id"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>policy_tree object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>Points at which predictions should be made. Note that this matrix should have the
same number of columns as the training matrix, and that the columns must appear in the same order.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>The type of prediction required, "action.id" is the action id and
"node.id" is the integer id of the leaf node the sample falls into. Default is "action.id".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A vector of predictions. For type = "action.id" each element is an integer from 1 to d where d is
the number of columns in the reward matrix. For type = "node.id" each element is an integer corresponding
to the node the sample falls into (level-ordered).
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Construct doubly robust scores using a causal forest.
n &lt;- 10000
p &lt;- 10
# Discretizing continuous covariates decreases runtime for policy learning.
X &lt;- round(matrix(rnorm(n * p), n, p), 2)
colnames(X) &lt;- make.names(1:p)
W &lt;- rbinom(n, 1, 1 / (1 + exp(X[, 3])))
tau &lt;- 1 / (1 + exp((X[, 1] + X[, 2]) / 2)) - 0.5
Y &lt;- X[, 3] + W * tau + rnorm(n)
c.forest &lt;- grf::causal_forest(X, Y, W)

# Retrieve doubly robust scores.
dr.scores &lt;- double_robust_scores(c.forest)

# Learn a depth-2 tree on a training set.
train &lt;- sample(1:n, n / 2)
tree &lt;- policy_tree(X[train, ], dr.scores[train, ], 2)
tree

# Evaluate the tree on a test set.
test &lt;- -train

# One way to assess the policy is to see whether the leaf node (group) the test set samples
# are predicted to belong to have mean outcomes in accordance with the prescribed policy.

# Get the leaf node assigned to each test sample.
node.id &lt;- predict(tree, X[test, ], type = "node.id")

# Doubly robust estimates of E[Y(control)] and E[Y(treated)] by leaf node.
values &lt;- aggregate(dr.scores[test, ], by = list(leaf.node = node.id),
                    FUN = function(dr) c(mean = mean(dr), se = sd(dr) / sqrt(length(dr))))
print(values, digits = 1)

# Take cost of treatment into account by, for example, offsetting the objective
# with an estimate of the average treatment effect.
ate &lt;- grf::average_treatment_effect(c.forest)
cost.offset &lt;- ate[["estimate"]]
dr.scores[, "treated"] &lt;- dr.scores[, "treated"] - cost.offset
tree.cost &lt;- policy_tree(X, dr.scores, 2)

# Predict treatment assignment for each sample.
predicted &lt;- predict(tree, X)

# If there are too many covariates to make tree search computationally feasible, then one
# approach is to consider for example only the top features according to GRF's variable importance.
var.imp &lt;- grf::variable_importance(c.forest)
top.5 &lt;- order(var.imp, decreasing = TRUE)[1:5]
tree.top5 &lt;- policy_tree(X[, top.5], dr.scores, 2, split.step = 50)

</code></pre>


</div>