<div class="container">

<table style="width: 100%;"><tr>
<td>optimal_action</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Optimal action for a belief</h2>

<h3>Description</h3>

<p>Determines the optimal action for a policy (solved POMDP) for a given belief
at a given epoch.
</p>


<h3>Usage</h3>

<pre><code class="language-R">optimal_action(model, belief = NULL, epoch = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a solved POMDP.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>belief</code></td>
<td>
<p>The belief (probability distribution over the states) as a
vector or a matrix with multiple belief states as rows. If <code>NULL</code>, then the initial belief of the
model is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epoch</code></td>
<td>
<p>what epoch of the policy should be used. Use 1 for converged policies.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The name of the optimal action.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>See Also</h3>

<p>Other policy: 
<code>estimate_belief_for_nodes()</code>,
<code>plot_belief_space()</code>,
<code>plot_policy_graph()</code>,
<code>policy()</code>,
<code>policy_graph()</code>,
<code>projection()</code>,
<code>reward()</code>,
<code>solve_POMDP()</code>,
<code>solve_SARSOP()</code>,
<code>value_function()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("Tiger")
Tiger

sol &lt;- solve_POMDP(model = Tiger)

# these are the states
sol$states

# belief that tiger is to the left
optimal_action(sol, c(1, 0))
optimal_action(sol, "tiger-left")

# belief that tiger is to the right
optimal_action(sol, c(0, 1))
optimal_action(sol, "tiger-right")

# belief is 50/50
optimal_action(sol, c(.5, .5))
optimal_action(sol, "uniform")

# the POMDP is converged, so all epoch give the same result.
optimal_action(sol, "tiger-right", epoch = 10)

</code></pre>


</div>