<div class="container">

<table style="width: 100%;"><tr>
<td>rankWorkflows</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Provide a ranking of workflows involved in an estimation process.
</h2>

<h3>Description</h3>

<p>Given a <code>ComparisonResults</code> object resulting from a
performance estimation experiment, this function provides a ranking
(by default the top 5) of  
the <em>best</em> workflows involved in the comparison. The rankings are provided by
task and for each evaluation metric. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">rankWorkflows(compRes,top=min(5,length(workflowNames(compRes))),
              maxs=rep(FALSE,dim(compRes[[1]][[1]]@iterationsScores)[2]),stat="avg")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>compRes</code></td>
<td>

<p>An object of class <code>ComparisonResults</code> with the results of the
performance estimation experiment.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>top</code></td>
<td>

<p>The number of workflows to include in the rankings (defaulting to 5
or the number of workflows in the experiment if less than 5)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxs</code></td>
<td>

<p>A vector of booleans with as many elements are there are statistics measured in
the experimental comparison. A <code>TRUE</code> value means the respective
metric is to be maximized, while a <code>FALSE</code> means
minimization. Defaults to all <code>FALSE</code> values.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stat</code></td>
<td>

<p>The statistic to be used to obtain the ranks. The options are the
statistics produced by the function <code>summary</code> applied to objects
of class  <code>ComparisonResults</code>, i.e. "avg", "std",
"med", "iqr", "min", "max" or "invalid" (defaults to "avg").
</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The function returns a named list with as many components as there
are predictive tasks in the experiment. For each task you will get
another named list, with as many elements as there evaluation
metrics. For each of these components you have a data frame with <em>N</em>
lines, where <em>N</em> is the size of the requested rank. Each line includes
the name of the workflow in the respective rank position and the
estimated score it got on that particular task / evaluation metric.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code>performanceEstimation</code>,
<code>topPerformers</code>,
<code>topPerformer</code>,
<code>metricsSummary</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
## Estimating several evaluation metrics on different variants of a
## regression tree and of a SVM, on  two data sets, using one repetition
## of  10-fold CV

data(swiss)
data(mtcars)
library(e1071)

## run the experimental comparison
results &lt;- performanceEstimation(
               c(PredTask(Infant.Mortality ~ ., swiss),
                 PredTask(mpg ~ ., mtcars)),
               c(workflowVariants(learner='svm',
                                  learner.pars=list(cost=c(1,5),gamma=c(0.1,0.01))
                                 )
               ),
               EstimationTask(metrics=c("mse","mae"),method=CV(nReps=2,nFolds=5))
                                 )
## get a ranking of the top workflows for each task and evaluation metric
rankWorkflows(results)
## get a ranking of the top workflows for each task and evaluation
## metric by the median score on all iterations instead of the mean score
rankWorkflows(results, stat="med")

## End(Not run)
</code></pre>


</div>