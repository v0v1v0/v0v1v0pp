<div class="container">

<table style="width: 100%;"><tr>
<td>text_model_download</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Download pre-trained language models from HuggingFace.</h2>

<h3>Description</h3>

<p>Download pre-trained language models (Transformers Models,
such as GPT, BERT, RoBERTa, DeBERTa, DistilBERT, etc.)
from <a href="https://huggingface.co/models">HuggingFace</a> to
your local ".cache" folder ("C:/Users/[YourUserName]/.cache/").
The models will never be removed unless you run
<code>text_model_remove</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">text_model_download(model = NULL)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>Character string(s) specifying the
pre-trained language model(s) to be downloaded.
For a full list of options, see
<a href="https://huggingface.co/models">HuggingFace</a>.
Defaults to download nothing and check currently downloaded models.
</p>
<p>Example choices:
</p>

<ul>
<li>
<p><code>"gpt2"</code> (50257 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"openai-gpt"</code> (40478 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"bert-base-uncased"</code> (30522 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"bert-large-uncased"</code> (30522 vocab, 1024 dims, 24 layers)
</p>
</li>
<li>
<p><code>"bert-base-cased"</code> (28996 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"bert-large-cased"</code> (28996 vocab, 1024 dims, 24 layers)
</p>
</li>
<li>
<p><code>"bert-base-chinese"</code> (21128 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"bert-base-multilingual-cased"</code> (119547 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"distilbert-base-uncased"</code> (30522 vocab, 768 dims, 6 layers)
</p>
</li>
<li>
<p><code>"distilbert-base-cased"</code> (28996 vocab, 768 dims, 6 layers)
</p>
</li>
<li>
<p><code>"distilbert-base-multilingual-cased"</code> (119547 vocab, 768 dims, 6 layers)
</p>
</li>
<li>
<p><code>"albert-base-v2"</code> (30000 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"albert-large-v2"</code> (30000 vocab, 1024 dims, 24 layers)
</p>
</li>
<li>
<p><code>"roberta-base"</code> (50265 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"roberta-large"</code> (50265 vocab, 1024 dims, 24 layers)
</p>
</li>
<li>
<p><code>"xlm-roberta-base"</code> (250002 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"xlm-roberta-large"</code> (250002 vocab, 1024 dims, 24 layers)
</p>
</li>
<li>
<p><code>"xlnet-base-cased"</code> (32000 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"xlnet-large-cased"</code> (32000 vocab, 1024 dims, 24 layers)
</p>
</li>
<li>
<p><code>"microsoft/deberta-v3-base"</code> (128100 vocab, 768 dims, 12 layers)
</p>
</li>
<li>
<p><code>"microsoft/deberta-v3-large"</code> (128100 vocab, 1024 dims, 24 layers)
</p>
</li>
<li>
<p><code>...</code> (see <a href="https://huggingface.co/models">https://huggingface.co/models</a>)
</p>
</li>
</ul>
</td>
</tr></table>
<h3>Value</h3>

<p>Invisibly return the names of all downloaded models.
</p>


<h3>See Also</h3>

<p><code>text_init</code>
</p>
<p><code>text_model_remove</code>
</p>
<p><code>text_to_vec</code>
</p>
<p><code>text_unmask</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# text_init()  # initialize the environment

text_model_download()  # check downloaded models
text_model_download(c(
  "bert-base-uncased",
  "bert-base-cased",
  "bert-base-multilingual-cased"
))

## End(Not run)

</code></pre>


</div>